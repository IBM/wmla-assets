{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer vision image classification with WMLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Upload this notebook to your environment](#Upload-notebook)\n",
    "- [Download dataset and model](#Download-dataset-model)\n",
    "- [Import dataset](#Import-dataset)\n",
    "- [Build the model](#Build-the-model)\n",
    "- [Tune Hyper-parameter](#Tune-hyper-parameter)\n",
    "- [Run Training](#Run-training)\n",
    "- [Inspect Training Run](#Inspect-training-run)\n",
    "- [Create an inference model](#Create-an-inference-model)\n",
    "- [Test it out](#Test-it-out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[Back to top](#Contents)\n",
    "\n",
    "This notebook details the process of performing a basic computer vision image classification example using the Deep Learning Impact functionality within Watson Machine Learning Accelerator.  \n",
    "\n",
    "Please visit [Watson Machine Learning Accelerator Learning Path](https://developer.ibm.com/series/learning-path-get-started-with-watson-machine-learning-accelerator/) for further insight of Watson ML Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload this notebook to your environment\n",
    "\n",
    "1.\tOpen a web browser and enter or paste this URL: https://<font color='red'>**IP_address**</font>:8443/platform\n",
    "2.\tYou should see the login display in Figure 1.\n",
    "3.\tLogin with your assigned user ID and password. You should then see the display in Figure 2.\n",
    "4.  Click on Instance Group **b0p0xx-spark231**\n",
    "5.  Navigate from the **Overview panel** to the **Notebooks** panel Tuning panel,  and click on **My Notebooks** then **Jupyter 5.4.0 - Owned by xxxx**, display in Figure 3.\n",
    "6.  When presented with a dialogue box,  click on **upload** and select this notebook and upload to the cluster,  display in Figure 4.\n",
    "\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/console.png) \n",
    "Figure 1. Spectrum Conductor login screen\n",
    "\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/conductor_dashboard.png) \n",
    "Figure 2. Spectrum Conductor Instance Group view\n",
    "\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/launch_notebook.png) \n",
    "Figure 3. Launch Notebook\n",
    "\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/upload_notebook.png) \n",
    "Figure 4. Upload Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and model\n",
    "\n",
    "<a id='Download-dataset-model'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "\n",
    "### Download dataset\n",
    "Now we are ready to go,  lets get started and download the dataset from github!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n"
     ]
    }
   ],
   "source": [
    "cd /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-07 15:58:32--  https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/CIFAR10-images.zip\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/IBM/wmla-assets/master/WMLA-learning-journey/image-classification-with-WMLA-UI/CIFAR10-images.zip [following]\n",
      "--2020-04-07 15:58:32--  https://raw.githubusercontent.com/IBM/wmla-assets/master/WMLA-learning-journey/image-classification-with-WMLA-UI/CIFAR10-images.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.36.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.36.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 81347372 (78M) [application/zip]\n",
      "Saving to: 'CIFAR10-images.zip.1'\n",
      "\n",
      "100%[======================================>] 81,347,372  93.2MB/s   in 0.8s   \n",
      "\n",
      "2020-04-07 15:58:33 (93.2 MB/s) - 'CIFAR10-images.zip.1' saved [81347372/81347372]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/CIFAR10-images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip CIFAR10-images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/CIFAR-10-images/train\n"
     ]
    }
   ],
   "source": [
    "cd CIFAR-10-images/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/CIFAR-10-images/test\n"
     ]
    }
   ],
   "source": [
    "cd ../test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_path = %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the Dataset Training and Testing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_path: /tmp/CIFAR-10-images/train\n",
      "testing_path:/tmp/CIFAR-10-images/test\n"
     ]
    }
   ],
   "source": [
    "print ('training_path: ' + training_path)\n",
    "print ('testing_path:' + testing_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dli-1.2.3-tensorflow-samples'...\n",
      "remote: Enumerating objects: 308, done.\u001b[K\n",
      "remote: Counting objects: 100% (308/308), done.\u001b[K\n",
      "remote: Compressing objects: 100% (227/227), done.\u001b[K\n",
      "remote: Total 539 (delta 111), reused 252 (delta 79)\u001b[K\n",
      "Receiving objects: 100% (539/539), 448.54 KiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (212/212), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://us-south.git.cloud.ibm.com/ibmconductor-deep-learning-impact/dli-1.2.3-tensorflow-samples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/dli-1.2.3-tensorflow-samples/tensorflow-1.13.1/cifar10\n"
     ]
    }
   ],
   "source": [
    "cd dli-1.2.3-tensorflow-samples/tensorflow-1.13.1/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path: /tmp/dli-1.2.3-tensorflow-samples/tensorflow-1.13.1/cifar10\n"
     ]
    }
   ],
   "source": [
    "model_path = %pwd\n",
    "print ('model_path: '+ model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "<a id='Import-dataset'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "1. Lets swtich \n",
    "1. At the top Left select **Workload** > **Spark** > **Deep Learning**\n",
    "1. Select the **Datasets** tab, and click **New**\n",
    "1. Click **Images for Object Classification**. When presented with a dialog box, provide a unique name (lets use \"Cifar10\"!!!) and select the TFRecords for 'Dataset stores images in',  and then set the value of \"Training folder\" and \"Testing folder\" with the folder that contains the images obtained in the previous step (\"**/tmp/CIFAR-10-images/train**\" + \"**/tmp/CIFAR-10-images/train**\").  The other fields are fine to use with the default settings. When you're ready, click Create.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/ImportDataset.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove dataset from the file system\n",
    "\n",
    "!rm -rf /tmp/CIFAR-10-images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "<a id='Build-the-model'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "1. Select the Models tab and click **New** > **Add Location**\n",
    "1. When presented with a diaglog box,  enter following attributes:\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modelcreation3.png)\n",
    "<br>\n",
    "1. Select the **Tensorflow-cifar10** and click **Next**.\n",
    "\n",
    "1. When presented with a dialog box, ensure that the Training engine is set to singlenode and that the data set points to the one you just created\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modelcreation1.png)\n",
    "<br>\n",
    "1. Set the following parameters and click **Add**\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modelcreation2.png)\n",
    "<br>\n",
    "1.  The model is now ready to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up Model \n",
    "\n",
    "!rm -rf /tmp/dli-1.2.3-tensorflow-samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyper-parameter\n",
    "\n",
    "<a id='Tune-hyper-parameter'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "1. You could search optimal hyperparameter by leveraging automated Hyper-parameter Tuning.\n",
    "1. Back at the **Models** tab, **click** on the model \n",
    "1. Navigate from the **Overview panel** to the **Hyperparameter Tuning** panel\n",
    "1. Click **New**\n",
    "1. When presented with a dialog box, enter following value and click **Start Tuning**\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune1.png)\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune2.png)\n",
    "1. Under the **Hyperparameter Tuning** panel, click on the hyperparameter search job \n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune3.png)\n",
    "1. Navigate from the **Input panel** to the **Progress panel** and **Best panel** to review the optimal set of hyperparameter\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune4.png)\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "<a id='Run-training'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "1. Back at the **Models** tab, select the model you created in previous step and click **Train**\n",
    "1. When presented with a dialog box, keep default parameter and click **Start Training**\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltrain1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Training Run\n",
    "\n",
    "<a id='Inspect-training-run'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "1. From the **Train** submenu of the **Models** tab, select the model that is training by clicking the link.\n",
    "1. Navigate from the **Overview panel** to the **Training** panel, and click the most recent link. You can watch as the results roll in.\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltrain2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyper-parameter\n",
    "\n",
    "<a id='Tune-hyper-parameter'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "1. You could search optimal hyperparameter by leveraging automated Hyper-parameter Tuning.\n",
    "1. Back at the **Models** tab, **click** on the model \n",
    "1. Navigate from the **Overview panel** to the **Hyperparameter Tuning** panel\n",
    "1. Click **New**\n",
    "1. When presented with a dialog box, enter following value and click **Start Tuning**\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune1.png)\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune2.png)\n",
    "1. Under the **Hyperparameter Tuning** panel, click on the hyperparameter search job \n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune3.png)\n",
    "1. Navigate from the **Input panel** to the **Progress panel** and **Best panel** to review the optimal set of hyperparameter\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune4.png)\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/modeltune5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inference model\n",
    "<a id='Create-an-inference-model'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "\n",
    "1. From the Training view, click Create Inference Model.\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/inference1.png)\n",
    "1. This creates a new model in the Models tab. You can view it by going to the Inference submenu.\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/inference2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out\n",
    "<a id='Test-it-out'></a>\n",
    "[Back to top](#Contents)\n",
    "\n",
    "1. Download [inference test image](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/car.jpg) to your laptop\n",
    "\n",
    "1. Go back to the Models tab, select the new inference model, and click Test. At the new Testing overview screen, select New Test.\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/inference3.png)\n",
    "\n",
    "1.  When presented with a dialog box, click **Choose File** to load the inference test image.  Click **Start Test**\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/inference4.png)\n",
    "\n",
    "1. Wait for the test state to change from RUNNING to FINISHED.  Click the link to view the results of the test.\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/inference5.png)\n",
    "\n",
    "1. As you can see, the images are available as a thumbnail preview along with their classified label and probability.\n",
    "\n",
    "![](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/image-classification-with-WMLA-UI/Shared-images/inference6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is version 1.0 and its content is copyright of IBM.   All rights reserved.   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
