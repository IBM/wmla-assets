{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Training using Accelerator API\n",
    "\n",
    "![SpectrumComputeFamily_Conductor-HorizontalColorWhite.png](https://raw.githubusercontent.com/IBM/wmla-learning-path/master/shared-images/hpo.png)\n",
    "\n",
    "In this notebook you will learn how to submit a model and dataset to the Watson Machine Learning Accelerator (WMLA) API to run Hyper Parameter Optimization (HPO).  \n",
    "\n",
    "For this notebook you will use a model and dataset that have already been setup to leverage the API.  For details on the API see this link in the Knowledge Center (KC).\n",
    "\n",
    "[API Documentation](https://www.ibm.com/support/knowledgecenter/en/SSFHA8_1.2.1/cm/deeplearning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "To use the WMLA API, we will be using the python requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T18:09:12.788334Z",
     "start_time": "2020-06-24T18:09:12.269121Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import tarfile\n",
    "import tempfile\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "# utility print function\n",
    "def nprint(mystring) :\n",
    "    print(\"**{}** : {}\".format(sys._getframe(1).f_code.co_name,mystring))\n",
    "\n",
    "# utility makedir\n",
    "def makeDirIfNotExist(directory) :\n",
    "    if not os.path.exists(directory):  \n",
    "        nprint(\"Making directory {}\".format(directory))\n",
    "        os.makedirs(directory) \n",
    "    else :\n",
    "        nprint(\"Directory {} already exists .. \".format(directory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment details and Project Config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T18:09:13.694206Z",
     "start_time": "2020-06-24T18:09:13.558602Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"This is your userid...\")\n",
    "!whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T18:09:13.935996Z",
     "start_time": "2020-06-24T18:09:13.927081Z"
    }
   },
   "outputs": [],
   "source": [
    "def getconfig(cfg_in={}):\n",
    "    cfg = {}\n",
    "    cfg[\"master_host\"] = 'https://p10a117.pbm.ihost.com'\n",
    "    cfg[\"dli_rest_port\"] = '9243'\n",
    "    cfg[\"sc_rest_port\"] = '8643'\n",
    "    cfg[\"num_images\"] = {\"train\":200,\"valid\":20,\"test\":20}\n",
    "    # ==== CLASS ENTER User login details below =====\n",
    "    cfg[\"wmla_user\"] = 'b0p036aX' # 'b0p036aX'  # <=enter your id here\n",
    "    cfg[\"wmla_pwd\"] = 'pwb0p036aX' # 'pwb0p036aX'  # <=enter your pwd here\n",
    "    # ==== CLASS ENTER User login details above =====\n",
    "    cfg[\"sig_name\"]  = 'b0p036a-dliauto'\n",
    "    cfg[\"code_dir\"] = \"./code_samples/pytorch_hpo\"\n",
    "\n",
    "    # overwrite configs if passed\n",
    "    for (k,v) in cfg_in.items() :\n",
    "        nprint(\"Overriding Config {}:{} with {}\".format(k,cfg[k],v))\n",
    "        cfg[k] = v\n",
    "    return cfg\n",
    "\n",
    "# cfg is used as a global variable throughout this notebook\n",
    "cfg=getconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T18:09:14.571412Z",
     "start_time": "2020-06-24T18:09:14.557100Z"
    }
   },
   "outputs": [],
   "source": [
    "# REST call variables\n",
    "commonHeaders = {'Accept': 'application/json'}\n",
    "\n",
    "# Use closures for cfg for now ..\n",
    "def get_tmp_dir() :\n",
    "    return \"./tmp\"\n",
    "\n",
    "def get_tar_file() :\n",
    "    return get_tmp_dir() + \"/\" + cfg[\"wmla_user\"]+\".modelDir.tar\"\n",
    "\n",
    "#get api endpoint\n",
    "def get_ep(mode=\"sc\") :\n",
    "    if mode==\"sc\" :\n",
    "        sc_rest_url =  cfg[\"master_host\"] +':'+ cfg[\"sc_rest_port\"] +'/platform/rest/conductor/v1'\n",
    "        return sc_rest_url\n",
    "    elif(mode==\"dl\") :\n",
    "        dl_rest_url = cfg[\"master_host\"] +':'+cfg[\"dli_rest_port\"] +'/platform/rest/deeplearning/v1'\n",
    "        return dl_rest_url\n",
    "    else :\n",
    "        nprint(\"Error mode : {} not supported\".format(mode))\n",
    "\n",
    "def myauth():\n",
    "    return(cfg[\"wmla_user\"],cfg[\"wmla_pwd\"])\n",
    "\n",
    "print (\"SC API Endpoints : {}\".format(get_ep(\"sc\")))\n",
    "print (\"DL API Endpoints : {}\".format(get_ep(\"dl\")))\n",
    "print (myauth())\n",
    "print (get_tar_file())\n",
    "#myauth = (wmla_user, wmla_pwd)\n",
    "\n",
    "# Setup Requests session\n",
    "req = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is any existing hpo tasks and also verify the platform health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest API: **GET platform/rest/deeplearning/v1/hypersearch**\n",
    "- Description: Get all the hpo task that the login user can access.\n",
    "- OUTPUT: A list of hpo tasks and each one with the same format which can be found in the api doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T18:09:20.216089Z",
     "start_time": "2020-06-24T18:09:20.175200Z"
    }
   },
   "outputs": [],
   "source": [
    "def hpo_health_check():\n",
    "    getTuneStatusUrl = get_ep(\"dl\") + '/hypersearch'\n",
    "    nprint ('getTuneStatusUrl: %s' %getTuneStatusUrl)\n",
    "    r = req.get(getTuneStatusUrl, headers=commonHeaders, verify=False, auth=myauth())\n",
    "    \n",
    "    if not r.ok:\n",
    "        nprint('check hpo task status failed: code=%s, %s'%(r.status_code, r.content))\n",
    "    else:\n",
    "        if len(r.json()) == 0:\n",
    "            nprint('There is no hpo task been created')\n",
    "        for item in r.json():\n",
    "            nprint('Hpo task: %s, State: %s'%(item['hpoName'], item['state']))\n",
    "            #print('Best:%s'%json.dumps(item.get('best'), sort_keys=True, indent=4))\n",
    "\n",
    "hpo_health_check()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch an HPO task using API\n",
    "\n",
    "\n",
    "This part of the lab will step through the steps required to train a simple MNIST (greyscale digits) dataset using WMLA HPO through API ... \n",
    "\n",
    "We will be using the **PyTorch** deep learning framework for this example.  Lets take a look at the code ..\n",
    "\n",
    "* pytorch_mnist_original.py : original sample code from pytorch.org\n",
    "* pytorch_mnist_HPO.py      : Modified code that implements the required HPO hooks for the WMLA framework\n",
    "    * Learing rate controlled by HPO\n",
    "* pytorch_mnist_HPO_advanced.py : Similar example just showing multiple settings \n",
    "    * Learning rate controlled by HPO\n",
    "    * Dropout controlled by HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T18:09:08.288198Z",
     "start_time": "2020-06-24T18:09:08.126830Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Code directory : {}\".format(cfg[\"code_dir\"]))\n",
    "!ls {cfg[\"code_dir\"]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run mnist using command line interface\n",
    "In the following cells we will be running an deep learning training job for Mnist dataset.  Typically you will develop your initial model and submit single jobs until you are happy with the result before you submit many many jobs.  Since our mnist program is written in python, lets submit if via the command line using the sample below.  \n",
    "\n",
    "1.  open a terminal in our juypter noteboook and go to code_samples directory\n",
    "```\n",
    "cd code_samples\n",
    "```\n",
    "\n",
    "2. now source our conda environment manually\n",
    "```\n",
    "source /gpfs/software/wmla-p10a117/wmla_anaconda/b0p036a/anaconda/envs/powerai162/etc/profile.d/conda.sh\n",
    "```\n",
    "3 . export some required environment variables\n",
    "```\n",
    "export DLI_DATA_FS=/gpfs/software/wmla-p10a117/dli_data_fs/\n",
    "export RESULT_DIR=./code_samples/\n",
    "export EGO_TOP=/gpfs/software/wmla-p10a117/wmla\n",
    "```\n",
    "\n",
    "4. authenticate to WMLA\n",
    "```\n",
    "python $EGO_TOP/dli/1.2.3/dlpd/bin/dlicmd.py  --logon  --master-host p10a117.pbm.ihost.com --username b0p036aX --password pwb0p036aX\n",
    "```\n",
    "\n",
    "5.  Submit a single Job\n",
    "\n",
    "```\n",
    "python $EGO_TOP/dli/1.2.3/dlpd/bin/dlicmd.py --exec-start PyTorch --cs-datastore-meta type=fs \\\n",
    "    --python-version 3.6  --master-host p10a117.pbm.ihost.com --ig b0p036a-dliauto \\\n",
    "    --gpuPerWorker 1 --model-main pytorch_mnist_HPO.py --model-dir pytorch_hpo \\\n",
    "    --epochs 2 --debug-level debug\n",
    "```\n",
    "   \n",
    "    \n",
    "6.  View Job results..\n",
    "```\n",
    "JOB_ID=vanstee-1987246869690900-700353922\n",
    "python  $EGO_TOP/dli/1.2.3/dlpd/bin/dlicmd.py  --exec-errlogs   $JOB_ID --master-host p10a117.pbm.ihost.com  | grep -v \" INFO \" | grep -v \"^+\" | grep -v \"^Spark\" | more\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model file update to Run HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Developer Note** \n",
    "The WMLA framework requires 2 changes to your code to support the HPO API, these are :\n",
    "- Inject hyper-parameters for the sub-training during search\n",
    "- Retrieve sub-training result metric\n",
    "\n",
    "We will cover these details in the next 2 sections\n",
    "\n",
    "For an even more detailed review, see the [documentation in Github](https://github.com/IBM/wmla-learning-path/blob/master/02_hpo_for_developers.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model update part 1 - Inject hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameters will be supplied in a file called **config.json** with JSON format,located in the current working directory and can be read direcly as the following example snippet.\n",
    "\n",
    "<pre>\n",
    "hyper_params = json.loads(open(\"<b>config.json</b>\").read())\n",
    "learning_rate = float(hyper_params.get(\"<b>learning_rate</b>\", \"0.01\"))\n",
    "</pre>\n",
    "\n",
    "After this, you can use these hyper-parameters during the model trainings. The **hyper-parameter name** and **value** type is defined through the search space part in body of REST call when launching a new hpo task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model update part 2 - Retrieve sub-training result metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of your training run, your code will need to create a file called **val_dict_list.json** with test metrics generated during training. These metrics will be used by the search algorithm to propose new sets of hyper-parameters. Please note that **val_dict_list.json** should be created under the result directory which can be retrieved through the environment variable **RESULT_DIR**.\n",
    "\n",
    "<pre>\n",
    "with open('{}/val_dict_list.json'.format(os.environ['<b>RESULT_DIR</b>']), 'w') as f:\n",
    "    json.dump(test_metrics, f)\n",
    "</pre>\n",
    "\n",
    "The content of **val_dict_list.json** will be some thing as below, **step** is some thing optional meaning the training iteration or epochs, one of **loss** and **accuracy** can be the name of target metric to optimize, at least one metric need to be included here. The specific name of metric used to optimize (minimize or maximize) is defined in the body of REST call when launching a new hpo task. \n",
    "\n",
    "```\n",
    "[\n",
    "{‘step’: 1, ‘loss’:0.2487, ‘accuracy’: 0.4523},\n",
    "{‘step’: 2, ‘loss’:0.1487, ‘accuracy’: 0.5523},\n",
    "{‘step’: 3, ‘loss’:0.1087, ‘accuracy’: 0.6523},\n",
    "…\n",
    "]\n",
    "```\n",
    "\n",
    "**We have added this to the pytorch_mnist_HPO.py file already for you in the ./code_samples/pytorch_hpo directory!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T18:34:39.655395Z",
     "start_time": "2020-06-24T18:34:39.520497Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cfg[\"code_dir\"])\n",
    "!ls {cfg[\"code_dir\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch HPO task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we package up our model to send to the API for HPO.  Lets see how this works ...\n",
    "\n",
    "\n",
    "\n",
    "REST API: **POST /platform/rest/deeplearning/v1/hypersearch**\n",
    "- Description: Start a new HPO task\n",
    "- Content-type: Multi-Form\n",
    "- Multi-Form Data:\n",
    "  - files: Model files tar package, ending with `.modelDir.tar`\n",
    "  - form-filed: {‘data’: ‘String format of input parameters to start hpo task, let’s call it as **hpo_input** and show its specification later’}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package model files for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package the updated model files into a tar file ending with `.modelDir.tar`\n",
    "\n",
    "REST API expects a modelDir.tar with the model code inside ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T17:51:05.451498Z",
     "start_time": "2020-06-24T17:51:05.418630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tar up \n",
    "def make_tarfile():\n",
    "    makeDirIfNotExist(get_tmp_dir())\n",
    "    tar_archive_base=os.path.basename(cfg[\"code_dir\"])\n",
    "    nprint(\"Tarring up {} to {}\".format(cfg[\"code_dir\"],get_tmp_dir()))\n",
    "    nprint(\"Adding base directory to archive : {}\".format(tar_archive_base))\n",
    "    with tarfile.open(get_tar_file(), \"w:gz\") as tar:\n",
    "        tar.add(cfg[\"code_dir\"], arcname=tar_archive_base)\n",
    "#MODEL_DIR_SUFFIX = \".modelDir.tar\"\n",
    "#tempFile = tempfile.mktemp(MODEL_DIR_SUFFIX)\n",
    "\n",
    "make_tarfile()\n",
    "\n",
    "files = {'file': open(get_tar_file(), 'rb')}\n",
    "print(\"Files : {}\".format(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct POST request data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hpo_input** will be a Python dict or json format as below, convert to string when calling REST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T17:51:06.621104Z",
     "start_time": "2020-06-24T17:51:06.610599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note, this \n",
    "data =  {\n",
    "        'modelSpec': # Define the model training related parameters\n",
    "        {\n",
    "            # Spark instance group which will be used to run the HPO sub-trainings. The Spark instance group selected\n",
    "            # here should match the sub-training args, for example, if the sub-training args try to run a EDT job,\n",
    "            # then we should put a Spark instance group with capability to run EDT job here.\n",
    "            'sigName': cfg[\"sig_name\"],\n",
    "\n",
    "            # These are the arguments we'll pass to the execution engine; they follow the same conventions\n",
    "            # of the dlicmd.py command line launcher\n",
    "            #\n",
    "            # See:\n",
    "            #   https://www.ibm.com/support/knowledgecenter/en/SSFHA8_1.2.1/cm/dlicmd.html\n",
    "            # In this example, args after --model-dir are all the required parameter for the original model itself.\n",
    "            #\n",
    "            'args': '--exec-start PyTorch --cs-datastore-meta type=fs --python-version 3.6\\\n",
    "                     --gpuPerWorker 1 --model-main pytorch_mnist_HPO.py --model-dir pytorch_hpo\\\n",
    "                     --epochs 3 --debug-level debug'\n",
    "                \n",
    "        },\n",
    "    \n",
    "        'algoDef': # Define the parameters for search algorithms\n",
    "        {\n",
    "            # Name of the search algorithm, one of Random, Bayesian, Tpe, Hyperband, ExperimentGridSearch\n",
    "            'algorithm': 'Random', \n",
    "            # Max running time of the hpo task in minutes, -1 means unlimited\n",
    "            'maxRunTime': 60,  \n",
    "            # Max number of training job to submitted for hpo task, -1 means unlimited’,\n",
    "            'maxJobNum': 4,            \n",
    "            # Max number of training job to run in parallel, default 1. It depends on both the\n",
    "            # avaiable resource and if the search algorithm support to run in parallel, current only Random\n",
    "            # fully supports to run in parallel, Hyperband and Tpe supports to to in parellel in some phase,\n",
    "            # Bayesian runs in sequence now.\n",
    "            'maxParalleJobNum': 2, \n",
    "            # Name of the target metric that we are trying to optimize when searching hyper-parameters.\n",
    "            # It is the same metric name that the model update part 2 trying to dump.\n",
    "            'objectiveMetric' : 'loss',\n",
    "            # Strategy as how to optimize the hyper-parameters, minimize means to find better hyper-parameters to\n",
    "            # make the above objectiveMetric as small as possible, maximize means the opposite.\n",
    "            'objective' : 'minimize',\n",
    "        },\n",
    "    \n",
    "        # Define the hyper-paremeters to search and the corresponding search space.\n",
    "        'hyperParams':\n",
    "        [\n",
    "             {\n",
    "                 # Hyperparameter name, which will be the hyper-parameter key in config.json\n",
    "                 'name': 'learning_rate',\n",
    "                 # One of Range, Discrete\n",
    "                 'type': 'Range',\n",
    "                 # one of int, double, str\n",
    "                 'dataType': 'DOUBLE',\n",
    "                 # lower bound and upper bound when type=range and dataType=double\n",
    "                 'minDbVal': 0.001,\n",
    "                 'maxDbVal': 0.1,\n",
    "                 # lower bound and upper bound when type=range and dataType=int\n",
    "                 'minIntVal': 0,\n",
    "                 'maxIntVal': 0,\n",
    "                 # Discrete value list when type=discrete\n",
    "                 'discreteDbVal': [],\n",
    "                 'discreteIntVal': [],\n",
    "                 'discreateStrVal': []\n",
    "                 #step size to split the Range space. ONLY valid when type is Range\n",
    "                 #'step': '0.002',\n",
    "             }\n",
    "         ]\n",
    "    }\n",
    "mydata={'data':json.dumps(data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the Post request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit hpo task through the Post call and a hpo name/id as string format will get back.\n",
    "\n",
    "**Note**:This cannot be submitted twice.. you need to rebuild the tar file prior to resubmitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T17:51:08.321471Z",
     "start_time": "2020-06-24T17:51:08.138173Z"
    }
   },
   "outputs": [],
   "source": [
    "def submit_job(job_dict,job_files,job_auth):\n",
    "    startTuneUrl=get_ep('dl') + '/hypersearch'\n",
    "    nprint(\"startTuneUrl : {}\".format(startTuneUrl))\n",
    "    nprint(\"files : {}\".format(job_files))\n",
    "    nprint(\"myauth() : {}\".format(job_auth))\n",
    "    #print(\"hpo_job_id : {}\".format(hpo_job_id))\n",
    "    r = req.post(startTuneUrl, headers=commonHeaders, data=job_dict, files=job_files, verify=False, auth=job_auth)\n",
    "    hpo_name=None\n",
    "    if r.ok:\n",
    "        hpo_name = r.json()\n",
    "        print ('\\nModel submitted successfully: {}'.format(hpo_name))\n",
    "        \n",
    "    else:\n",
    "        print('\\nModel submission failed with code={}, {}'. format(r.status_code, r.content))\n",
    "    return hpo_name\n",
    "\n",
    "hpo_job_id = submit_job(mydata,files,myauth())\n",
    "print(\"hpo_job_id : {}\".format(hpo_job_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Status until complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T17:57:40.834725Z",
     "start_time": "2020-06-24T17:57:30.708760Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_job_status(job_id,refresh_rate=3) :\n",
    "\n",
    "    getHpoUrl = get_ep('dl') +'/hypersearch/'+ job_id\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "    keep_running=True\n",
    "    res=None\n",
    "    while(keep_running):\n",
    "        res = req.get(getHpoUrl, headers=commonHeaders, verify=False, auth=myauth())\n",
    "        experiments=res.json()['experiments']\n",
    "        experiments = pd.DataFrame.from_dict(experiments)\n",
    "        pd.set_option('max_colwidth', 120)\n",
    "        clear_output()\n",
    "        print(\"Refreshing every {} seconds\".format(refresh_rate))\n",
    "        display(experiments)\n",
    "        pp.pprint(res.json())\n",
    "        if(res.json()['state'] not in ['SUBMITTED','RUNNING']) :\n",
    "            keep_running=False\n",
    "        time.sleep(refresh_rate)\n",
    "    return res\n",
    "job_status = query_job_status(hpo_job_id,refresh_rate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the Best Result of HPO Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T17:57:53.144422Z",
     "start_time": "2020-06-24T17:57:53.135883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets query our result to see what happened during HPO training!\n",
    "\n",
    "#res.ok\n",
    "#res.json()\n",
    "#print(type(res))\n",
    "#print(dir(res))\n",
    "#print(json.dumps(res.json(), indent=4, sort_keys=True))\n",
    "        \n",
    "print('Hpo task %s completes with state %s'%(hpo_job_id, job_status.json()['state']))\n",
    "print(\"Best HPO result ...\")\n",
    "job_status.json()[\"best\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Complete \n",
    "Congratulations, you have completed our demonstration of using WMLA for distributed hyperparameter optimization search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional Advanced Example] : Random Optimization using Multiple Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T17:58:59.363956Z",
     "start_time": "2020-06-24T17:58:59.347581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note, this \n",
    "data =  {\n",
    "        'modelSpec': # Define the model training related parameters\n",
    "        {\n",
    "            # Spark instance group which will be used to run the HPO sub-trainings. The Spark instance group selected\n",
    "            # here should match the sub-training args, for example, if the sub-training args try to run a EDT job,\n",
    "            # then we should put a Spark instance group with capability to run EDT job here.\n",
    "            'sigName': cfg[\"sig_name\"],\n",
    "\n",
    "            # These are the arguments we'll pass to the execution engine; they follow the same conventions\n",
    "            # of the dlicmd.py command line launcher\n",
    "            #\n",
    "            # See:\n",
    "            #   https://www.ibm.com/support/knowledgecenter/en/SSFHA8_1.2.1/cm/dlicmd.html\n",
    "            # In this example, args after --model-dir are all the required parameter for the original model itself.\n",
    "            #\n",
    "            'args': '--exec-start PyTorch --cs-datastore-meta type=fs --python-version 3.6\\\n",
    "                     --gpuPerWorker 1 --model-main pytorch_mnist_HPO_advanced.py --model-dir pytorch_hpo\\\n",
    "                     --epochs 3 --debug-level debug'\n",
    "                \n",
    "        },\n",
    "    \n",
    "        'algoDef': # Define the parameters for search algorithms\n",
    "        {\n",
    "            # Name of the search algorithm, one of Random, Bayesian, Tpe, Hyperband, ExperimentGridSearch\n",
    "            'algorithm': 'Random', \n",
    "            # Max running time of the hpo task in minutes, -1 means unlimited\n",
    "            'maxRunTime': 60,  \n",
    "            # Max number of training job to submitted for hpo task, -1 means unlimited’,\n",
    "            'maxJobNum': 30,            \n",
    "            # Max number of training job to run in parallel, default 1. It depends on both the\n",
    "            # avaiable resource and if the search algorithm support to run in parallel, current only Random\n",
    "            # fully supports to run in parallel, Hyperband and Tpe supports to to in parellel in some phase,\n",
    "            # Bayesian runs in sequence now.\n",
    "            'maxParalleJobNum': 30, \n",
    "            # Name of the target metric that we are trying to optimize when searching hyper-parameters.\n",
    "            # It is the same metric name that the model update part 2 trying to dump.\n",
    "            'objectiveMetric' : 'loss',\n",
    "            # Strategy as how to optimize the hyper-parameters, minimize means to find better hyper-parameters to\n",
    "            # make the above objectiveMetric as small as possible, maximize means the opposite.\n",
    "            'objective' : 'minimize',\n",
    "        },\n",
    "    \n",
    "        # Define the hyper-paremeters to search and the corresponding search space.\n",
    "        'hyperParams':\n",
    "        [\n",
    "             {\n",
    "                 # Hyperparameter name, which will be the hyper-parameter key in config.json\n",
    "                 'name': 'learning_rate',\n",
    "                 # One of Range, Discrete\n",
    "                 'type': 'Range',\n",
    "                 # one of int, double, str\n",
    "                 'dataType': 'DOUBLE',\n",
    "                 # lower bound and upper bound when type=range and dataType=double\n",
    "                 'minDbVal': 0.0001,\n",
    "                 'maxDbVal': 0.1,\n",
    "                 # lower bound and upper bound when type=range and dataType=int\n",
    "                 'minIntVal': 0,\n",
    "                 'maxIntVal': 0,\n",
    "                 # Discrete value list when type=discrete\n",
    "                 'discreteDbVal': [],\n",
    "                 'discreteIntVal': [],\n",
    "                 'discreateStrVal': []\n",
    "                 #step size to split the Range space. ONLY valid when type is Range\n",
    "                 #'step': '0.002',\n",
    "             },{\n",
    "                 # Hyperparameter name, which will be the hyper-parameter key in config.json\n",
    "                 'name': 'num_hidden_layers',\n",
    "                 # One of Range, Discrete\n",
    "                 'type': 'Range',\n",
    "                 # one of int, double, str\n",
    "                 'dataType': 'INT',\n",
    "                 # lower bound and upper bound when type=range and dataType=double\n",
    "                 'minDbVal': 0.0,\n",
    "                 'maxDbVal': 0.0,\n",
    "                 # lower bound and upper bound when type=range and dataType=int\n",
    "                 'minIntVal': 25,\n",
    "                 'maxIntVal': 200,\n",
    "                 # Discrete value list when type=discrete\n",
    "                 'discreteDbVal': [],\n",
    "                 'discreteIntVal': [],\n",
    "                 'discreateStrVal': []\n",
    "                 #step size to split the Range space. ONLY valid when type is Range\n",
    "                 #'step': '0.002',\n",
    "             },             {\n",
    "                 # Hyperparameter name, which will be the hyper-parameter key in config.json\n",
    "                 'name': 'dropout_rate',\n",
    "                 # One of Range, Discrete\n",
    "                 'type': 'Range',\n",
    "                 # one of int, double, str\n",
    "                 'dataType': 'DOUBLE',\n",
    "                 # lower bound and upper bound when type=range and dataType=double\n",
    "                 'minDbVal': 0.00,\n",
    "                 'maxDbVal': 0.40,\n",
    "                 # lower bound and upper bound when type=range and dataType=int\n",
    "                 'minIntVal': 0,\n",
    "                 'maxIntVal': 0,\n",
    "                 # Discrete value list when type=discrete\n",
    "                 'discreteDbVal': [],\n",
    "                 'discreteIntVal': [],\n",
    "                 'discreateStrVal': []\n",
    "                 #step size to split the Range space. ONLY valid when type is Range\n",
    "                 #'step': '0.002',\n",
    "             }\n",
    "         ]\n",
    "    }\n",
    "mydata={'data':json.dumps(data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Advanced Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T17:59:00.680879Z",
     "start_time": "2020-06-24T17:59:00.511878Z"
    }
   },
   "outputs": [],
   "source": [
    "files = {'file': open(get_tar_file(), 'rb')}\n",
    "print(\"Files : {}\".format(files))\n",
    "hpo_job_id = submit_job(mydata,files,myauth())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T18:03:05.335453Z",
     "start_time": "2020-06-24T17:59:53.807805Z"
    }
   },
   "outputs": [],
   "source": [
    "job_status = query_job_status(hpo_job_id,refresh_rate=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T12:29:58.192443Z",
     "start_time": "2020-06-05T12:29:58.190219Z"
    }
   },
   "source": [
    "### Best Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T16:08:23.147180Z",
     "start_time": "2020-06-05T16:08:23.128889Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Hpo task %s completes with state %s'%(hpo_job_id, job_status.json()['state']))\n",
    "print(\"Best HPO result ...\")\n",
    "job_status.json()[\"best\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
