{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Accelerated Elastic Deep Learning Service in Cloud Pak for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook created by Kelvin Lui, Xue Yin Zhuang, Xue Zhou Yuan (January 2021)\n",
    "\n",
    "Watson Machine Learning Accelerator in Cloud Pak for Data offers GPU Accelerated Elastic Deep Learning service.   This service enables multiple data scientists to accelerate deep learning model training across multiple GPUs and server, share GPUs in a dynamic fashion,  and drives data scientist productivity and overall GPU utilization.\n",
    "\n",
    "In this notebook, you will learn how to scale PyTorch model with multiple GPUs with GPU Accelerated Elastic Deep Learning service, monitor the running job, and debug any issues seen.\n",
    "\n",
    "This notebook uses Watson Machine learning Accelerator 1.2.3 with Cloud Pak for Data 3.5.\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "- [The big picture](#The-big-picture)\n",
    "- [Changes to your code](#Changes-to-your-code)\n",
    "- [Set up API end point and log on](#Set-up-API-end-point-and-log-on)\n",
    "- [Submit job via API](#Submit-job-via-API)\n",
    "- [Monitor running job](#Monitor-running-job)\n",
    "- [Retrieve output and saved models](#Retrieve-output-and-saved-models)\n",
    "  - [Output - Retrieve training output](#Output:--Retrieve-Training-Metric)\n",
    "  - [Save Models](#Save-Model)\n",
    "- [Debugging any issues](#Debugging-any-issues)\n",
    "- [Further information and useful links](#Further-information-and-useful-links)\n",
    "- [Appendix](#Appendix)\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The big picture\n",
    "[Back to top](#Contents)\n",
    "\n",
    "This notebook details the process of taking your PyTorch model and making the changes required to train the model using [IBM Watson Machine Learning GPU Accelerated Elastic Deep Learning service](https://developer.ibm.com/series/learning-path-get-started-with-watson-machine-learning-accelerator/) (WML Accelerator) \n",
    "\n",
    "\n",
    "The image below shows the various elements required to use Elastic Deep Learning Serivce. In this notebook we will step through each of these elements in more detail. Through this process you will offload your code to a WML Accelerator cluster, monitor the running job, retrieve the output and debug any issues seen. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/5_running_job.png) is also available.\n",
    "\n",
    "![overall](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/5_running_job.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to your code\n",
    "[Back to top](#Contents)\n",
    "\n",
    "In this section we will take PyTorch model and make the relevant changes required. An overview of these changes can be seen in the diagram below. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/2_code_adaptations.png) is also available.\n",
    "\n",
    "![code](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/2_code_adaptations.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key changes to your code in order to use elastic distributed training are the following:\n",
    "- Importing libraries and setting up environment variables\n",
    "- Data loading function for elastic distributed training \n",
    "- Extract parameters for training\n",
    "- Replace training and testing loops with the loop equivalents for elastic distributed training\n",
    "\n",
    "For the purpose of this tutorial we train RestNet50 model with Elastic Distributed Training (EDT).\n",
    "\n",
    "See the blog associated with this notebook with more detailed explanation of the above changes.\n",
    "https://developer.ibm.com/articles/elastic-distributed-training-edt-in-watson-machine-learning-accelerator/\n",
    "\n",
    "\n",
    "Your modified code should be made available in a directory which also contains the elastic distributed training helper scripts: `edtcallback.py`, `emetrics.py` and `elog.py`. See more information about the Elastic Distributed Training API in [IBM Documentation](https://www.ibm.com/docs/en/wmla/1.2.3?topic=learning-elastic-distributed-training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir = f'/tmp/resnet-wmla' \n",
    "model_main = f'elastic-main.py'\n",
    "model_callback = f'edtcallback.py'\n",
    "model_elog = f'elog.py'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet50 model: elastic-main.py. This is the main file that is required by the elastic distributed training engine. It acts as the program main entrance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/resnet-wmla/elastic-main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_main}\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from callbacks import Callback\n",
    "from fabric_model import FabricModel\n",
    "from edtcallback import EDTLoggerCallback\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "## Define model and extract training parameters\n",
    "def get_max_worker():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='EDT Example')\n",
    "    parser.add_argument('--numWorker', type=int, default='16', help='input the max number ')\n",
    "    parser.add_argument('--gpuPerWorker', type=int, default='1', help='input the path of initial weight file')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    num_worker = args.numWorker * args.gpuPerWorker\n",
    "    return num_worker\n",
    "\n",
    "BATCH_SIZE_PER_DEVICE = 64\n",
    "NUM_EPOCHS = 2\n",
    "MAX_NUM_WORKERS = get_max_worker()\n",
    "\n",
    "START_LEARNING_RATE = 0.4\n",
    "LR_STEP_SIZE = 30\n",
    "LR_GAMMA = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "## Define dataset location \n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "if DATA_DIR is None:\n",
    "    DATA_DIR = '/tmp'\n",
    "print(\"DATA_DIR: \" + DATA_DIR)\n",
    "TRAIN_DATA = DATA_DIR + \"/cifar10\"\n",
    "TEST_DATA = DATA_DIR + \"/cifar10\"\n",
    "\n",
    "\n",
    "## <Xue Yin>  Documentation of Callback function\n",
    "class LRScheduleCallback(Callback):\n",
    "    def __init__(self, step_size, gamma):\n",
    "        super(LRScheduleCallback, self).__init__()\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        if (epoch != 0) and (epoch % self.step_size == 0):\n",
    "            for param_group in self.params['optimizer'].param_groups:\n",
    "                param_group['lr'] *= self.gamma\n",
    "\n",
    "        print(\"LRScheduleCallback epoch={}, learning_rate={}\".format(epoch,\n",
    "              self.params['optimizer'].param_groups[0]['lr']))\n",
    "\n",
    "## Data loading function for EDT\n",
    "def getDatasets():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    return (torchvision.datasets.CIFAR10(root=TRAIN_DATA, train=True, download=True, transform=transform_train),\n",
    "            torchvision.datasets.CIFAR10(root=TEST_DATA, train=False, download=True, transform=transform_test))\n",
    "\n",
    "def custom_train(model, data, eva, train_loader, fn_args):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    opt = model.get_optimizer()\n",
    "    opt.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    cri = model.get_loss_function()\n",
    "    loss = cri(outputs, labels)\n",
    "    loss.backward()\n",
    "    acc = eva(outputs, labels)\n",
    "    return acc, loss\n",
    "\n",
    "def custom_test(model, test_iter, fn_args):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    cri = model.get_loss_function()\n",
    "    valid_loss = 0.0\n",
    "    counter = 0\n",
    "    for(inputs, labels) in test_iter:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = cri(output, labels)\n",
    "        valid_loss += loss.item()\n",
    "        counter += 1\n",
    "    valid_loss /= counter\n",
    "    return valid_loss\n",
    "\n",
    "def main(model_type):\n",
    "    print('==> Building model..' + str(model_type))\n",
    "    model = models.__dict__[model_type]()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=START_LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    loss_function = F.cross_entropy\n",
    "    \n",
    "#     edt_m = FabricModel(model, getDatasets, loss_function, optimizer, enable_onnx=True, fn_step_train=custom_train, \n",
    "#                         fn_test=custom_test, user_callback=[LRScheduleCallback(LR_STEP_SIZE, LR_GAMMA)], driver_logger=EDTLoggerCallback())\n",
    "    edt_m = FabricModel(model, getDatasets, loss_function, optimizer, enable_onnx=True, fn_step_train=custom_train, \n",
    "                        fn_test=custom_test, driver_logger=EDTLoggerCallback())\n",
    "    print('==> epochs:' + str(NUM_EPOCHS) + ', batchsize:' + str(BATCH_SIZE_PER_DEVICE) + ', engines_number:' + str(MAX_NUM_WORKERS))\n",
    "    edt_m.train(NUM_EPOCHS, BATCH_SIZE_PER_DEVICE, MAX_NUM_WORKERS, num_dataloader_threads=4, validation_freq=10, checkpoint_freq=0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(\"resnet50\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDT helper scripts: edtcallback.py\n",
    "The edtcallback.py scripts counts model loss and accuracy and logs them to the driver log."

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/resnet-wmla/edtcallback.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_callback}\n",
    "#! /usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from callbacks import LoggerCallback\n",
    "from emetrics import EMetrics\n",
    "from elog import ELog\n",
    "\n",
    "'''\n",
    "    EDTLoggerCallback class define LoggerCallback to trigger Elog.\n",
    "'''\n",
    "\n",
    "class EDTLoggerCallback(LoggerCallback):\n",
    "    def __init__(self):\n",
    "        self.gs =0\n",
    "\n",
    "    def log_train_metrics(self, loss, acc, completed_batch,  worker=0):\n",
    "        acc = acc/100.0\n",
    "        self.gs += 1\n",
    "        with EMetrics.open() as em:\n",
    "            em.record(EMetrics.TEST_GROUP,completed_batch,{'loss': loss, 'accuracy': acc})\n",
    "        with ELog.open() as log:\n",
    "            log.recordTrain(\"Train\", completed_batch, self.gs, loss, acc, worker)\n",
    "\n",
    "    def log_test_metrics(self, loss, acc, completed_batch, worker=0):\n",
    "        acc = acc/100.0\n",
    "        with ELog.open() as log:\n",
    "            log.recordTest(\"Test\", loss, acc, worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDT helper scripts: elog.py.\n",
    "The elog.py script defines the path and content of the training and test log.\n"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/resnet-wmla/elog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_elog}\n",
    "import time\n",
    "import os\n",
    "\n",
    "'''\n",
    "    ELog class define the path and content of train and test log.\n",
    "'''\n",
    "\n",
    "class ELog(object):\n",
    "\n",
    "    def __init__(self,subId,f):\n",
    "        if \"TRAINING_ID\" in os.environ:\n",
    "            self.trainingId = os.environ[\"TRAINING_ID\"]\n",
    "        elif \"DLI_EXECID\" in os.environ:\n",
    "            self.trainingId = os.environ[\"DLI_EXECID\"]\n",
    "        else:\n",
    "            self.trainingId = \"\"\n",
    "        self.subId = subId\n",
    "        self.f = f\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, tb):\n",
    "        self.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def open(subId=None):\n",
    "        if \"LOG_DIR\" in os.environ:\n",
    "            folder = os.environ[\"LOG_DIR\"]\n",
    "        elif \"JOB_STATE_DIR\" in os.environ:\n",
    "            folder = os.path.join(os.environ[\"JOB_STATE_DIR\"],\"logs\")\n",
    "        else:\n",
    "            folder = \"/tmp\"\n",
    "\n",
    "        if subId is not None:\n",
    "            folder = os.path.join(folder, subId)\n",
    "\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        f = open(os.path.join(folder, \"stdout\"), \"a\")\n",
    "        return ELog(subId,f)\n",
    "\n",
    "    def recordText(self,text):\n",
    "        timestr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        timestr = \"[\"+ timestr + \"]\"\n",
    "        if self.f:\n",
    "            self.f.write(timestr + \" \" + text + \"\\n\")\n",
    "            self.f.flush()\n",
    "\n",
    "    def recordTrain(self,title,iteration,global_steps,loss,accuracy,worker):\n",
    "        text = title\n",
    "        text = text + \",\tTimestamp: \" + str(int(round(time.time() * 1000)))\n",
    "        text = text + \",\tGlobal steps: \" + str(global_steps)\n",
    "        text = text + \",\tIteration: \" + str(iteration)\n",
    "        text = text + \",\tLoss: \" + str(float('%.5f' % loss) )\n",
    "        text = text + \",\tAccuracy: \" + str(float('%.5f' % accuracy) )\n",
    "        self.recordText(text)\n",
    "\n",
    "    def recordTest(self,title,loss,accuracy,worker):\n",
    "        text = title\n",
    "        text = text + \",\tTimestamp: \" + str(int(round(time.time() * 1000)))\n",
    "        text = text + \",\tLoss: \" + str(float('%.5f' % loss) )\n",
    "        text = text + \",\tAccuracy: \" + str(float('%.5f' % accuracy) )\n",
    "        self.recordText(text)\n",
    "\n",
    "    def close(self):\n",
    "        if self.f:\n",
    "            self.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Package model files for training\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [24, 8.0]\n",
    "#import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "#Package the updated model files into a tar file ending with `.modelDir.tar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tempFile: /var/folders/5n/bsvbwc4x2pv391y0zqg1b22c0000gn/T/tmpzy2uun2u.modelDir.tar\n"
     ]
    }
   ],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "\n",
    "MODEL_DIR_SUFFIX = \".modelDir.tar\"\n",
    "tempFile = tempfile.mktemp(MODEL_DIR_SUFFIX)\n",
    "make_tarfile(tempFile, model_dir)\n",
    "\n",
    "print(\" tempFile: \" + tempFile)\n",
    "files = {'file': open(tempFile, 'rb')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up API endpoint and log on\n",
    "[Back to top](#Contents)\n",
    "\n",
    "In this section we set up the API endpoint which will be used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
 "1. Source the environment:\n",
    "\n",
    "$EGO_TOP is the installation directory.  The default value is /opt/ibm/spectrumcomputing. \n",
    "\n",
    "```\n",
    ". $EGO_TOP/profile.platform",
    "\n",
   "```\n",
 "2. Log in to the WML Accelerator clsuter:\n",
    "\n",
    "```\n",
    "egosh user logon -u <wmla_user>\n",
    "Logged on successfully\n",
    "\n",
    "```\n",
    "\n",
    "3. Retrieve the Conductor Rest API port:\n",
    "\n",
    "```\n",
    "egosh client view |grep -A 3 ASCD_REST_BASE_URL_1\n",
    "CLIENT NAME: ASCD_REST_BASE_URL_1\n",
    "DESCRIPTION: http://<WMLA-server>:8280/platform/rest/\n",
    "\n",
    "```\n",
    "\n",
    "4.  Retrieve the Deep Learning Impact (DLI) Rest API port:\n",
    "\n",
    "```\n",
    "egosh client view |grep -A 3 DLPD_REST_BASE_URL_1\n",
    "CLIENT NAME: DLPD_REST_BASE_URL_1\n",
    "DESCRIPTION: http://<WMLA-server>:9280/platform/rest/\n",
    "\n",
    "```\n",
    "\n",
    "Note that the port numbers in your URL will depend on whether SSL has been enabled or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job via API\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Now we need to structure our API job submission. There are various elements to this process as seen in the diagram below. Note that **this** Jupyter notebook is the one referred to below. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/4_api_setup.png) is also available.\n",
    "\n",
    "![code](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/4_api_setup.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections use the Watson ML Accelerator API to complete the various tasks required. \n",
    "We have provided examples of a number of tasks but for more information, refer to the documentation for additional details at to see more details and sample output. \n",
    "\n",
    "- https://www.ibm.com/docs/en/wmla/1.2.3?topic=api-deeplearning\n",
    "- https://www.ibm.com/docs/en/spectrum-conductor/2.5.0?topic=reference-restful-api-references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [24, 8.0]\n",
    "#import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ma1gpu11.ma.platformlab.ibm.com:28433/platform/rest/conductor/v1\n",
      "https://ma1gpu11.ma.platformlab.ibm.com:11573/platform/rest/deeplearning/v1\n"
     ]
    }
   ],
   "source": [
    "# Environment details:\n",
    "\n",
    "protocol = 'https'\n",
    "\n",
    "#master_host = 'colonia05.platform'\n",
    "#master_host = 'baremetal03.ibm-ibm-platform-lab-team-test-account.cloud'\n",
    "master_host = 'ma1gpu11.ma.platformlab.ibm.com'\n",
    "\n",
    "dli_rest_port = '11573'\n",
    "sc_rest_port = '28433'\n",
    "\n",
    "\n",
    "sc_rest_url =  protocol+'://'+master_host+':'+sc_rest_port+'/platform/rest/conductor/v1'\n",
    "dl_rest_url = protocol+'://'+master_host+':'+dli_rest_port+'/platform/rest/deeplearning/v1'\n",
    "\n",
    "print (sc_rest_url)\n",
    "print (dl_rest_url)\n",
    "# User login details\n",
    "\n",
    "wmla_user = 'Admin'\n",
    "wmla_pwd = 'Admin'\n",
    "\n",
    "\n",
    "myauth = (wmla_user, wmla_pwd)\n",
    "\n",
    "# Spark instance group details\n",
    "#sig_name = '**** ADD HERE ****'\n",
    "#sigName = 'ClaimsDamageDetection-IG'\n",
    "sigName = 'elasticsig'\n",
    "\n",
    "# REST call variables\n",
    "headers = {'Accept': 'application/json'}\n",
    "\n",
    "\n",
    "#startTuneUrl='%s://%s:%s/platform/rest/deeplearning/v1/hypersearch' % (protocol, master_host, dli_rest_port)\n",
    "#sc_rest_url ='%s://%s:%d/platform/rest/conductor/v1' % (protocol, hostname, conductorport)\n",
    "\n",
    "req = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log on\n",
    "\n",
    "\n",
    "Obtain login session tokens to be used for session authentication within the RESTful API. Tokens are valid for 8 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logon succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelvinlui/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(sc_rest_url+'/auth/logon', verify=False, auth=myauth, headers=headers) \n",
    "\n",
    "if r.ok:\n",
    "    print ('\\nLogon succeeded')\n",
    "    \n",
    "else: \n",
    "    print('\\nLogon failed with code={}, {}'. format(r.status_code, r.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check deep learning framework details\n",
    "\n",
    "Check what framework plugins are available and see example execution commands.  In this demonstration we will use **edtPyTorch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"edtTensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Tensorflow - IBM Elastic Distributed Training (EDT)\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start edtTensorflow <connection-options> --ig <ig> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\"\n",
      "        ],\n",
      "        \"frameworkVersion\": \"2.4.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"PyTorch\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start PyTorch <connection-options> --ig <ig> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\"\n",
      "        ],\n",
      "        \"frameworkVersion\": \"1.7.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"disttensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Distributed TensorFlow\",\n",
      "            \"Instead of passing parameters such as ps_hosts, worker_hosts, specify --numPs\",\n",
      "            \"as in example below. Parameter servers (ps) and worker hosts will be allocated\",\n",
      "            \"dynamically.\",\n",
      "            \"Use --gpuPerWorker flag to specify number of GPUs per worker.\",\n",
      "            \"The maximum number of worker is 2.\",\n",
      "            \"The maximum number of GPUs per worker is 2.\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start disttensorflow <connection-options> --ig <ig> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py --numPs 1 --numWorker 1 --gpuPerWorker 1\",\n",
      "            \"$ python dlicmd.py --exec-start disttensorflow <connection-options> --ig <ig> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py --numPs 1 --numWorker 2 --gpuPerWorker 1\"\n",
      "        ],\n",
      "        \"distributeStrategy\": \"MultiWorkerMirroredStrategy\",\n",
      "        \"numPs\": 1,\n",
      "        \"frameworkVersion\": \"2.4.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"tensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Single-node TensorFlow\",\n",
      "            \"NOTES:\",\n",
      "            \"- Since DLI manages GPU allocation, if you explicitly assign devices using\",\n",
      "            \"  calls such as `tf.device`, you should use Tensorflow configuration flag\",\n",
      "            \"  `allow_soft_placement=True`\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start tensorflow <connection-options> --ig <ig> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\"\n",
      "        ],\n",
      "        \"frameworkVersion\": \"2.4.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"edtPyTorch\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"PyTorch - IBM Elastic Distributed Training (EDT)\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start edtPyTorch <connection-options> --ig <ig> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\"\n",
      "        ],\n",
      "        \"frameworkVersion\": \"1.7.1\"\n",
      "    }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelvinlui/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(dl_rest_url+'/execs/frameworks', auth=myauth, headers=headers, verify=False).json()\n",
    "# Using the raw json, easier to see the examples given\n",
    "print(json.dumps(r, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments for API call\n",
    "Equivalent of flags used if running command directly on WML Accelerator CLI, including:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: --exec-start edtPyTorch         --cs-datastore-meta type=fs,data_path=pytorch-mnist         --model-dir resnet-wmla         --edt-options maxWorkers=2         --model-main elastic-main.py         \n"
     ]
    }
   ],
   "source": [
    "framework_name = 'edtPyTorch' # DL Framework to use, from list given above\n",
    "dataset_location = 'pytorch-mnist' # relative path of your data set under $DLI_DATA_FS\n",
    "local_dir_containing_your_code = 'resnet-wmla'\n",
    "number_of_GPU = '2' # number of GPUs for elastic distribution\n",
    "name_of_your_code_file = 'elastic-main.py' # Main model file as opened locally above\n",
    "\n",
    "\n",
    "args = '--exec-start {} \\\n",
    "        --cs-datastore-meta type=fs,data_path={} \\\n",
    "        --model-dir {} \\\n",
    "        --edt-options maxWorkers={} \\\n",
    "        --model-main {} \\\n",
    "        '.format(framework_name, dataset_location, local_dir_containing_your_code, number_of_GPU, name_of_your_code_file)\n",
    "\n",
    "print (\"args: \" + args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelvinlui/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model submitted successfully \\Driver ID: driver-20210304172400-0008-82fb7400-e86e-417d-8ab3-30b2cbe07287\n",
      "Exec ID: Admin-2518647360655711-224680126\n",
      "SIG ID: a8ec6f8f-224f-4584-8b57-df97955fb657\n"
     ]
    }
   ],
   "source": [
    "r = requests.post(dl_rest_url+'/execs?sigName='+sigName+'&args='+args, files=files,\n",
    "                  auth=myauth, headers=headers, verify=False)\n",
    "\n",
    "if r.ok:\n",
    "    exec_id = r.json()['id']\n",
    "    sig_id = r.json()['sigId']\n",
    "    driver_id = r.json()['submissionId']\n",
    "    print ('\\nModel submitted successfully \\Driver ID: {}'.format(driver_id))\n",
    "    print ('Exec ID: {}'.format(exec_id))\n",
    "    print ('SIG ID: {}'.format(sig_id))\n",
    "else: \n",
    "    print('\\nModel submission failed with code={}, {}'. format(r.status_code, r.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor running job\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Once the job is submitted successfully we can monitor the running job. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelvinlui/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Driver ID</th>\n",
       "      <th>State</th>\n",
       "      <th>Run duration (mins)</th>\n",
       "      <th>GPU slots</th>\n",
       "      <th>Total GPU memory used</th>\n",
       "      <th>Total GPU utilsation (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>driver-20210304172015-0007-b2ff28b2-1bca-4f49-a505-f9048e77b417</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>3.19625</td>\n",
       "      <td>2</td>\n",
       "      <td>15618</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Driver ID    State  \\\n",
       "0  driver-20210304172015-0007-b2ff28b2-1bca-4f49-a505-f9048e77b417  RUNNING   \n",
       "\n",
       "   Run duration (mins)  GPU slots Total GPU memory used  \\\n",
       "0              3.19625          2                 15618   \n",
       "\n",
       "  Total GPU utilsation (%)   \n",
       "0                     166.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status of all RUNNING jobs in SIG (rerun cell to refresh)\n",
    "\n",
    "\n",
    "\n",
    "monitor = []\n",
    "monitor_output = []\n",
    "\n",
    "r = requests.get(sc_rest_url+'/instances/'+sig_id+'/applications?state=RUNNING', \n",
    "                auth=myauth, headers=headers, verify=False).json()\n",
    "\n",
    "\n",
    "       \n",
    "if (len(r) == 0):\n",
    "    print ('No jobs running')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Filter out the relevant information\n",
    "    monitor.append([(\n",
    "        job['driver']['id'],\n",
    "        job['driver']['state'],\n",
    "        job['apprunduration'],\n",
    "        job['gpuslots'],\n",
    "        job['gpumemused']['total'],\n",
    "        job['gpudevutil']['total'],\n",
    "    ) for job in r])\n",
    "\n",
    "    monitor_output = pd.DataFrame([item for monitor in monitor for item in monitor])\n",
    "    monitor_output.columns = [\n",
    "        'Driver ID', \n",
    "        'State', \n",
    "        'Run duration (mins)',\n",
    "        'GPU slots',\n",
    "        'Total GPU memory used',\n",
    "        'Total GPU utilsation (%) ',\n",
    "    ]\n",
    "    \n",
    "    for job in r:\n",
    "        executors = job['executors']\n",
    "        \n",
    "\n",
    "monitor_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve output and saved models\n",
    "[Back to top](#Contents)\n",
    "\n",
    "After the job completes then we can retrieve the output, logs and saved models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output:  Retrieve Training Metric\n",
    "\n",
    "<span style='color:deeppink'>**TODO:** **Junfeng/Xue Zhou/Shan Gao, Please debug the empty returning string.\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelvinlui/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>global_steps</th>\n",
       "      <th>iteration</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>timestamp2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1614896710223</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.11528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:10.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1614896710429</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.11385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:10.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1614896710633</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.11369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:10.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1614896710835</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.11350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:10.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1614896711039</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.11371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:11.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1614896711245</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.11431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:11.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1614896711448</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.11437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:11.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1614896711653</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.11424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:11.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1614896711857</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.11422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:11.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1614896712061</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.11409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-04 22:25:12.061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp  global_steps  iteration     loss  accuracy  \\\n",
       "0  1614896710223             1          2  0.11528       0.0   \n",
       "1  1614896710429             2          3  0.11385       0.0   \n",
       "2  1614896710633             3          4  0.11369       0.0   \n",
       "3  1614896710835             4          5  0.11350       0.0   \n",
       "4  1614896711039             5          6  0.11371       0.0   \n",
       "5  1614896711245             6          7  0.11431       0.0   \n",
       "6  1614896711448             7          8  0.11437       0.0   \n",
       "7  1614896711653             8          9  0.11424       0.0   \n",
       "8  1614896711857             9         10  0.11422       0.0   \n",
       "9  1614896712061            10         11  0.11409       0.0   \n",
       "\n",
       "               timestamp2  \n",
       "0 2021-03-04 22:25:10.223  \n",
       "1 2021-03-04 22:25:10.429  \n",
       "2 2021-03-04 22:25:10.633  \n",
       "3 2021-03-04 22:25:10.835  \n",
       "4 2021-03-04 22:25:11.039  \n",
       "5 2021-03-04 22:25:11.245  \n",
       "6 2021-03-04 22:25:11.448  \n",
       "7 2021-03-04 22:25:11.653  \n",
       "8 2021-03-04 22:25:11.857  \n",
       "9 2021-03-04 22:25:12.061  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(dl_rest_url+'/execs/'+exec_id+'/log', auth=myauth, headers=headers, verify=False).json()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "cols = ['timestamp','global_steps','iteration','loss','accuracy']\n",
    "final_data = pd.read_csv(StringIO(r.replace(':',',')), \n",
    "                 usecols=[4,6,8,10,12], \n",
    "                 names=cols)\n",
    "final_data['timestamp2'] = final_data.timestamp.apply(pd.to_datetime, unit='ms')\n",
    "final_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [24, 8.0]\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize = (15,10))\n",
    "\n",
    "\n",
    "sns.lineplot(final_data.timestamp2,final_data.global_steps, color=\"g\", ax=axes[0,0])\n",
    "axes[0,0].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "axes[0,0].set_xlabel(\"Time\")\n",
    "\n",
    "sns.lineplot(final_data.global_steps,final_data.loss, color=\"r\", ax=axes[0,1])\n",
    "sns.lineplot(final_data.global_steps,final_data.accuracy, color=\"b\", ax=axes[1,0])\n",
    "\n",
    "axes[1,1].axes.get_xaxis().set_visible(False)\n",
    "axes[1,1].axes.get_yaxis().set_visible(False)\n",
    "axes[1,1].text(0.05, 0.8, 'ID: '+exec_id, size=14)\n",
    "#axes[1,1].text(0.05, 0.65, 'SIG: '+sig_name, size=14)\n",
    "#axes[1,1].text(0.05, 0.55, 'Status: '+status, size=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ma1gpu11.ma.platformlab.ibm.com:11573/platform/rest/deeplearning/v1/execs/Admin-2518647360655711-224680126/result\n",
      "Save model:  /tmp/resnet-wmla/Admin-2518647360655711-224680126.zip\n"
     ]
    }
   ],
   "source": [
    "# Get model from training job - downloads zip file (with progress bar) of saved model to directory local to this notebook\n",
    "# (note that you need to save model in your code using the environment variable for location)\n",
    "\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "\n",
    "# r = requests.get(dl_rest_url+'/execs/'+exec_id+'/result', auth=myauth, stream=True, headers=headers, verify=False)\n",
    "\n",
    "# total_size = int(r.headers.get('Content-Disposition').split('size=')[1])\n",
    "# block_size = 1024 #1 Kibibyte\n",
    "# t=tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "# with open('model.zip', 'wb') as f:\n",
    "#     for data in r.iter_content(block_size):\n",
    "#         t.update(len(data))\n",
    "#         f.write(data)\n",
    "# t.close()\n",
    "\n",
    "def download_trained_model(exec_id) :\n",
    "\n",
    "    from IPython.display import display, FileLink\n",
    "\n",
    "    # save result file\n",
    "   # commonHeaders3={'accept': 'application/octet-stream', 'X-Auth-Token': access_token}\n",
    "    headers={'Accept': 'application/octet-stream'}\n",
    "    execURL = dl_rest_url  +'/execs/'+ exec_id + '/result'\n",
    "    res = req.get(execURL, headers=headers, verify=False, auth=myauth)\n",
    "    print (execURL)\n",
    "\n",
    "    tmpfile =  model_dir + '/' + exec_id +'.zip'\n",
    "    print ('Save model: ', tmpfile )\n",
    "    with open(tmpfile,'wb') as f:\n",
    "        f.write(res.content)\n",
    "        f.close()\n",
    "        \n",
    "download_trained_model(exec_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging any issues\n",
    "[Back to top](#Contents)\n",
    "\n",
    "In the case where you have issues during the process detailed above, there are a number of detailed logs that you can view to understand what is happening on the WML Accelerator cluster.\n",
    "\n",
    "WML Accelerator leverages Spark architecture for distributing Maching Learning and Deep Learning jobs.  In Spark,  when an item of processing has to be done, there is a driver process that is in charge of taking the users code and converting it into a set of multiple tasks. There are also executor processes, each operating on a separate node in the cluster, that are in charge of running the tasks, as delegated by the driver.\n",
    "\n",
    "You can monitor ML/DL application activity,  performance and resource usage in Driver Log & Executor Log.\n",
    "- Driver Log captures issues related to dependencies and environment variable,  for example,  missing dataset or invalid execution parameter flags.\n",
    "- Executor Log records ML/DL training process. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Training Driver Stdout Log\n",
    "\n",
    "\n",
    "<span style='color:deeppink'>**TODO:** Xue Yin/Xue Zhou, Please debug the empty returning string.\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve_stdout_url: https://ma1gpu11.ma.platformlab.ibm.com:11573/platform/rest/deeplearning/v1/execs/Admin-2518647360655711-224680126/log?logType=outlog\n",
      "('[2021-03-04 17:25:10] Train,\\tTimestamp: 1614896710223,\\tGlobal steps: 1,\\t'\n",
      " 'Iteration: 2,\\tLoss: 0.11528,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:10] Train,\\tTimestamp: 1614896710429,\\tGlobal steps: 2,\\t'\n",
      " 'Iteration: 3,\\tLoss: 0.11385,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:10] Train,\\tTimestamp: 1614896710633,\\tGlobal steps: 3,\\t'\n",
      " 'Iteration: 4,\\tLoss: 0.11369,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:10] Train,\\tTimestamp: 1614896710835,\\tGlobal steps: 4,\\t'\n",
      " 'Iteration: 5,\\tLoss: 0.1135,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:11] Train,\\tTimestamp: 1614896711039,\\tGlobal steps: 5,\\t'\n",
      " 'Iteration: 6,\\tLoss: 0.11371,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:11] Train,\\tTimestamp: 1614896711245,\\tGlobal steps: 6,\\t'\n",
      " 'Iteration: 7,\\tLoss: 0.11431,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:11] Train,\\tTimestamp: 1614896711448,\\tGlobal steps: 7,\\t'\n",
      " 'Iteration: 8,\\tLoss: 0.11437,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:11] Train,\\tTimestamp: 1614896711653,\\tGlobal steps: 8,\\t'\n",
      " 'Iteration: 9,\\tLoss: 0.11424,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:11] Train,\\tTimestamp: 1614896711857,\\tGlobal steps: 9,\\t'\n",
      " 'Iteration: 10,\\tLoss: 0.11422,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:12] Train,\\tTimestamp: 1614896712061,\\tGlobal steps: 10,\\t'\n",
      " 'Iteration: 11,\\tLoss: 0.11409,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:12] Train,\\tTimestamp: 1614896712266,\\tGlobal steps: 11,\\t'\n",
      " 'Iteration: 12,\\tLoss: 0.11403,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:12] Train,\\tTimestamp: 1614896712471,\\tGlobal steps: 12,\\t'\n",
      " 'Iteration: 13,\\tLoss: 0.11406,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:12] Train,\\tTimestamp: 1614896712675,\\tGlobal steps: 13,\\t'\n",
      " 'Iteration: 14,\\tLoss: 0.11418,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:12] Train,\\tTimestamp: 1614896712878,\\tGlobal steps: 14,\\t'\n",
      " 'Iteration: 15,\\tLoss: 0.11441,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:13] Train,\\tTimestamp: 1614896713081,\\tGlobal steps: 15,\\t'\n",
      " 'Iteration: 16,\\tLoss: 0.11454,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:13] Train,\\tTimestamp: 1614896713491,\\tGlobal steps: 16,\\t'\n",
      " 'Iteration: 17,\\tLoss: 0.11452,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:25:13] Train,\\tTimestamp: 1614896713701,\\tGlobal steps: 17,\\t'\n",
      " 'Iteration: 18,\\tLoss: 0.12849,\\tAccuracy: 6e-05\\n'\n",
      " '[2021-03-04 17:25:13] Train,\\tTimestamp: 1614896713905,\\tGlobal steps: 18,\\t'\n",
      " 'Iteration: 19,\\tLoss: 0.14302,\\tAccuracy: 0.0001\\n'\n",
      " '[2021-03-04 17:25:14] Train,\\tTimestamp: 1614896714108,\\tGlobal steps: 19,\\t'\n",
      " 'Iteration: 20,\\tLoss: 0.15505,\\tAccuracy: 0.00012\\n'\n",
      " '[2021-03-04 17:25:14] Train,\\tTimestamp: 1614896714313,\\tGlobal steps: 20,\\t'\n",
      " 'Iteration: 21,\\tLoss: 0.16435,\\tAccuracy: 0.00018\\n'\n",
      " '[2021-03-04 17:25:14] Train,\\tTimestamp: 1614896714517,\\tGlobal steps: 21,\\t'\n",
      " 'Iteration: 22,\\tLoss: 0.17019,\\tAccuracy: 0.00024\\n'\n",
      " '[2021-03-04 17:25:14] Train,\\tTimestamp: 1614896714722,\\tGlobal steps: 22,\\t'\n",
      " 'Iteration: 23,\\tLoss: 0.17681,\\tAccuracy: 0.00029\\n'\n",
      " '[2021-03-04 17:25:14] Train,\\tTimestamp: 1614896714926,\\tGlobal steps: 23,\\t'\n",
      " 'Iteration: 24,\\tLoss: 0.18525,\\tAccuracy: 0.00029\\n'\n",
      " '[2021-03-04 17:25:15] Train,\\tTimestamp: 1614896715129,\\tGlobal steps: 24,\\t'\n",
      " 'Iteration: 25,\\tLoss: 0.19339,\\tAccuracy: 0.00032\\n'\n",
      " '[2021-03-04 17:25:15] Train,\\tTimestamp: 1614896715333,\\tGlobal steps: 25,\\t'\n",
      " 'Iteration: 26,\\tLoss: 0.20134,\\tAccuracy: 0.00033\\n'\n",
      " '[2021-03-04 17:25:15] Train,\\tTimestamp: 1614896715540,\\tGlobal steps: 26,\\t'\n",
      " 'Iteration: 27,\\tLoss: 0.20631,\\tAccuracy: 0.00036\\n'\n",
      " '[2021-03-04 17:25:15] Train,\\tTimestamp: 1614896715744,\\tGlobal steps: 27,\\t'\n",
      " 'Iteration: 28,\\tLoss: 0.21311,\\tAccuracy: 0.00039\\n'\n",
      " '[2021-03-04 17:25:15] Train,\\tTimestamp: 1614896715947,\\tGlobal steps: 28,\\t'\n",
      " 'Iteration: 29,\\tLoss: 0.21863,\\tAccuracy: 0.00041\\n'\n",
      " '[2021-03-04 17:25:16] Train,\\tTimestamp: 1614896716151,\\tGlobal steps: 29,\\t'\n",
      " 'Iteration: 30,\\tLoss: 0.22339,\\tAccuracy: 0.00043\\n'\n",
      " '[2021-03-04 17:25:16] Train,\\tTimestamp: 1614896716355,\\tGlobal steps: 30,\\t'\n",
      " 'Iteration: 31,\\tLoss: 0.22875,\\tAccuracy: 0.00044\\n'\n",
      " '[2021-03-04 17:25:16] Train,\\tTimestamp: 1614896716560,\\tGlobal steps: 31,\\t'\n",
      " 'Iteration: 32,\\tLoss: 0.23177,\\tAccuracy: 0.00047\\n'\n",
      " '[2021-03-04 17:25:16] Train,\\tTimestamp: 1614896716784,\\tGlobal steps: 32,\\t'\n",
      " 'Iteration: 33,\\tLoss: 0.23684,\\tAccuracy: 0.00047\\n'\n",
      " '[2021-03-04 17:25:16] Train,\\tTimestamp: 1614896716988,\\tGlobal steps: 33,\\t'\n",
      " 'Iteration: 34,\\tLoss: 0.27912,\\tAccuracy: 0.00049\\n'\n",
      " '[2021-03-04 17:25:17] Train,\\tTimestamp: 1614896717193,\\tGlobal steps: 34,\\t'\n",
      " 'Iteration: 35,\\tLoss: 0.28376,\\tAccuracy: 0.00051\\n'\n",
      " '[2021-03-04 17:25:17] Train,\\tTimestamp: 1614896717396,\\tGlobal steps: 35,\\t'\n",
      " 'Iteration: 36,\\tLoss: 0.28717,\\tAccuracy: 0.00051\\n'\n",
      " '[2021-03-04 17:25:17] Train,\\tTimestamp: 1614896717599,\\tGlobal steps: 36,\\t'\n",
      " 'Iteration: 37,\\tLoss: 0.31352,\\tAccuracy: 0.00054\\n'\n",
      " '[2021-03-04 17:25:17] Train,\\tTimestamp: 1614896717803,\\tGlobal steps: 37,\\t'\n",
      " 'Iteration: 38,\\tLoss: 0.31444,\\tAccuracy: 0.00058\\n'\n",
      " '[2021-03-04 17:25:18] Train,\\tTimestamp: 1614896718007,\\tGlobal steps: 38,\\t'\n",
      " 'Iteration: 39,\\tLoss: 0.31474,\\tAccuracy: 0.00058\\n'\n",
      " '[2021-03-04 17:25:18] Train,\\tTimestamp: 1614896718210,\\tGlobal steps: 39,\\t'\n",
      " 'Iteration: 40,\\tLoss: 0.32793,\\tAccuracy: 0.0006\\n'\n",
      " '[2021-03-04 17:25:18] Train,\\tTimestamp: 1614896718414,\\tGlobal steps: 40,\\t'\n",
      " 'Iteration: 41,\\tLoss: 0.33855,\\tAccuracy: 0.00061\\n'\n",
      " '[2021-03-04 17:25:18] Train,\\tTimestamp: 1614896718617,\\tGlobal steps: 41,\\t'\n",
      " 'Iteration: 42,\\tLoss: 0.33675,\\tAccuracy: 0.00062\\n'\n",
      " '[2021-03-04 17:25:18] Train,\\tTimestamp: 1614896718821,\\tGlobal steps: 42,\\t'\n",
      " 'Iteration: 43,\\tLoss: 0.33556,\\tAccuracy: 0.00064\\n'\n",
      " '[2021-03-04 17:25:19] Train,\\tTimestamp: 1614896719025,\\tGlobal steps: 43,\\t'\n",
      " 'Iteration: 44,\\tLoss: 0.36341,\\tAccuracy: 0.00065\\n'\n",
      " '[2021-03-04 17:25:19] Train,\\tTimestamp: 1614896719228,\\tGlobal steps: 44,\\t'\n",
      " 'Iteration: 45,\\tLoss: 0.36676,\\tAccuracy: 0.00067\\n'\n",
      " '[2021-03-04 17:25:19] Train,\\tTimestamp: 1614896719431,\\tGlobal steps: 45,\\t'\n",
      " 'Iteration: 46,\\tLoss: 0.39415,\\tAccuracy: 0.00068\\n'\n",
      " '[2021-03-04 17:25:19] Train,\\tTimestamp: 1614896719635,\\tGlobal steps: 46,\\t'\n",
      " 'Iteration: 47,\\tLoss: 0.39003,\\tAccuracy: 0.00071\\n'\n",
      " '[2021-03-04 17:25:19] Train,\\tTimestamp: 1614896719838,\\tGlobal steps: 47,\\t'\n",
      " 'Iteration: 48,\\tLoss: 0.40682,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:20] Train,\\tTimestamp: 1614896720062,\\tGlobal steps: 48,\\t'\n",
      " 'Iteration: 49,\\tLoss: 0.41734,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:20] Train,\\tTimestamp: 1614896720266,\\tGlobal steps: 49,\\t'\n",
      " 'Iteration: 50,\\tLoss: 0.42482,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:20] Train,\\tTimestamp: 1614896720469,\\tGlobal steps: 50,\\t'\n",
      " 'Iteration: 51,\\tLoss: 0.43169,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:20] Train,\\tTimestamp: 1614896720672,\\tGlobal steps: 51,\\t'\n",
      " 'Iteration: 52,\\tLoss: 0.43374,\\tAccuracy: 0.00073\\n'\n",
      " '[2021-03-04 17:25:20] Train,\\tTimestamp: 1614896720876,\\tGlobal steps: 52,\\t'\n",
      " 'Iteration: 53,\\tLoss: 0.4363,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:21] Train,\\tTimestamp: 1614896721079,\\tGlobal steps: 53,\\t'\n",
      " 'Iteration: 54,\\tLoss: 0.44293,\\tAccuracy: 0.00073\\n'\n",
      " '[2021-03-04 17:25:21] Train,\\tTimestamp: 1614896721282,\\tGlobal steps: 54,\\t'\n",
      " 'Iteration: 55,\\tLoss: 0.44839,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:21] Train,\\tTimestamp: 1614896721486,\\tGlobal steps: 55,\\t'\n",
      " 'Iteration: 56,\\tLoss: 0.4553,\\tAccuracy: 0.00073\\n'\n",
      " '[2021-03-04 17:25:21] Train,\\tTimestamp: 1614896721690,\\tGlobal steps: 56,\\t'\n",
      " 'Iteration: 57,\\tLoss: 0.45983,\\tAccuracy: 0.00071\\n'\n",
      " '[2021-03-04 17:25:21] Train,\\tTimestamp: 1614896721893,\\tGlobal steps: 57,\\t'\n",
      " 'Iteration: 58,\\tLoss: 0.46258,\\tAccuracy: 0.00071\\n'\n",
      " '[2021-03-04 17:25:22] Train,\\tTimestamp: 1614896722095,\\tGlobal steps: 58,\\t'\n",
      " 'Iteration: 59,\\tLoss: 0.4675,\\tAccuracy: 0.00071\\n'\n",
      " '[2021-03-04 17:25:22] Train,\\tTimestamp: 1614896722297,\\tGlobal steps: 59,\\t'\n",
      " 'Iteration: 60,\\tLoss: 0.47589,\\tAccuracy: 0.00071\\n'\n",
      " '[2021-03-04 17:25:22] Train,\\tTimestamp: 1614896722500,\\tGlobal steps: 60,\\t'\n",
      " 'Iteration: 61,\\tLoss: 0.47635,\\tAccuracy: 0.0007\\n'\n",
      " '[2021-03-04 17:25:22] Train,\\tTimestamp: 1614896722703,\\tGlobal steps: 61,\\t'\n",
      " 'Iteration: 62,\\tLoss: 0.47839,\\tAccuracy: 0.00071\\n'\n",
      " '[2021-03-04 17:25:22] Train,\\tTimestamp: 1614896722905,\\tGlobal steps: 62,\\t'\n",
      " 'Iteration: 63,\\tLoss: 0.47811,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:23] Train,\\tTimestamp: 1614896723109,\\tGlobal steps: 63,\\t'\n",
      " 'Iteration: 64,\\tLoss: 0.48685,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:23] Train,\\tTimestamp: 1614896723332,\\tGlobal steps: 64,\\t'\n",
      " 'Iteration: 65,\\tLoss: 0.48933,\\tAccuracy: 0.00073\\n'\n",
      " '[2021-03-04 17:25:23] Train,\\tTimestamp: 1614896723536,\\tGlobal steps: 65,\\t'\n",
      " 'Iteration: 66,\\tLoss: 0.48842,\\tAccuracy: 0.00072\\n'\n",
      " '[2021-03-04 17:25:23] Train,\\tTimestamp: 1614896723739,\\tGlobal steps: 66,\\t'\n",
      " 'Iteration: 67,\\tLoss: 0.49309,\\tAccuracy: 0.00074\\n'\n",
      " '[2021-03-04 17:25:23] Train,\\tTimestamp: 1614896723944,\\tGlobal steps: 67,\\t'\n",
      " 'Iteration: 68,\\tLoss: 0.4939,\\tAccuracy: 0.00074\\n'\n",
      " '[2021-03-04 17:25:24] Train,\\tTimestamp: 1614896724147,\\tGlobal steps: 68,\\t'\n",
      " 'Iteration: 69,\\tLoss: 0.49149,\\tAccuracy: 0.00075\\n'\n",
      " '[2021-03-04 17:25:24] Train,\\tTimestamp: 1614896724350,\\tGlobal steps: 69,\\t'\n",
      " 'Iteration: 70,\\tLoss: 0.49092,\\tAccuracy: 0.00075\\n'\n",
      " '[2021-03-04 17:25:24] Train,\\tTimestamp: 1614896724553,\\tGlobal steps: 70,\\t'\n",
      " 'Iteration: 71,\\tLoss: 0.48978,\\tAccuracy: 0.00075\\n'\n",
      " '[2021-03-04 17:25:24] Train,\\tTimestamp: 1614896724756,\\tGlobal steps: 71,\\t'\n",
      " 'Iteration: 72,\\tLoss: 0.48846,\\tAccuracy: 0.00076\\n'\n",
      " '[2021-03-04 17:25:24] Train,\\tTimestamp: 1614896724959,\\tGlobal steps: 72,\\t'\n",
      " 'Iteration: 73,\\tLoss: 0.48861,\\tAccuracy: 0.00077\\n'\n",
      " '[2021-03-04 17:25:25] Train,\\tTimestamp: 1614896725161,\\tGlobal steps: 73,\\t'\n",
      " 'Iteration: 74,\\tLoss: 0.49016,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:25:25] Train,\\tTimestamp: 1614896725365,\\tGlobal steps: 74,\\t'\n",
      " 'Iteration: 75,\\tLoss: 0.49048,\\tAccuracy: 0.00077\\n'\n",
      " '[2021-03-04 17:25:25] Train,\\tTimestamp: 1614896725566,\\tGlobal steps: 75,\\t'\n",
      " 'Iteration: 76,\\tLoss: 0.49984,\\tAccuracy: 0.00077\\n'\n",
      " '[2021-03-04 17:25:25] Train,\\tTimestamp: 1614896725769,\\tGlobal steps: 76,\\t'\n",
      " 'Iteration: 77,\\tLoss: 0.51126,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:25:25] Train,\\tTimestamp: 1614896725972,\\tGlobal steps: 77,\\t'\n",
      " 'Iteration: 78,\\tLoss: 0.51692,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:25:26] Train,\\tTimestamp: 1614896726174,\\tGlobal steps: 78,\\t'\n",
      " 'Iteration: 79,\\tLoss: 0.52372,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:25:26] Train,\\tTimestamp: 1614896726378,\\tGlobal steps: 79,\\t'\n",
      " 'Iteration: 80,\\tLoss: 0.52298,\\tAccuracy: 0.00079\\n'\n",
      " '[2021-03-04 17:25:26] Train,\\tTimestamp: 1614896726601,\\tGlobal steps: 80,\\t'\n",
      " 'Iteration: 81,\\tLoss: 0.5281,\\tAccuracy: 0.00079\\n'\n",
      " '[2021-03-04 17:25:26] Train,\\tTimestamp: 1614896726803,\\tGlobal steps: 81,\\t'\n",
      " 'Iteration: 82,\\tLoss: 0.5294,\\tAccuracy: 0.00079\\n'\n",
      " '[2021-03-04 17:25:27] Train,\\tTimestamp: 1614896727009,\\tGlobal steps: 82,\\t'\n",
      " 'Iteration: 83,\\tLoss: 0.53215,\\tAccuracy: 0.00079\\n'\n",
      " '[2021-03-04 17:25:27] Train,\\tTimestamp: 1614896727212,\\tGlobal steps: 83,\\t'\n",
      " 'Iteration: 84,\\tLoss: 0.54017,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:25:27] Train,\\tTimestamp: 1614896727414,\\tGlobal steps: 84,\\t'\n",
      " 'Iteration: 85,\\tLoss: 0.54171,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:25:27] Train,\\tTimestamp: 1614896727616,\\tGlobal steps: 85,\\t'\n",
      " 'Iteration: 86,\\tLoss: 0.54539,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:25:27] Train,\\tTimestamp: 1614896727818,\\tGlobal steps: 86,\\t'\n",
      " 'Iteration: 87,\\tLoss: 0.5466,\\tAccuracy: 0.00079\\n'\n",
      " '[2021-03-04 17:25:28] Train,\\tTimestamp: 1614896728021,\\tGlobal steps: 87,\\t'\n",
      " 'Iteration: 88,\\tLoss: 0.54265,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:25:28] Train,\\tTimestamp: 1614896728225,\\tGlobal steps: 88,\\t'\n",
      " 'Iteration: 89,\\tLoss: 0.53885,\\tAccuracy: 0.0008\\n'\n",
      " '[2021-03-04 17:25:28] Train,\\tTimestamp: 1614896728427,\\tGlobal steps: 89,\\t'\n",
      " 'Iteration: 90,\\tLoss: 0.5376,\\tAccuracy: 0.0008\\n'\n",
      " '[2021-03-04 17:25:28] Train,\\tTimestamp: 1614896728631,\\tGlobal steps: 90,\\t'\n",
      " 'Iteration: 91,\\tLoss: 0.53869,\\tAccuracy: 0.0008\\n'\n",
      " '[2021-03-04 17:25:28] Train,\\tTimestamp: 1614896728834,\\tGlobal steps: 91,\\t'\n",
      " 'Iteration: 92,\\tLoss: 0.54083,\\tAccuracy: 0.0008\\n'\n",
      " '[2021-03-04 17:25:29] Train,\\tTimestamp: 1614896729036,\\tGlobal steps: 92,\\t'\n",
      " 'Iteration: 93,\\tLoss: 0.53794,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:29] Train,\\tTimestamp: 1614896729239,\\tGlobal steps: 93,\\t'\n",
      " 'Iteration: 94,\\tLoss: 0.53621,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:29] Train,\\tTimestamp: 1614896729442,\\tGlobal steps: 94,\\t'\n",
      " 'Iteration: 95,\\tLoss: 0.53322,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:29] Train,\\tTimestamp: 1614896729645,\\tGlobal steps: 95,\\t'\n",
      " 'Iteration: 96,\\tLoss: 0.53417,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:29] Train,\\tTimestamp: 1614896729867,\\tGlobal steps: 96,\\t'\n",
      " 'Iteration: 97,\\tLoss: 0.53758,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:30] Train,\\tTimestamp: 1614896730070,\\tGlobal steps: 97,\\t'\n",
      " 'Iteration: 98,\\tLoss: 0.53333,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:30] Train,\\tTimestamp: 1614896730273,\\tGlobal steps: 98,\\t'\n",
      " 'Iteration: 99,\\tLoss: 0.52897,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:30] Train,\\tTimestamp: 1614896730476,\\tGlobal steps: 99,\\t'\n",
      " 'Iteration: 100,\\tLoss: 0.52516,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:30] Train,\\tTimestamp: 1614896730679,\\tGlobal steps: 100,\\t'\n",
      " 'Iteration: 101,\\tLoss: 0.52152,\\tAccuracy: 0.0008\\n'\n",
      " '[2021-03-04 17:25:30] Train,\\tTimestamp: 1614896730882,\\tGlobal steps: 101,\\t'\n",
      " 'Iteration: 102,\\tLoss: 0.5177,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:31] Train,\\tTimestamp: 1614896731085,\\tGlobal steps: 102,\\t'\n",
      " 'Iteration: 103,\\tLoss: 0.51373,\\tAccuracy: 0.00082\\n'\n",
      " '[2021-03-04 17:25:31] Train,\\tTimestamp: 1614896731287,\\tGlobal steps: 103,\\t'\n",
      " 'Iteration: 104,\\tLoss: 0.50985,\\tAccuracy: 0.00081\\n'\n",
      " '[2021-03-04 17:25:31] Train,\\tTimestamp: 1614896731489,\\tGlobal steps: 104,\\t'\n",
      " 'Iteration: 105,\\tLoss: 0.50612,\\tAccuracy: 0.00082\\n'\n",
      " '[2021-03-04 17:25:31] Train,\\tTimestamp: 1614896731692,\\tGlobal steps: 105,\\t'\n",
      " 'Iteration: 106,\\tLoss: 0.50283,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:25:31] Train,\\tTimestamp: 1614896731895,\\tGlobal steps: 106,\\t'\n",
      " 'Iteration: 107,\\tLoss: 0.49952,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:25:32] Train,\\tTimestamp: 1614896732098,\\tGlobal steps: 107,\\t'\n",
      " 'Iteration: 108,\\tLoss: 0.4966,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:25:32] Train,\\tTimestamp: 1614896732301,\\tGlobal steps: 108,\\t'\n",
      " 'Iteration: 109,\\tLoss: 0.49369,\\tAccuracy: 0.00082\\n'\n",
      " '[2021-03-04 17:25:32] Train,\\tTimestamp: 1614896732504,\\tGlobal steps: 109,\\t'\n",
      " 'Iteration: 110,\\tLoss: 0.49394,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:25:32] Train,\\tTimestamp: 1614896732706,\\tGlobal steps: 110,\\t'\n",
      " 'Iteration: 111,\\tLoss: 0.49151,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:25:32] Train,\\tTimestamp: 1614896732908,\\tGlobal steps: 111,\\t'\n",
      " 'Iteration: 112,\\tLoss: 0.48861,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:25:33] Train,\\tTimestamp: 1614896733131,\\tGlobal steps: 112,\\t'\n",
      " 'Iteration: 113,\\tLoss: 0.48634,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:25:33] Train,\\tTimestamp: 1614896733333,\\tGlobal steps: 113,\\t'\n",
      " 'Iteration: 114,\\tLoss: 0.48334,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:25:33] Train,\\tTimestamp: 1614896733536,\\tGlobal steps: 114,\\t'\n",
      " 'Iteration: 115,\\tLoss: 0.48046,\\tAccuracy: 0.00084\\n'\n",
      " '[2021-03-04 17:25:33] Train,\\tTimestamp: 1614896733739,\\tGlobal steps: 115,\\t'\n",
      " 'Iteration: 116,\\tLoss: 0.48422,\\tAccuracy: 0.00084\\n'\n",
      " '[2021-03-04 17:25:33] Train,\\tTimestamp: 1614896733959,\\tGlobal steps: 116,\\t'\n",
      " 'Iteration: 117,\\tLoss: 0.48089,\\tAccuracy: 0.00085\\n'\n",
      " '[2021-03-04 17:25:34] Train,\\tTimestamp: 1614896734161,\\tGlobal steps: 117,\\t'\n",
      " 'Iteration: 118,\\tLoss: 0.47761,\\tAccuracy: 0.00085\\n'\n",
      " '[2021-03-04 17:25:34] Train,\\tTimestamp: 1614896734363,\\tGlobal steps: 118,\\t'\n",
      " 'Iteration: 119,\\tLoss: 0.47497,\\tAccuracy: 0.00085\\n'\n",
      " '[2021-03-04 17:25:34] Train,\\tTimestamp: 1614896734564,\\tGlobal steps: 119,\\t'\n",
      " 'Iteration: 120,\\tLoss: 0.47182,\\tAccuracy: 0.00085\\n'\n",
      " '[2021-03-04 17:25:34] Train,\\tTimestamp: 1614896734767,\\tGlobal steps: 120,\\t'\n",
      " 'Iteration: 121,\\tLoss: 0.46877,\\tAccuracy: 0.00086\\n'\n",
      " '[2021-03-04 17:25:34] Train,\\tTimestamp: 1614896734970,\\tGlobal steps: 121,\\t'\n",
      " 'Iteration: 122,\\tLoss: 0.46578,\\tAccuracy: 0.00085\\n'\n",
      " '[2021-03-04 17:25:35] Train,\\tTimestamp: 1614896735173,\\tGlobal steps: 122,\\t'\n",
      " 'Iteration: 123,\\tLoss: 0.46288,\\tAccuracy: 0.00085\\n'\n",
      " '[2021-03-04 17:25:35] Train,\\tTimestamp: 1614896735375,\\tGlobal steps: 123,\\t'\n",
      " 'Iteration: 124,\\tLoss: 0.46034,\\tAccuracy: 0.00085\\n'\n",
      " '[2021-03-04 17:25:35] Train,\\tTimestamp: 1614896735578,\\tGlobal steps: 124,\\t'\n",
      " 'Iteration: 125,\\tLoss: 0.45767,\\tAccuracy: 0.00086\\n'\n",
      " '[2021-03-04 17:25:35] Train,\\tTimestamp: 1614896735781,\\tGlobal steps: 125,\\t'\n",
      " 'Iteration: 126,\\tLoss: 0.45543,\\tAccuracy: 0.00086\\n'\n",
      " '[2021-03-04 17:25:35] Train,\\tTimestamp: 1614896735985,\\tGlobal steps: 126,\\t'\n",
      " 'Iteration: 127,\\tLoss: 0.45267,\\tAccuracy: 0.00086\\n'\n",
      " '[2021-03-04 17:25:36] Train,\\tTimestamp: 1614896736188,\\tGlobal steps: 127,\\t'\n",
      " 'Iteration: 128,\\tLoss: 0.4499,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:36] Train,\\tTimestamp: 1614896736412,\\tGlobal steps: 128,\\t'\n",
      " 'Iteration: 129,\\tLoss: 0.44743,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:36] Train,\\tTimestamp: 1614896736616,\\tGlobal steps: 129,\\t'\n",
      " 'Iteration: 130,\\tLoss: 0.44541,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:36] Train,\\tTimestamp: 1614896736818,\\tGlobal steps: 130,\\t'\n",
      " 'Iteration: 131,\\tLoss: 0.44349,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:37] Train,\\tTimestamp: 1614896737021,\\tGlobal steps: 131,\\t'\n",
      " 'Iteration: 132,\\tLoss: 0.44134,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:37] Train,\\tTimestamp: 1614896737223,\\tGlobal steps: 132,\\t'\n",
      " 'Iteration: 133,\\tLoss: 0.44214,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:37] Train,\\tTimestamp: 1614896737426,\\tGlobal steps: 133,\\t'\n",
      " 'Iteration: 134,\\tLoss: 0.43983,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:37] Train,\\tTimestamp: 1614896737629,\\tGlobal steps: 134,\\t'\n",
      " 'Iteration: 135,\\tLoss: 0.43774,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:37] Train,\\tTimestamp: 1614896737832,\\tGlobal steps: 135,\\t'\n",
      " 'Iteration: 136,\\tLoss: 0.43511,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:38] Train,\\tTimestamp: 1614896738035,\\tGlobal steps: 136,\\t'\n",
      " 'Iteration: 137,\\tLoss: 0.43308,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:38] Train,\\tTimestamp: 1614896738237,\\tGlobal steps: 137,\\t'\n",
      " 'Iteration: 138,\\tLoss: 0.43063,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:38] Train,\\tTimestamp: 1614896738439,\\tGlobal steps: 138,\\t'\n",
      " 'Iteration: 139,\\tLoss: 0.42888,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:38] Train,\\tTimestamp: 1614896738644,\\tGlobal steps: 139,\\t'\n",
      " 'Iteration: 140,\\tLoss: 0.42635,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:38] Train,\\tTimestamp: 1614896738845,\\tGlobal steps: 140,\\t'\n",
      " 'Iteration: 141,\\tLoss: 0.42599,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:39] Train,\\tTimestamp: 1614896739047,\\tGlobal steps: 141,\\t'\n",
      " 'Iteration: 142,\\tLoss: 0.42417,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:25:39] Train,\\tTimestamp: 1614896739251,\\tGlobal steps: 142,\\t'\n",
      " 'Iteration: 143,\\tLoss: 0.42196,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:39] Train,\\tTimestamp: 1614896739454,\\tGlobal steps: 143,\\t'\n",
      " 'Iteration: 144,\\tLoss: 0.42022,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:39] Train,\\tTimestamp: 1614896739677,\\tGlobal steps: 144,\\t'\n",
      " 'Iteration: 145,\\tLoss: 0.41834,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:39] Train,\\tTimestamp: 1614896739879,\\tGlobal steps: 145,\\t'\n",
      " 'Iteration: 146,\\tLoss: 0.41596,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:40] Train,\\tTimestamp: 1614896740083,\\tGlobal steps: 146,\\t'\n",
      " 'Iteration: 147,\\tLoss: 0.41355,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:40] Train,\\tTimestamp: 1614896740286,\\tGlobal steps: 147,\\t'\n",
      " 'Iteration: 148,\\tLoss: 0.41134,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:25:40] Train,\\tTimestamp: 1614896740489,\\tGlobal steps: 148,\\t'\n",
      " 'Iteration: 149,\\tLoss: 0.4093,\\tAccuracy: 0.00089\\n'\n",
      " '[2021-03-04 17:25:40] Train,\\tTimestamp: 1614896740692,\\tGlobal steps: 149,\\t'\n",
      " 'Iteration: 150,\\tLoss: 0.40721,\\tAccuracy: 0.00089\\n'\n",
      " '[2021-03-04 17:25:40] Train,\\tTimestamp: 1614896740895,\\tGlobal steps: 150,\\t'\n",
      " 'Iteration: 151,\\tLoss: 0.40547,\\tAccuracy: 0.00089\\n'\n",
      " '[2021-03-04 17:25:41] Train,\\tTimestamp: 1614896741097,\\tGlobal steps: 151,\\t'\n",
      " 'Iteration: 152,\\tLoss: 0.40357,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:25:41] Train,\\tTimestamp: 1614896741300,\\tGlobal steps: 152,\\t'\n",
      " 'Iteration: 153,\\tLoss: 0.40164,\\tAccuracy: 0.00089\\n'\n",
      " '[2021-03-04 17:25:41] Train,\\tTimestamp: 1614896741502,\\tGlobal steps: 153,\\t'\n",
      " 'Iteration: 154,\\tLoss: 0.39967,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:25:41] Train,\\tTimestamp: 1614896741705,\\tGlobal steps: 154,\\t'\n",
      " 'Iteration: 155,\\tLoss: 0.39786,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:25:41] Train,\\tTimestamp: 1614896741909,\\tGlobal steps: 155,\\t'\n",
      " 'Iteration: 156,\\tLoss: 0.39603,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:25:42] Train,\\tTimestamp: 1614896742111,\\tGlobal steps: 156,\\t'\n",
      " 'Iteration: 157,\\tLoss: 0.39396,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:25:42] Train,\\tTimestamp: 1614896742313,\\tGlobal steps: 157,\\t'\n",
      " 'Iteration: 158,\\tLoss: 0.39205,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:25:42] Train,\\tTimestamp: 1614896742516,\\tGlobal steps: 158,\\t'\n",
      " 'Iteration: 159,\\tLoss: 0.39038,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:42] Train,\\tTimestamp: 1614896742719,\\tGlobal steps: 159,\\t'\n",
      " 'Iteration: 160,\\tLoss: 0.38878,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:42] Train,\\tTimestamp: 1614896742942,\\tGlobal steps: 160,\\t'\n",
      " 'Iteration: 161,\\tLoss: 0.38721,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:43] Train,\\tTimestamp: 1614896743145,\\tGlobal steps: 161,\\t'\n",
      " 'Iteration: 162,\\tLoss: 0.38528,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:43] Train,\\tTimestamp: 1614896743348,\\tGlobal steps: 162,\\t'\n",
      " 'Iteration: 163,\\tLoss: 0.38487,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:43] Train,\\tTimestamp: 1614896743551,\\tGlobal steps: 163,\\t'\n",
      " 'Iteration: 164,\\tLoss: 0.3834,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:43] Train,\\tTimestamp: 1614896743753,\\tGlobal steps: 164,\\t'\n",
      " 'Iteration: 165,\\tLoss: 0.38165,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:43] Train,\\tTimestamp: 1614896743955,\\tGlobal steps: 165,\\t'\n",
      " 'Iteration: 166,\\tLoss: 0.38142,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:44] Train,\\tTimestamp: 1614896744157,\\tGlobal steps: 166,\\t'\n",
      " 'Iteration: 167,\\tLoss: 0.37961,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:44] Train,\\tTimestamp: 1614896744360,\\tGlobal steps: 167,\\t'\n",
      " 'Iteration: 168,\\tLoss: 0.37802,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:44] Train,\\tTimestamp: 1614896744562,\\tGlobal steps: 168,\\t'\n",
      " 'Iteration: 169,\\tLoss: 0.37628,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:44] Train,\\tTimestamp: 1614896744764,\\tGlobal steps: 169,\\t'\n",
      " 'Iteration: 170,\\tLoss: 0.37478,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:44] Train,\\tTimestamp: 1614896744965,\\tGlobal steps: 170,\\t'\n",
      " 'Iteration: 171,\\tLoss: 0.37323,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:45] Train,\\tTimestamp: 1614896745167,\\tGlobal steps: 171,\\t'\n",
      " 'Iteration: 172,\\tLoss: 0.37174,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:45] Train,\\tTimestamp: 1614896745369,\\tGlobal steps: 172,\\t'\n",
      " 'Iteration: 173,\\tLoss: 0.36997,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:45] Train,\\tTimestamp: 1614896745571,\\tGlobal steps: 173,\\t'\n",
      " 'Iteration: 174,\\tLoss: 0.36847,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:45] Train,\\tTimestamp: 1614896745773,\\tGlobal steps: 174,\\t'\n",
      " 'Iteration: 175,\\tLoss: 0.36697,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:45] Train,\\tTimestamp: 1614896745975,\\tGlobal steps: 175,\\t'\n",
      " 'Iteration: 176,\\tLoss: 0.36545,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:46] Train,\\tTimestamp: 1614896746196,\\tGlobal steps: 176,\\t'\n",
      " 'Iteration: 177,\\tLoss: 0.36378,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:46] Train,\\tTimestamp: 1614896746398,\\tGlobal steps: 177,\\t'\n",
      " 'Iteration: 178,\\tLoss: 0.36401,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:46] Train,\\tTimestamp: 1614896746599,\\tGlobal steps: 178,\\t'\n",
      " 'Iteration: 179,\\tLoss: 0.36243,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:46] Train,\\tTimestamp: 1614896746801,\\tGlobal steps: 179,\\t'\n",
      " 'Iteration: 180,\\tLoss: 0.3608,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:47] Train,\\tTimestamp: 1614896747003,\\tGlobal steps: 180,\\t'\n",
      " 'Iteration: 181,\\tLoss: 0.35923,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:47] Train,\\tTimestamp: 1614896747206,\\tGlobal steps: 181,\\t'\n",
      " 'Iteration: 182,\\tLoss: 0.35779,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:47] Train,\\tTimestamp: 1614896747408,\\tGlobal steps: 182,\\t'\n",
      " 'Iteration: 183,\\tLoss: 0.35627,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:47] Train,\\tTimestamp: 1614896747610,\\tGlobal steps: 183,\\t'\n",
      " 'Iteration: 184,\\tLoss: 0.35469,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:47] Train,\\tTimestamp: 1614896747813,\\tGlobal steps: 184,\\t'\n",
      " 'Iteration: 185,\\tLoss: 0.35327,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:48] Train,\\tTimestamp: 1614896748015,\\tGlobal steps: 185,\\t'\n",
      " 'Iteration: 186,\\tLoss: 0.35187,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:48] Train,\\tTimestamp: 1614896748218,\\tGlobal steps: 186,\\t'\n",
      " 'Iteration: 187,\\tLoss: 0.3505,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:48] Train,\\tTimestamp: 1614896748420,\\tGlobal steps: 187,\\t'\n",
      " 'Iteration: 188,\\tLoss: 0.34911,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:48] Train,\\tTimestamp: 1614896748621,\\tGlobal steps: 188,\\t'\n",
      " 'Iteration: 189,\\tLoss: 0.34956,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:48] Train,\\tTimestamp: 1614896748836,\\tGlobal steps: 189,\\t'\n",
      " 'Iteration: 190,\\tLoss: 0.34825,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:49] Train,\\tTimestamp: 1614896749038,\\tGlobal steps: 190,\\t'\n",
      " 'Iteration: 191,\\tLoss: 0.34693,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:49] Train,\\tTimestamp: 1614896749240,\\tGlobal steps: 191,\\t'\n",
      " 'Iteration: 192,\\tLoss: 0.34556,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:49] Train,\\tTimestamp: 1614896749461,\\tGlobal steps: 192,\\t'\n",
      " 'Iteration: 193,\\tLoss: 0.34416,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:49] Train,\\tTimestamp: 1614896749663,\\tGlobal steps: 193,\\t'\n",
      " 'Iteration: 194,\\tLoss: 0.34315,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:49] Train,\\tTimestamp: 1614896749865,\\tGlobal steps: 194,\\t'\n",
      " 'Iteration: 195,\\tLoss: 0.34179,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:50] Train,\\tTimestamp: 1614896750067,\\tGlobal steps: 195,\\t'\n",
      " 'Iteration: 196,\\tLoss: 0.34032,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:50] Train,\\tTimestamp: 1614896750268,\\tGlobal steps: 196,\\t'\n",
      " 'Iteration: 197,\\tLoss: 0.33889,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:25:50] Train,\\tTimestamp: 1614896750471,\\tGlobal steps: 197,\\t'\n",
      " 'Iteration: 198,\\tLoss: 0.33765,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:50] Train,\\tTimestamp: 1614896750673,\\tGlobal steps: 198,\\t'\n",
      " 'Iteration: 199,\\tLoss: 0.33634,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:50] Train,\\tTimestamp: 1614896750875,\\tGlobal steps: 199,\\t'\n",
      " 'Iteration: 200,\\tLoss: 0.3351,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:51] Train,\\tTimestamp: 1614896751077,\\tGlobal steps: 200,\\t'\n",
      " 'Iteration: 201,\\tLoss: 0.33373,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:51] Train,\\tTimestamp: 1614896751280,\\tGlobal steps: 201,\\t'\n",
      " 'Iteration: 202,\\tLoss: 0.33235,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:51] Train,\\tTimestamp: 1614896751481,\\tGlobal steps: 202,\\t'\n",
      " 'Iteration: 203,\\tLoss: 0.33116,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:51] Train,\\tTimestamp: 1614896751682,\\tGlobal steps: 203,\\t'\n",
      " 'Iteration: 204,\\tLoss: 0.32995,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:51] Train,\\tTimestamp: 1614896751884,\\tGlobal steps: 204,\\t'\n",
      " 'Iteration: 205,\\tLoss: 0.32875,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:52] Train,\\tTimestamp: 1614896752085,\\tGlobal steps: 205,\\t'\n",
      " 'Iteration: 206,\\tLoss: 0.3276,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:52] Train,\\tTimestamp: 1614896752285,\\tGlobal steps: 206,\\t'\n",
      " 'Iteration: 207,\\tLoss: 0.32644,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:52] Train,\\tTimestamp: 1614896752487,\\tGlobal steps: 207,\\t'\n",
      " 'Iteration: 208,\\tLoss: 0.32535,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:25:52] Train,\\tTimestamp: 1614896752707,\\tGlobal steps: 208,\\t'\n",
      " 'Iteration: 209,\\tLoss: 0.32414,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:52] Train,\\tTimestamp: 1614896752908,\\tGlobal steps: 209,\\t'\n",
      " 'Iteration: 210,\\tLoss: 0.32291,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:53] Train,\\tTimestamp: 1614896753109,\\tGlobal steps: 210,\\t'\n",
      " 'Iteration: 211,\\tLoss: 0.3217,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:53] Train,\\tTimestamp: 1614896753311,\\tGlobal steps: 211,\\t'\n",
      " 'Iteration: 212,\\tLoss: 0.32065,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:53] Train,\\tTimestamp: 1614896753513,\\tGlobal steps: 212,\\t'\n",
      " 'Iteration: 213,\\tLoss: 0.31938,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:53] Train,\\tTimestamp: 1614896753714,\\tGlobal steps: 213,\\t'\n",
      " 'Iteration: 214,\\tLoss: 0.31898,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:53] Train,\\tTimestamp: 1614896753916,\\tGlobal steps: 214,\\t'\n",
      " 'Iteration: 215,\\tLoss: 0.31789,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:54] Train,\\tTimestamp: 1614896754117,\\tGlobal steps: 215,\\t'\n",
      " 'Iteration: 216,\\tLoss: 0.31678,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:54] Train,\\tTimestamp: 1614896754319,\\tGlobal steps: 216,\\t'\n",
      " 'Iteration: 217,\\tLoss: 0.31573,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:54] Train,\\tTimestamp: 1614896754520,\\tGlobal steps: 217,\\t'\n",
      " 'Iteration: 218,\\tLoss: 0.31469,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:54] Train,\\tTimestamp: 1614896754720,\\tGlobal steps: 218,\\t'\n",
      " 'Iteration: 219,\\tLoss: 0.31354,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:54] Train,\\tTimestamp: 1614896754921,\\tGlobal steps: 219,\\t'\n",
      " 'Iteration: 220,\\tLoss: 0.31251,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:55] Train,\\tTimestamp: 1614896755123,\\tGlobal steps: 220,\\t'\n",
      " 'Iteration: 221,\\tLoss: 0.31148,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:55] Train,\\tTimestamp: 1614896755325,\\tGlobal steps: 221,\\t'\n",
      " 'Iteration: 222,\\tLoss: 0.31055,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:55] Train,\\tTimestamp: 1614896755526,\\tGlobal steps: 222,\\t'\n",
      " 'Iteration: 223,\\tLoss: 0.30945,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:55] Train,\\tTimestamp: 1614896755728,\\tGlobal steps: 223,\\t'\n",
      " 'Iteration: 224,\\tLoss: 0.30844,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:55] Train,\\tTimestamp: 1614896755949,\\tGlobal steps: 224,\\t'\n",
      " 'Iteration: 225,\\tLoss: 0.30749,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:56] Train,\\tTimestamp: 1614896756151,\\tGlobal steps: 225,\\t'\n",
      " 'Iteration: 226,\\tLoss: 0.30643,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:56] Train,\\tTimestamp: 1614896756354,\\tGlobal steps: 226,\\t'\n",
      " 'Iteration: 227,\\tLoss: 0.30528,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:56] Train,\\tTimestamp: 1614896756556,\\tGlobal steps: 227,\\t'\n",
      " 'Iteration: 228,\\tLoss: 0.30427,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:56] Train,\\tTimestamp: 1614896756758,\\tGlobal steps: 228,\\t'\n",
      " 'Iteration: 229,\\tLoss: 0.30325,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:56] Train,\\tTimestamp: 1614896756960,\\tGlobal steps: 229,\\t'\n",
      " 'Iteration: 230,\\tLoss: 0.30229,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:57] Train,\\tTimestamp: 1614896757161,\\tGlobal steps: 230,\\t'\n",
      " 'Iteration: 231,\\tLoss: 0.30198,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:57] Train,\\tTimestamp: 1614896757364,\\tGlobal steps: 231,\\t'\n",
      " 'Iteration: 232,\\tLoss: 0.301,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:57] Train,\\tTimestamp: 1614896757565,\\tGlobal steps: 232,\\t'\n",
      " 'Iteration: 233,\\tLoss: 0.30002,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:57] Train,\\tTimestamp: 1614896757767,\\tGlobal steps: 233,\\t'\n",
      " 'Iteration: 234,\\tLoss: 0.29909,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:57] Train,\\tTimestamp: 1614896757969,\\tGlobal steps: 234,\\t'\n",
      " 'Iteration: 235,\\tLoss: 0.29805,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:58] Train,\\tTimestamp: 1614896758171,\\tGlobal steps: 235,\\t'\n",
      " 'Iteration: 236,\\tLoss: 0.29703,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:58] Train,\\tTimestamp: 1614896758373,\\tGlobal steps: 236,\\t'\n",
      " 'Iteration: 237,\\tLoss: 0.29612,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:25:58] Train,\\tTimestamp: 1614896758575,\\tGlobal steps: 237,\\t'\n",
      " 'Iteration: 238,\\tLoss: 0.29512,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:25:58] Train,\\tTimestamp: 1614896758777,\\tGlobal steps: 238,\\t'\n",
      " 'Iteration: 239,\\tLoss: 0.29413,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:25:58] Train,\\tTimestamp: 1614896758978,\\tGlobal steps: 239,\\t'\n",
      " 'Iteration: 240,\\tLoss: 0.29319,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:25:59] Train,\\tTimestamp: 1614896759202,\\tGlobal steps: 240,\\t'\n",
      " 'Iteration: 241,\\tLoss: 0.29226,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:25:59] Train,\\tTimestamp: 1614896759404,\\tGlobal steps: 241,\\t'\n",
      " 'Iteration: 242,\\tLoss: 0.29135,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:25:59] Train,\\tTimestamp: 1614896759607,\\tGlobal steps: 242,\\t'\n",
      " 'Iteration: 243,\\tLoss: 0.29051,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:25:59] Train,\\tTimestamp: 1614896759810,\\tGlobal steps: 243,\\t'\n",
      " 'Iteration: 244,\\tLoss: 0.28963,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:00] Train,\\tTimestamp: 1614896760016,\\tGlobal steps: 244,\\t'\n",
      " 'Iteration: 245,\\tLoss: 0.28926,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:00] Train,\\tTimestamp: 1614896760219,\\tGlobal steps: 245,\\t'\n",
      " 'Iteration: 246,\\tLoss: 0.28835,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:00] Train,\\tTimestamp: 1614896760422,\\tGlobal steps: 246,\\t'\n",
      " 'Iteration: 247,\\tLoss: 0.28742,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:00] Train,\\tTimestamp: 1614896760625,\\tGlobal steps: 247,\\t'\n",
      " 'Iteration: 248,\\tLoss: 0.28659,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:00] Train,\\tTimestamp: 1614896760827,\\tGlobal steps: 248,\\t'\n",
      " 'Iteration: 249,\\tLoss: 0.28565,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:01] Train,\\tTimestamp: 1614896761031,\\tGlobal steps: 249,\\t'\n",
      " 'Iteration: 250,\\tLoss: 0.2848,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:01] Train,\\tTimestamp: 1614896761236,\\tGlobal steps: 250,\\t'\n",
      " 'Iteration: 251,\\tLoss: 0.28388,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:01] Train,\\tTimestamp: 1614896761439,\\tGlobal steps: 251,\\t'\n",
      " 'Iteration: 252,\\tLoss: 0.28309,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:01] Train,\\tTimestamp: 1614896761643,\\tGlobal steps: 252,\\t'\n",
      " 'Iteration: 253,\\tLoss: 0.28222,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:01] Train,\\tTimestamp: 1614896761847,\\tGlobal steps: 253,\\t'\n",
      " 'Iteration: 254,\\tLoss: 0.28143,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:02] Train,\\tTimestamp: 1614896762050,\\tGlobal steps: 254,\\t'\n",
      " 'Iteration: 255,\\tLoss: 0.28061,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:02] Train,\\tTimestamp: 1614896762253,\\tGlobal steps: 255,\\t'\n",
      " 'Iteration: 256,\\tLoss: 0.27981,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:02] Train,\\tTimestamp: 1614896762482,\\tGlobal steps: 256,\\t'\n",
      " 'Iteration: 257,\\tLoss: 0.27905,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:02] Train,\\tTimestamp: 1614896762685,\\tGlobal steps: 257,\\t'\n",
      " 'Iteration: 258,\\tLoss: 0.27822,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:02] Train,\\tTimestamp: 1614896762888,\\tGlobal steps: 258,\\t'\n",
      " 'Iteration: 259,\\tLoss: 0.27764,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:03] Train,\\tTimestamp: 1614896763092,\\tGlobal steps: 259,\\t'\n",
      " 'Iteration: 260,\\tLoss: 0.27683,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:03] Train,\\tTimestamp: 1614896763296,\\tGlobal steps: 260,\\t'\n",
      " 'Iteration: 261,\\tLoss: 0.27609,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:03] Train,\\tTimestamp: 1614896763499,\\tGlobal steps: 261,\\t'\n",
      " 'Iteration: 262,\\tLoss: 0.2753,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:03] Train,\\tTimestamp: 1614896763703,\\tGlobal steps: 262,\\t'\n",
      " 'Iteration: 263,\\tLoss: 0.27442,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:03] Train,\\tTimestamp: 1614896763907,\\tGlobal steps: 263,\\t'\n",
      " 'Iteration: 264,\\tLoss: 0.27359,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:04] Train,\\tTimestamp: 1614896764111,\\tGlobal steps: 264,\\t'\n",
      " 'Iteration: 265,\\tLoss: 0.27279,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:04] Train,\\tTimestamp: 1614896764314,\\tGlobal steps: 265,\\t'\n",
      " 'Iteration: 266,\\tLoss: 0.27201,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:04] Train,\\tTimestamp: 1614896764517,\\tGlobal steps: 266,\\t'\n",
      " 'Iteration: 267,\\tLoss: 0.27141,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:04] Train,\\tTimestamp: 1614896764721,\\tGlobal steps: 267,\\t'\n",
      " 'Iteration: 268,\\tLoss: 0.27069,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:04] Train,\\tTimestamp: 1614896764924,\\tGlobal steps: 268,\\t'\n",
      " 'Iteration: 269,\\tLoss: 0.2699,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:05] Train,\\tTimestamp: 1614896765126,\\tGlobal steps: 269,\\t'\n",
      " 'Iteration: 270,\\tLoss: 0.26915,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:05] Train,\\tTimestamp: 1614896765330,\\tGlobal steps: 270,\\t'\n",
      " 'Iteration: 271,\\tLoss: 0.26837,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:05] Train,\\tTimestamp: 1614896765533,\\tGlobal steps: 271,\\t'\n",
      " 'Iteration: 272,\\tLoss: 0.26763,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:05] Train,\\tTimestamp: 1614896765761,\\tGlobal steps: 272,\\t'\n",
      " 'Iteration: 273,\\tLoss: 0.26685,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:05] Train,\\tTimestamp: 1614896765965,\\tGlobal steps: 273,\\t'\n",
      " 'Iteration: 274,\\tLoss: 0.26607,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:06] Train,\\tTimestamp: 1614896766169,\\tGlobal steps: 274,\\t'\n",
      " 'Iteration: 275,\\tLoss: 0.26537,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:06] Train,\\tTimestamp: 1614896766374,\\tGlobal steps: 275,\\t'\n",
      " 'Iteration: 276,\\tLoss: 0.26459,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:06] Train,\\tTimestamp: 1614896766577,\\tGlobal steps: 276,\\t'\n",
      " 'Iteration: 277,\\tLoss: 0.26385,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:06] Train,\\tTimestamp: 1614896766781,\\tGlobal steps: 277,\\t'\n",
      " 'Iteration: 278,\\tLoss: 0.2631,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:06] Train,\\tTimestamp: 1614896766985,\\tGlobal steps: 278,\\t'\n",
      " 'Iteration: 279,\\tLoss: 0.26237,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:07] Train,\\tTimestamp: 1614896767188,\\tGlobal steps: 279,\\t'\n",
      " 'Iteration: 280,\\tLoss: 0.26161,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:07] Train,\\tTimestamp: 1614896767391,\\tGlobal steps: 280,\\t'\n",
      " 'Iteration: 281,\\tLoss: 0.26087,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:07] Train,\\tTimestamp: 1614896767595,\\tGlobal steps: 281,\\t'\n",
      " 'Iteration: 282,\\tLoss: 0.26013,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:07] Train,\\tTimestamp: 1614896767798,\\tGlobal steps: 282,\\t'\n",
      " 'Iteration: 283,\\tLoss: 0.25975,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:08] Train,\\tTimestamp: 1614896768002,\\tGlobal steps: 283,\\t'\n",
      " 'Iteration: 284,\\tLoss: 0.25904,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:08] Train,\\tTimestamp: 1614896768205,\\tGlobal steps: 284,\\t'\n",
      " 'Iteration: 285,\\tLoss: 0.25833,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:08] Train,\\tTimestamp: 1614896768409,\\tGlobal steps: 285,\\t'\n",
      " 'Iteration: 286,\\tLoss: 0.25765,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:08] Train,\\tTimestamp: 1614896768612,\\tGlobal steps: 286,\\t'\n",
      " 'Iteration: 287,\\tLoss: 0.25693,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:08] Train,\\tTimestamp: 1614896768816,\\tGlobal steps: 287,\\t'\n",
      " 'Iteration: 288,\\tLoss: 0.25625,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:09] Train,\\tTimestamp: 1614896769057,\\tGlobal steps: 288,\\t'\n",
      " 'Iteration: 289,\\tLoss: 0.25555,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:09] Train,\\tTimestamp: 1614896769259,\\tGlobal steps: 289,\\t'\n",
      " 'Iteration: 290,\\tLoss: 0.25488,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:09] Train,\\tTimestamp: 1614896769463,\\tGlobal steps: 290,\\t'\n",
      " 'Iteration: 291,\\tLoss: 0.25462,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:09] Train,\\tTimestamp: 1614896769666,\\tGlobal steps: 291,\\t'\n",
      " 'Iteration: 292,\\tLoss: 0.25395,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:09] Train,\\tTimestamp: 1614896769868,\\tGlobal steps: 292,\\t'\n",
      " 'Iteration: 293,\\tLoss: 0.25328,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:10] Train,\\tTimestamp: 1614896770072,\\tGlobal steps: 293,\\t'\n",
      " 'Iteration: 294,\\tLoss: 0.25261,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:10] Train,\\tTimestamp: 1614896770276,\\tGlobal steps: 294,\\t'\n",
      " 'Iteration: 295,\\tLoss: 0.25192,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:10] Train,\\tTimestamp: 1614896770479,\\tGlobal steps: 295,\\t'\n",
      " 'Iteration: 296,\\tLoss: 0.25127,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:10] Train,\\tTimestamp: 1614896770682,\\tGlobal steps: 296,\\t'\n",
      " 'Iteration: 297,\\tLoss: 0.25061,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:10] Train,\\tTimestamp: 1614896770886,\\tGlobal steps: 297,\\t'\n",
      " 'Iteration: 298,\\tLoss: 0.24992,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:11] Train,\\tTimestamp: 1614896771089,\\tGlobal steps: 298,\\t'\n",
      " 'Iteration: 299,\\tLoss: 0.24937,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:11] Train,\\tTimestamp: 1614896771292,\\tGlobal steps: 299,\\t'\n",
      " 'Iteration: 300,\\tLoss: 0.24867,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:11] Train,\\tTimestamp: 1614896771495,\\tGlobal steps: 300,\\t'\n",
      " 'Iteration: 301,\\tLoss: 0.24805,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:11] Train,\\tTimestamp: 1614896771699,\\tGlobal steps: 301,\\t'\n",
      " 'Iteration: 302,\\tLoss: 0.24742,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:11] Train,\\tTimestamp: 1614896771903,\\tGlobal steps: 302,\\t'\n",
      " 'Iteration: 303,\\tLoss: 0.24677,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:12] Train,\\tTimestamp: 1614896772107,\\tGlobal steps: 303,\\t'\n",
      " 'Iteration: 304,\\tLoss: 0.24616,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:26:12] Train,\\tTimestamp: 1614896772335,\\tGlobal steps: 304,\\t'\n",
      " 'Iteration: 305,\\tLoss: 0.24554,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:12] Train,\\tTimestamp: 1614896772539,\\tGlobal steps: 305,\\t'\n",
      " 'Iteration: 306,\\tLoss: 0.24487,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:12] Train,\\tTimestamp: 1614896772742,\\tGlobal steps: 306,\\t'\n",
      " 'Iteration: 307,\\tLoss: 0.24422,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:12] Train,\\tTimestamp: 1614896772945,\\tGlobal steps: 307,\\t'\n",
      " 'Iteration: 308,\\tLoss: 0.24361,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:13] Train,\\tTimestamp: 1614896773148,\\tGlobal steps: 308,\\t'\n",
      " 'Iteration: 309,\\tLoss: 0.24299,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:13] Train,\\tTimestamp: 1614896773350,\\tGlobal steps: 309,\\t'\n",
      " 'Iteration: 310,\\tLoss: 0.24239,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:13] Train,\\tTimestamp: 1614896773553,\\tGlobal steps: 310,\\t'\n",
      " 'Iteration: 311,\\tLoss: 0.24182,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:13] Train,\\tTimestamp: 1614896773756,\\tGlobal steps: 311,\\t'\n",
      " 'Iteration: 312,\\tLoss: 0.24122,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:13] Train,\\tTimestamp: 1614896773958,\\tGlobal steps: 312,\\t'\n",
      " 'Iteration: 313,\\tLoss: 0.24063,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:14] Train,\\tTimestamp: 1614896774162,\\tGlobal steps: 313,\\t'\n",
      " 'Iteration: 314,\\tLoss: 0.24,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:14] Train,\\tTimestamp: 1614896774365,\\tGlobal steps: 314,\\t'\n",
      " 'Iteration: 315,\\tLoss: 0.2394,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:14] Train,\\tTimestamp: 1614896774568,\\tGlobal steps: 315,\\t'\n",
      " 'Iteration: 316,\\tLoss: 0.23882,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:14] Train,\\tTimestamp: 1614896774773,\\tGlobal steps: 316,\\t'\n",
      " 'Iteration: 317,\\tLoss: 0.23822,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:14] Train,\\tTimestamp: 1614896774976,\\tGlobal steps: 317,\\t'\n",
      " 'Iteration: 318,\\tLoss: 0.23763,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:15] Train,\\tTimestamp: 1614896775180,\\tGlobal steps: 318,\\t'\n",
      " 'Iteration: 319,\\tLoss: 0.23705,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:15] Train,\\tTimestamp: 1614896775383,\\tGlobal steps: 319,\\t'\n",
      " 'Iteration: 320,\\tLoss: 0.23643,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:15] Train,\\tTimestamp: 1614896775612,\\tGlobal steps: 320,\\t'\n",
      " 'Iteration: 321,\\tLoss: 0.23586,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:15] Train,\\tTimestamp: 1614896775814,\\tGlobal steps: 321,\\t'\n",
      " 'Iteration: 322,\\tLoss: 0.2353,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:16] Train,\\tTimestamp: 1614896776017,\\tGlobal steps: 322,\\t'\n",
      " 'Iteration: 323,\\tLoss: 0.23472,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:16] Train,\\tTimestamp: 1614896776220,\\tGlobal steps: 323,\\t'\n",
      " 'Iteration: 324,\\tLoss: 0.23415,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:16] Train,\\tTimestamp: 1614896776423,\\tGlobal steps: 324,\\t'\n",
      " 'Iteration: 325,\\tLoss: 0.23359,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:16] Train,\\tTimestamp: 1614896776626,\\tGlobal steps: 325,\\t'\n",
      " 'Iteration: 326,\\tLoss: 0.23301,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:16] Train,\\tTimestamp: 1614896776829,\\tGlobal steps: 326,\\t'\n",
      " 'Iteration: 327,\\tLoss: 0.23272,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:17] Train,\\tTimestamp: 1614896777033,\\tGlobal steps: 327,\\t'\n",
      " 'Iteration: 328,\\tLoss: 0.23216,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:17] Train,\\tTimestamp: 1614896777236,\\tGlobal steps: 328,\\t'\n",
      " 'Iteration: 329,\\tLoss: 0.2316,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:17] Train,\\tTimestamp: 1614896777439,\\tGlobal steps: 329,\\t'\n",
      " 'Iteration: 330,\\tLoss: 0.23104,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:17] Train,\\tTimestamp: 1614896777642,\\tGlobal steps: 330,\\t'\n",
      " 'Iteration: 331,\\tLoss: 0.23049,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:17] Train,\\tTimestamp: 1614896777845,\\tGlobal steps: 331,\\t'\n",
      " 'Iteration: 332,\\tLoss: 0.22995,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:18] Train,\\tTimestamp: 1614896778048,\\tGlobal steps: 332,\\t'\n",
      " 'Iteration: 333,\\tLoss: 0.22939,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:18] Train,\\tTimestamp: 1614896778252,\\tGlobal steps: 333,\\t'\n",
      " 'Iteration: 334,\\tLoss: 0.22883,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:18] Train,\\tTimestamp: 1614896778454,\\tGlobal steps: 334,\\t'\n",
      " 'Iteration: 335,\\tLoss: 0.22831,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:18] Train,\\tTimestamp: 1614896778657,\\tGlobal steps: 335,\\t'\n",
      " 'Iteration: 336,\\tLoss: 0.22778,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:18] Train,\\tTimestamp: 1614896778884,\\tGlobal steps: 336,\\t'\n",
      " 'Iteration: 337,\\tLoss: 0.22725,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:19] Train,\\tTimestamp: 1614896779087,\\tGlobal steps: 337,\\t'\n",
      " 'Iteration: 338,\\tLoss: 0.22671,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:19] Train,\\tTimestamp: 1614896779291,\\tGlobal steps: 338,\\t'\n",
      " 'Iteration: 339,\\tLoss: 0.22616,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:19] Train,\\tTimestamp: 1614896779494,\\tGlobal steps: 339,\\t'\n",
      " 'Iteration: 340,\\tLoss: 0.22562,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:19] Train,\\tTimestamp: 1614896779695,\\tGlobal steps: 340,\\t'\n",
      " 'Iteration: 341,\\tLoss: 0.22512,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:19] Train,\\tTimestamp: 1614896779897,\\tGlobal steps: 341,\\t'\n",
      " 'Iteration: 342,\\tLoss: 0.2246,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:20] Train,\\tTimestamp: 1614896780098,\\tGlobal steps: 342,\\t'\n",
      " 'Iteration: 343,\\tLoss: 0.22431,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:20] Train,\\tTimestamp: 1614896780300,\\tGlobal steps: 343,\\t'\n",
      " 'Iteration: 344,\\tLoss: 0.2238,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:20] Train,\\tTimestamp: 1614896780502,\\tGlobal steps: 344,\\t'\n",
      " 'Iteration: 345,\\tLoss: 0.2233,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:20] Train,\\tTimestamp: 1614896780709,\\tGlobal steps: 345,\\t'\n",
      " 'Iteration: 346,\\tLoss: 0.22279,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:20] Train,\\tTimestamp: 1614896780913,\\tGlobal steps: 346,\\t'\n",
      " 'Iteration: 347,\\tLoss: 0.22228,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:21] Train,\\tTimestamp: 1614896781116,\\tGlobal steps: 347,\\t'\n",
      " 'Iteration: 348,\\tLoss: 0.22177,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:21] Train,\\tTimestamp: 1614896781319,\\tGlobal steps: 348,\\t'\n",
      " 'Iteration: 349,\\tLoss: 0.22126,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:21] Train,\\tTimestamp: 1614896781523,\\tGlobal steps: 349,\\t'\n",
      " 'Iteration: 350,\\tLoss: 0.22076,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:21] Train,\\tTimestamp: 1614896781725,\\tGlobal steps: 350,\\t'\n",
      " 'Iteration: 351,\\tLoss: 0.22025,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:21] Train,\\tTimestamp: 1614896781928,\\tGlobal steps: 351,\\t'\n",
      " 'Iteration: 352,\\tLoss: 0.21975,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:22] Train,\\tTimestamp: 1614896782156,\\tGlobal steps: 352,\\t'\n",
      " 'Iteration: 353,\\tLoss: 0.21925,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:22] Train,\\tTimestamp: 1614896782359,\\tGlobal steps: 353,\\t'\n",
      " 'Iteration: 354,\\tLoss: 0.21876,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:22] Train,\\tTimestamp: 1614896782562,\\tGlobal steps: 354,\\t'\n",
      " 'Iteration: 355,\\tLoss: 0.21826,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:22] Train,\\tTimestamp: 1614896782765,\\tGlobal steps: 355,\\t'\n",
      " 'Iteration: 356,\\tLoss: 0.21777,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:22] Train,\\tTimestamp: 1614896782968,\\tGlobal steps: 356,\\t'\n",
      " 'Iteration: 357,\\tLoss: 0.21729,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:23] Train,\\tTimestamp: 1614896783171,\\tGlobal steps: 357,\\t'\n",
      " 'Iteration: 358,\\tLoss: 0.2168,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:23] Train,\\tTimestamp: 1614896783374,\\tGlobal steps: 358,\\t'\n",
      " 'Iteration: 359,\\tLoss: 0.21632,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:23] Train,\\tTimestamp: 1614896783578,\\tGlobal steps: 359,\\t'\n",
      " 'Iteration: 360,\\tLoss: 0.21583,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:23] Train,\\tTimestamp: 1614896783782,\\tGlobal steps: 360,\\t'\n",
      " 'Iteration: 361,\\tLoss: 0.21536,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:23] Train,\\tTimestamp: 1614896783985,\\tGlobal steps: 361,\\t'\n",
      " 'Iteration: 362,\\tLoss: 0.21488,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:24] Train,\\tTimestamp: 1614896784188,\\tGlobal steps: 362,\\t'\n",
      " 'Iteration: 363,\\tLoss: 0.21441,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:24] Train,\\tTimestamp: 1614896784390,\\tGlobal steps: 363,\\t'\n",
      " 'Iteration: 364,\\tLoss: 0.21393,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:24] Train,\\tTimestamp: 1614896784593,\\tGlobal steps: 364,\\t'\n",
      " 'Iteration: 365,\\tLoss: 0.21346,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:24] Train,\\tTimestamp: 1614896784796,\\tGlobal steps: 365,\\t'\n",
      " 'Iteration: 366,\\tLoss: 0.213,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:24] Train,\\tTimestamp: 1614896784998,\\tGlobal steps: 366,\\t'\n",
      " 'Iteration: 367,\\tLoss: 0.21253,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:25] Train,\\tTimestamp: 1614896785202,\\tGlobal steps: 367,\\t'\n",
      " 'Iteration: 368,\\tLoss: 0.21207,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:25] Train,\\tTimestamp: 1614896785430,\\tGlobal steps: 368,\\t'\n",
      " 'Iteration: 369,\\tLoss: 0.21161,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:25] Train,\\tTimestamp: 1614896785634,\\tGlobal steps: 369,\\t'\n",
      " 'Iteration: 370,\\tLoss: 0.21114,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:25] Train,\\tTimestamp: 1614896785835,\\tGlobal steps: 370,\\t'\n",
      " 'Iteration: 371,\\tLoss: 0.21067,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:26] Train,\\tTimestamp: 1614896786040,\\tGlobal steps: 371,\\t'\n",
      " 'Iteration: 372,\\tLoss: 0.21043,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:26] Train,\\tTimestamp: 1614896786247,\\tGlobal steps: 372,\\t'\n",
      " 'Iteration: 373,\\tLoss: 0.20996,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:26] Train,\\tTimestamp: 1614896786451,\\tGlobal steps: 373,\\t'\n",
      " 'Iteration: 374,\\tLoss: 0.20951,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:26] Train,\\tTimestamp: 1614896786654,\\tGlobal steps: 374,\\t'\n",
      " 'Iteration: 375,\\tLoss: 0.20905,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:26] Train,\\tTimestamp: 1614896786857,\\tGlobal steps: 375,\\t'\n",
      " 'Iteration: 376,\\tLoss: 0.20861,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:27] Train,\\tTimestamp: 1614896787060,\\tGlobal steps: 376,\\t'\n",
      " 'Iteration: 377,\\tLoss: 0.20816,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:27] Train,\\tTimestamp: 1614896787263,\\tGlobal steps: 377,\\t'\n",
      " 'Iteration: 378,\\tLoss: 0.20771,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:27] Train,\\tTimestamp: 1614896787466,\\tGlobal steps: 378,\\t'\n",
      " 'Iteration: 379,\\tLoss: 0.20726,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:27] Train,\\tTimestamp: 1614896787670,\\tGlobal steps: 379,\\t'\n",
      " 'Iteration: 380,\\tLoss: 0.20682,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:27] Train,\\tTimestamp: 1614896787874,\\tGlobal steps: 380,\\t'\n",
      " 'Iteration: 381,\\tLoss: 0.20639,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:28] Train,\\tTimestamp: 1614896788078,\\tGlobal steps: 381,\\t'\n",
      " 'Iteration: 382,\\tLoss: 0.20597,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:28] Train,\\tTimestamp: 1614896788282,\\tGlobal steps: 382,\\t'\n",
      " 'Iteration: 383,\\tLoss: 0.20553,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:28] Train,\\tTimestamp: 1614896788485,\\tGlobal steps: 383,\\t'\n",
      " 'Iteration: 384,\\tLoss: 0.2051,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:28] Train,\\tTimestamp: 1614896788707,\\tGlobal steps: 384,\\t'\n",
      " 'Iteration: 385,\\tLoss: 0.20467,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:28] Train,\\tTimestamp: 1614896788908,\\tGlobal steps: 385,\\t'\n",
      " 'Iteration: 386,\\tLoss: 0.20425,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:29] Train,\\tTimestamp: 1614896789111,\\tGlobal steps: 386,\\t'\n",
      " 'Iteration: 387,\\tLoss: 0.20382,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:29] Train,\\tTimestamp: 1614896789313,\\tGlobal steps: 387,\\t'\n",
      " 'Iteration: 388,\\tLoss: 0.20341,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:29] Train,\\tTimestamp: 1614896789514,\\tGlobal steps: 388,\\t'\n",
      " 'Iteration: 389,\\tLoss: 0.203,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:29] Train,\\tTimestamp: 1614896789715,\\tGlobal steps: 389,\\t'\n",
      " 'Iteration: 390,\\tLoss: 0.20258,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:29] Train,\\tTimestamp: 1614896789917,\\tGlobal steps: 390,\\t'\n",
      " 'Iteration: 391,\\tLoss: 0.20218,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:30] Train,\\tTimestamp: 1614896790119,\\tGlobal steps: 391,\\t'\n",
      " 'Iteration: 392,\\tLoss: 0.20177,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:30] Train,\\tTimestamp: 1614896790322,\\tGlobal steps: 392,\\t'\n",
      " 'Iteration: 393,\\tLoss: 0.20137,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:30] Train,\\tTimestamp: 1614896790525,\\tGlobal steps: 393,\\t'\n",
      " 'Iteration: 394,\\tLoss: 0.20097,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:30] Train,\\tTimestamp: 1614896790726,\\tGlobal steps: 394,\\t'\n",
      " 'Iteration: 395,\\tLoss: 0.20057,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:30] Train,\\tTimestamp: 1614896790928,\\tGlobal steps: 395,\\t'\n",
      " 'Iteration: 396,\\tLoss: 0.20018,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:31] Train,\\tTimestamp: 1614896791129,\\tGlobal steps: 396,\\t'\n",
      " 'Iteration: 397,\\tLoss: 0.19978,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:31] Train,\\tTimestamp: 1614896791331,\\tGlobal steps: 397,\\t'\n",
      " 'Iteration: 398,\\tLoss: 0.19938,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:31] Train,\\tTimestamp: 1614896791536,\\tGlobal steps: 398,\\t'\n",
      " 'Iteration: 399,\\tLoss: 0.19899,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:31] Train,\\tTimestamp: 1614896791738,\\tGlobal steps: 399,\\t'\n",
      " 'Iteration: 400,\\tLoss: 0.1986,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:31] Train,\\tTimestamp: 1614896791959,\\tGlobal steps: 400,\\t'\n",
      " 'Iteration: 401,\\tLoss: 0.19822,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:32] Train,\\tTimestamp: 1614896792160,\\tGlobal steps: 401,\\t'\n",
      " 'Iteration: 402,\\tLoss: 0.19783,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:32] Train,\\tTimestamp: 1614896792366,\\tGlobal steps: 402,\\t'\n",
      " 'Iteration: 403,\\tLoss: 0.19744,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:32] Train,\\tTimestamp: 1614896792568,\\tGlobal steps: 403,\\t'\n",
      " 'Iteration: 404,\\tLoss: 0.19706,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:32] Train,\\tTimestamp: 1614896792768,\\tGlobal steps: 404,\\t'\n",
      " 'Iteration: 405,\\tLoss: 0.19667,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:32] Train,\\tTimestamp: 1614896792970,\\tGlobal steps: 405,\\t'\n",
      " 'Iteration: 406,\\tLoss: 0.1963,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:33] Train,\\tTimestamp: 1614896793170,\\tGlobal steps: 406,\\t'\n",
      " 'Iteration: 407,\\tLoss: 0.19592,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:33] Train,\\tTimestamp: 1614896793372,\\tGlobal steps: 407,\\t'\n",
      " 'Iteration: 408,\\tLoss: 0.19553,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:33] Train,\\tTimestamp: 1614896793574,\\tGlobal steps: 408,\\t'\n",
      " 'Iteration: 409,\\tLoss: 0.19515,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:33] Train,\\tTimestamp: 1614896793776,\\tGlobal steps: 409,\\t'\n",
      " 'Iteration: 410,\\tLoss: 0.19479,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:33] Train,\\tTimestamp: 1614896793977,\\tGlobal steps: 410,\\t'\n",
      " 'Iteration: 411,\\tLoss: 0.19441,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:34] Train,\\tTimestamp: 1614896794180,\\tGlobal steps: 411,\\t'\n",
      " 'Iteration: 412,\\tLoss: 0.19404,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:34] Train,\\tTimestamp: 1614896794381,\\tGlobal steps: 412,\\t'\n",
      " 'Iteration: 413,\\tLoss: 0.19367,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:34] Train,\\tTimestamp: 1614896794583,\\tGlobal steps: 413,\\t'\n",
      " 'Iteration: 414,\\tLoss: 0.1933,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:34] Train,\\tTimestamp: 1614896794785,\\tGlobal steps: 414,\\t'\n",
      " 'Iteration: 415,\\tLoss: 0.19294,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:34] Train,\\tTimestamp: 1614896794986,\\tGlobal steps: 415,\\t'\n",
      " 'Iteration: 416,\\tLoss: 0.19268,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:35] Train,\\tTimestamp: 1614896795207,\\tGlobal steps: 416,\\t'\n",
      " 'Iteration: 417,\\tLoss: 0.19231,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:35] Train,\\tTimestamp: 1614896795408,\\tGlobal steps: 417,\\t'\n",
      " 'Iteration: 418,\\tLoss: 0.19194,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:35] Train,\\tTimestamp: 1614896795609,\\tGlobal steps: 418,\\t'\n",
      " 'Iteration: 419,\\tLoss: 0.19157,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:35] Train,\\tTimestamp: 1614896795810,\\tGlobal steps: 419,\\t'\n",
      " 'Iteration: 420,\\tLoss: 0.19121,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:36] Train,\\tTimestamp: 1614896796011,\\tGlobal steps: 420,\\t'\n",
      " 'Iteration: 421,\\tLoss: 0.19084,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:36] Train,\\tTimestamp: 1614896796213,\\tGlobal steps: 421,\\t'\n",
      " 'Iteration: 422,\\tLoss: 0.19048,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:36] Train,\\tTimestamp: 1614896796415,\\tGlobal steps: 422,\\t'\n",
      " 'Iteration: 423,\\tLoss: 0.19027,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:36] Train,\\tTimestamp: 1614896796616,\\tGlobal steps: 423,\\t'\n",
      " 'Iteration: 424,\\tLoss: 0.18991,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:36] Train,\\tTimestamp: 1614896796818,\\tGlobal steps: 424,\\t'\n",
      " 'Iteration: 425,\\tLoss: 0.18956,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:37] Train,\\tTimestamp: 1614896797022,\\tGlobal steps: 425,\\t'\n",
      " 'Iteration: 426,\\tLoss: 0.1892,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:37] Train,\\tTimestamp: 1614896797223,\\tGlobal steps: 426,\\t'\n",
      " 'Iteration: 427,\\tLoss: 0.18896,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:37] Train,\\tTimestamp: 1614896797424,\\tGlobal steps: 427,\\t'\n",
      " 'Iteration: 428,\\tLoss: 0.18861,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:37] Train,\\tTimestamp: 1614896797627,\\tGlobal steps: 428,\\t'\n",
      " 'Iteration: 429,\\tLoss: 0.18826,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:37] Train,\\tTimestamp: 1614896797828,\\tGlobal steps: 429,\\t'\n",
      " 'Iteration: 430,\\tLoss: 0.18791,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:38] Train,\\tTimestamp: 1614896798029,\\tGlobal steps: 430,\\t'\n",
      " 'Iteration: 431,\\tLoss: 0.18757,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:38] Train,\\tTimestamp: 1614896798232,\\tGlobal steps: 431,\\t'\n",
      " 'Iteration: 432,\\tLoss: 0.18722,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:38] Train,\\tTimestamp: 1614896798453,\\tGlobal steps: 432,\\t'\n",
      " 'Iteration: 433,\\tLoss: 0.18688,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:38] Train,\\tTimestamp: 1614896798654,\\tGlobal steps: 433,\\t'\n",
      " 'Iteration: 434,\\tLoss: 0.18654,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:38] Train,\\tTimestamp: 1614896798856,\\tGlobal steps: 434,\\t'\n",
      " 'Iteration: 435,\\tLoss: 0.1862,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:39] Train,\\tTimestamp: 1614896799057,\\tGlobal steps: 435,\\t'\n",
      " 'Iteration: 436,\\tLoss: 0.18586,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:39] Train,\\tTimestamp: 1614896799259,\\tGlobal steps: 436,\\t'\n",
      " 'Iteration: 437,\\tLoss: 0.18552,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:39] Train,\\tTimestamp: 1614896799461,\\tGlobal steps: 437,\\t'\n",
      " 'Iteration: 438,\\tLoss: 0.18519,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:39] Train,\\tTimestamp: 1614896799662,\\tGlobal steps: 438,\\t'\n",
      " 'Iteration: 439,\\tLoss: 0.18486,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:39] Train,\\tTimestamp: 1614896799864,\\tGlobal steps: 439,\\t'\n",
      " 'Iteration: 440,\\tLoss: 0.18454,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:40] Train,\\tTimestamp: 1614896800066,\\tGlobal steps: 440,\\t'\n",
      " 'Iteration: 441,\\tLoss: 0.1842,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:40] Train,\\tTimestamp: 1614896800267,\\tGlobal steps: 441,\\t'\n",
      " 'Iteration: 442,\\tLoss: 0.18387,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:40] Train,\\tTimestamp: 1614896800469,\\tGlobal steps: 442,\\t'\n",
      " 'Iteration: 443,\\tLoss: 0.18355,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:40] Train,\\tTimestamp: 1614896800671,\\tGlobal steps: 443,\\t'\n",
      " 'Iteration: 444,\\tLoss: 0.18323,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:40] Train,\\tTimestamp: 1614896800872,\\tGlobal steps: 444,\\t'\n",
      " 'Iteration: 445,\\tLoss: 0.18291,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:41] Train,\\tTimestamp: 1614896801074,\\tGlobal steps: 445,\\t'\n",
      " 'Iteration: 446,\\tLoss: 0.18259,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:41] Train,\\tTimestamp: 1614896801275,\\tGlobal steps: 446,\\t'\n",
      " 'Iteration: 447,\\tLoss: 0.18227,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:41] Train,\\tTimestamp: 1614896801479,\\tGlobal steps: 447,\\t'\n",
      " 'Iteration: 448,\\tLoss: 0.18195,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:41] Train,\\tTimestamp: 1614896801702,\\tGlobal steps: 448,\\t'\n",
      " 'Iteration: 449,\\tLoss: 0.18163,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:41] Train,\\tTimestamp: 1614896801902,\\tGlobal steps: 449,\\t'\n",
      " 'Iteration: 450,\\tLoss: 0.18143,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:42] Train,\\tTimestamp: 1614896802105,\\tGlobal steps: 450,\\t'\n",
      " 'Iteration: 451,\\tLoss: 0.18111,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:42] Train,\\tTimestamp: 1614896802306,\\tGlobal steps: 451,\\t'\n",
      " 'Iteration: 452,\\tLoss: 0.1808,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:42] Train,\\tTimestamp: 1614896802507,\\tGlobal steps: 452,\\t'\n",
      " 'Iteration: 453,\\tLoss: 0.18048,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:42] Train,\\tTimestamp: 1614896802709,\\tGlobal steps: 453,\\t'\n",
      " 'Iteration: 454,\\tLoss: 0.18017,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:42] Train,\\tTimestamp: 1614896802910,\\tGlobal steps: 454,\\t'\n",
      " 'Iteration: 455,\\tLoss: 0.17987,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:43] Train,\\tTimestamp: 1614896803110,\\tGlobal steps: 455,\\t'\n",
      " 'Iteration: 456,\\tLoss: 0.17956,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:43] Train,\\tTimestamp: 1614896803311,\\tGlobal steps: 456,\\t'\n",
      " 'Iteration: 457,\\tLoss: 0.17927,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:43] Train,\\tTimestamp: 1614896803512,\\tGlobal steps: 457,\\t'\n",
      " 'Iteration: 458,\\tLoss: 0.17897,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:43] Train,\\tTimestamp: 1614896803712,\\tGlobal steps: 458,\\t'\n",
      " 'Iteration: 459,\\tLoss: 0.17867,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:43] Train,\\tTimestamp: 1614896803913,\\tGlobal steps: 459,\\t'\n",
      " 'Iteration: 460,\\tLoss: 0.17836,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:44] Train,\\tTimestamp: 1614896804115,\\tGlobal steps: 460,\\t'\n",
      " 'Iteration: 461,\\tLoss: 0.17806,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:26:44] Train,\\tTimestamp: 1614896804316,\\tGlobal steps: 461,\\t'\n",
      " 'Iteration: 462,\\tLoss: 0.17776,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:44] Train,\\tTimestamp: 1614896804517,\\tGlobal steps: 462,\\t'\n",
      " 'Iteration: 463,\\tLoss: 0.17746,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:44] Train,\\tTimestamp: 1614896804718,\\tGlobal steps: 463,\\t'\n",
      " 'Iteration: 464,\\tLoss: 0.17717,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:44] Train,\\tTimestamp: 1614896804937,\\tGlobal steps: 464,\\t'\n",
      " 'Iteration: 465,\\tLoss: 0.17687,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:45] Train,\\tTimestamp: 1614896805138,\\tGlobal steps: 465,\\t'\n",
      " 'Iteration: 466,\\tLoss: 0.17658,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:45] Train,\\tTimestamp: 1614896805542,\\tGlobal steps: 466,\\t'\n",
      " 'Iteration: 467,\\tLoss: 0.17628,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:46] Train,\\tTimestamp: 1614896806006,\\tGlobal steps: 467,\\t'\n",
      " 'Iteration: 468,\\tLoss: 0.17599,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:46] Train,\\tTimestamp: 1614896806281,\\tGlobal steps: 468,\\t'\n",
      " 'Iteration: 469,\\tLoss: 0.17569,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:46] Train,\\tTimestamp: 1614896806663,\\tGlobal steps: 469,\\t'\n",
      " 'Iteration: 470,\\tLoss: 0.17546,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:47] Train,\\tTimestamp: 1614896807110,\\tGlobal steps: 470,\\t'\n",
      " 'Iteration: 471,\\tLoss: 0.17517,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:47] Train,\\tTimestamp: 1614896807383,\\tGlobal steps: 471,\\t'\n",
      " 'Iteration: 472,\\tLoss: 0.17498,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:47] Train,\\tTimestamp: 1614896807595,\\tGlobal steps: 472,\\t'\n",
      " 'Iteration: 473,\\tLoss: 0.17469,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:47] Train,\\tTimestamp: 1614896807814,\\tGlobal steps: 473,\\t'\n",
      " 'Iteration: 474,\\tLoss: 0.1744,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:48] Train,\\tTimestamp: 1614896808040,\\tGlobal steps: 474,\\t'\n",
      " 'Iteration: 475,\\tLoss: 0.17411,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:48] Train,\\tTimestamp: 1614896808258,\\tGlobal steps: 475,\\t'\n",
      " 'Iteration: 476,\\tLoss: 0.17383,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:48] Train,\\tTimestamp: 1614896808461,\\tGlobal steps: 476,\\t'\n",
      " 'Iteration: 477,\\tLoss: 0.17354,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:48] Train,\\tTimestamp: 1614896808663,\\tGlobal steps: 477,\\t'\n",
      " 'Iteration: 478,\\tLoss: 0.17326,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:48] Train,\\tTimestamp: 1614896808864,\\tGlobal steps: 478,\\t'\n",
      " 'Iteration: 479,\\tLoss: 0.17297,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:49] Train,\\tTimestamp: 1614896809065,\\tGlobal steps: 479,\\t'\n",
      " 'Iteration: 480,\\tLoss: 0.17269,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:49] Train,\\tTimestamp: 1614896809294,\\tGlobal steps: 480,\\t'\n",
      " 'Iteration: 481,\\tLoss: 0.17242,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:49] Train,\\tTimestamp: 1614896809495,\\tGlobal steps: 481,\\t'\n",
      " 'Iteration: 482,\\tLoss: 0.17214,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:49] Train,\\tTimestamp: 1614896809696,\\tGlobal steps: 482,\\t'\n",
      " 'Iteration: 483,\\tLoss: 0.17186,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:49] Train,\\tTimestamp: 1614896809897,\\tGlobal steps: 483,\\t'\n",
      " 'Iteration: 484,\\tLoss: 0.17157,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:50] Train,\\tTimestamp: 1614896810097,\\tGlobal steps: 484,\\t'\n",
      " 'Iteration: 485,\\tLoss: 0.17135,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:50] Train,\\tTimestamp: 1614896810300,\\tGlobal steps: 485,\\t'\n",
      " 'Iteration: 486,\\tLoss: 0.17108,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:50] Train,\\tTimestamp: 1614896810501,\\tGlobal steps: 486,\\t'\n",
      " 'Iteration: 487,\\tLoss: 0.17082,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:50] Train,\\tTimestamp: 1614896810703,\\tGlobal steps: 487,\\t'\n",
      " 'Iteration: 488,\\tLoss: 0.17054,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:50] Train,\\tTimestamp: 1614896810905,\\tGlobal steps: 488,\\t'\n",
      " 'Iteration: 489,\\tLoss: 0.17027,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:51] Train,\\tTimestamp: 1614896811106,\\tGlobal steps: 489,\\t'\n",
      " 'Iteration: 490,\\tLoss: 0.17,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:51] Train,\\tTimestamp: 1614896811552,\\tGlobal steps: 490,\\t'\n",
      " 'Iteration: 491,\\tLoss: 0.16973,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:51] Train,\\tTimestamp: 1614896811981,\\tGlobal steps: 491,\\t'\n",
      " 'Iteration: 492,\\tLoss: 0.16946,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:52] Train,\\tTimestamp: 1614896812222,\\tGlobal steps: 492,\\t'\n",
      " 'Iteration: 493,\\tLoss: 0.16919,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:52] Train,\\tTimestamp: 1614896812424,\\tGlobal steps: 493,\\t'\n",
      " 'Iteration: 494,\\tLoss: 0.16893,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:52] Train,\\tTimestamp: 1614896812626,\\tGlobal steps: 494,\\t'\n",
      " 'Iteration: 495,\\tLoss: 0.16867,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:52] Train,\\tTimestamp: 1614896812827,\\tGlobal steps: 495,\\t'\n",
      " 'Iteration: 496,\\tLoss: 0.16841,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:53] Train,\\tTimestamp: 1614896813049,\\tGlobal steps: 496,\\t'\n",
      " 'Iteration: 497,\\tLoss: 0.16815,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:53] Train,\\tTimestamp: 1614896813251,\\tGlobal steps: 497,\\t'\n",
      " 'Iteration: 498,\\tLoss: 0.16788,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:53] Train,\\tTimestamp: 1614896813453,\\tGlobal steps: 498,\\t'\n",
      " 'Iteration: 499,\\tLoss: 0.16762,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:53] Train,\\tTimestamp: 1614896813654,\\tGlobal steps: 499,\\t'\n",
      " 'Iteration: 500,\\tLoss: 0.16736,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:26:53] Train,\\tTimestamp: 1614896813855,\\tGlobal steps: 500,\\t'\n",
      " 'Iteration: 501,\\tLoss: 0.16709,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:54] Train,\\tTimestamp: 1614896814057,\\tGlobal steps: 501,\\t'\n",
      " 'Iteration: 502,\\tLoss: 0.16683,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:54] Train,\\tTimestamp: 1614896814258,\\tGlobal steps: 502,\\t'\n",
      " 'Iteration: 503,\\tLoss: 0.16657,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:54] Train,\\tTimestamp: 1614896814465,\\tGlobal steps: 503,\\t'\n",
      " 'Iteration: 504,\\tLoss: 0.16631,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:54] Train,\\tTimestamp: 1614896814667,\\tGlobal steps: 504,\\t'\n",
      " 'Iteration: 505,\\tLoss: 0.16606,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:54] Train,\\tTimestamp: 1614896814868,\\tGlobal steps: 505,\\t'\n",
      " 'Iteration: 506,\\tLoss: 0.16586,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:55] Train,\\tTimestamp: 1614896815069,\\tGlobal steps: 506,\\t'\n",
      " 'Iteration: 507,\\tLoss: 0.16561,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:55] Train,\\tTimestamp: 1614896815270,\\tGlobal steps: 507,\\t'\n",
      " 'Iteration: 508,\\tLoss: 0.16536,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:55] Train,\\tTimestamp: 1614896815472,\\tGlobal steps: 508,\\t'\n",
      " 'Iteration: 509,\\tLoss: 0.1651,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:55] Train,\\tTimestamp: 1614896815673,\\tGlobal steps: 509,\\t'\n",
      " 'Iteration: 510,\\tLoss: 0.16485,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:55] Train,\\tTimestamp: 1614896815874,\\tGlobal steps: 510,\\t'\n",
      " 'Iteration: 511,\\tLoss: 0.16461,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:56] Train,\\tTimestamp: 1614896816077,\\tGlobal steps: 511,\\t'\n",
      " 'Iteration: 512,\\tLoss: 0.16436,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:56] Train,\\tTimestamp: 1614896816298,\\tGlobal steps: 512,\\t'\n",
      " 'Iteration: 513,\\tLoss: 0.16417,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:56] Train,\\tTimestamp: 1614896816500,\\tGlobal steps: 513,\\t'\n",
      " 'Iteration: 514,\\tLoss: 0.16392,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:56] Train,\\tTimestamp: 1614896816702,\\tGlobal steps: 514,\\t'\n",
      " 'Iteration: 515,\\tLoss: 0.16367,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:56] Train,\\tTimestamp: 1614896816903,\\tGlobal steps: 515,\\t'\n",
      " 'Iteration: 516,\\tLoss: 0.16343,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:57] Train,\\tTimestamp: 1614896817104,\\tGlobal steps: 516,\\t'\n",
      " 'Iteration: 517,\\tLoss: 0.16318,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:57] Train,\\tTimestamp: 1614896817307,\\tGlobal steps: 517,\\t'\n",
      " 'Iteration: 518,\\tLoss: 0.16294,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:57] Train,\\tTimestamp: 1614896817508,\\tGlobal steps: 518,\\t'\n",
      " 'Iteration: 519,\\tLoss: 0.1627,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:57] Train,\\tTimestamp: 1614896817710,\\tGlobal steps: 519,\\t'\n",
      " 'Iteration: 520,\\tLoss: 0.16246,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:57] Train,\\tTimestamp: 1614896817911,\\tGlobal steps: 520,\\t'\n",
      " 'Iteration: 521,\\tLoss: 0.16221,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:58] Train,\\tTimestamp: 1614896818112,\\tGlobal steps: 521,\\t'\n",
      " 'Iteration: 522,\\tLoss: 0.16197,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:58] Train,\\tTimestamp: 1614896818313,\\tGlobal steps: 522,\\t'\n",
      " 'Iteration: 523,\\tLoss: 0.16173,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:58] Train,\\tTimestamp: 1614896818515,\\tGlobal steps: 523,\\t'\n",
      " 'Iteration: 524,\\tLoss: 0.1615,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:58] Train,\\tTimestamp: 1614896818716,\\tGlobal steps: 524,\\t'\n",
      " 'Iteration: 525,\\tLoss: 0.16126,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:58] Train,\\tTimestamp: 1614896818916,\\tGlobal steps: 525,\\t'\n",
      " 'Iteration: 526,\\tLoss: 0.16102,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:59] Train,\\tTimestamp: 1614896819118,\\tGlobal steps: 526,\\t'\n",
      " 'Iteration: 527,\\tLoss: 0.16079,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:59] Train,\\tTimestamp: 1614896819319,\\tGlobal steps: 527,\\t'\n",
      " 'Iteration: 528,\\tLoss: 0.16055,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:59] Train,\\tTimestamp: 1614896819539,\\tGlobal steps: 528,\\t'\n",
      " 'Iteration: 529,\\tLoss: 0.16032,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:59] Train,\\tTimestamp: 1614896819741,\\tGlobal steps: 529,\\t'\n",
      " 'Iteration: 530,\\tLoss: 0.16009,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:26:59] Train,\\tTimestamp: 1614896819943,\\tGlobal steps: 530,\\t'\n",
      " 'Iteration: 531,\\tLoss: 0.15986,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:00] Train,\\tTimestamp: 1614896820145,\\tGlobal steps: 531,\\t'\n",
      " 'Iteration: 532,\\tLoss: 0.15962,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:00] Train,\\tTimestamp: 1614896820346,\\tGlobal steps: 532,\\t'\n",
      " 'Iteration: 533,\\tLoss: 0.15939,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:00] Train,\\tTimestamp: 1614896820548,\\tGlobal steps: 533,\\t'\n",
      " 'Iteration: 534,\\tLoss: 0.15917,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:00] Train,\\tTimestamp: 1614896820749,\\tGlobal steps: 534,\\t'\n",
      " 'Iteration: 535,\\tLoss: 0.15894,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:00] Train,\\tTimestamp: 1614896820950,\\tGlobal steps: 535,\\t'\n",
      " 'Iteration: 536,\\tLoss: 0.15872,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:01] Train,\\tTimestamp: 1614896821152,\\tGlobal steps: 536,\\t'\n",
      " 'Iteration: 537,\\tLoss: 0.15849,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:01] Train,\\tTimestamp: 1614896821353,\\tGlobal steps: 537,\\t'\n",
      " 'Iteration: 538,\\tLoss: 0.15827,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:01] Train,\\tTimestamp: 1614896821555,\\tGlobal steps: 538,\\t'\n",
      " 'Iteration: 539,\\tLoss: 0.15804,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:01] Train,\\tTimestamp: 1614896821756,\\tGlobal steps: 539,\\t'\n",
      " 'Iteration: 540,\\tLoss: 0.15782,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:01] Train,\\tTimestamp: 1614896821958,\\tGlobal steps: 540,\\t'\n",
      " 'Iteration: 541,\\tLoss: 0.1576,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:02] Train,\\tTimestamp: 1614896822159,\\tGlobal steps: 541,\\t'\n",
      " 'Iteration: 542,\\tLoss: 0.15738,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:02] Train,\\tTimestamp: 1614896822361,\\tGlobal steps: 542,\\t'\n",
      " 'Iteration: 543,\\tLoss: 0.15716,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:02] Train,\\tTimestamp: 1614896822562,\\tGlobal steps: 543,\\t'\n",
      " 'Iteration: 544,\\tLoss: 0.15694,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:02] Train,\\tTimestamp: 1614896822785,\\tGlobal steps: 544,\\t'\n",
      " 'Iteration: 545,\\tLoss: 0.15672,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:02] Train,\\tTimestamp: 1614896822986,\\tGlobal steps: 545,\\t'\n",
      " 'Iteration: 546,\\tLoss: 0.1565,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:03] Train,\\tTimestamp: 1614896823191,\\tGlobal steps: 546,\\t'\n",
      " 'Iteration: 547,\\tLoss: 0.15628,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:03] Train,\\tTimestamp: 1614896823392,\\tGlobal steps: 547,\\t'\n",
      " 'Iteration: 548,\\tLoss: 0.15606,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:03] Train,\\tTimestamp: 1614896823594,\\tGlobal steps: 548,\\t'\n",
      " 'Iteration: 549,\\tLoss: 0.15585,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:03] Train,\\tTimestamp: 1614896823796,\\tGlobal steps: 549,\\t'\n",
      " 'Iteration: 550,\\tLoss: 0.15563,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:03] Train,\\tTimestamp: 1614896823999,\\tGlobal steps: 550,\\t'\n",
      " 'Iteration: 551,\\tLoss: 0.15542,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:04] Train,\\tTimestamp: 1614896824216,\\tGlobal steps: 551,\\t'\n",
      " 'Iteration: 552,\\tLoss: 0.1552,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:27:04] Train,\\tTimestamp: 1614896824418,\\tGlobal steps: 552,\\t'\n",
      " 'Iteration: 553,\\tLoss: 0.15499,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:04] Train,\\tTimestamp: 1614896824619,\\tGlobal steps: 553,\\t'\n",
      " 'Iteration: 554,\\tLoss: 0.15478,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:04] Train,\\tTimestamp: 1614896824822,\\tGlobal steps: 554,\\t'\n",
      " 'Iteration: 555,\\tLoss: 0.15456,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:05] Train,\\tTimestamp: 1614896825025,\\tGlobal steps: 555,\\t'\n",
      " 'Iteration: 556,\\tLoss: 0.15435,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:05] Train,\\tTimestamp: 1614896825227,\\tGlobal steps: 556,\\t'\n",
      " 'Iteration: 557,\\tLoss: 0.15414,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:05] Train,\\tTimestamp: 1614896825429,\\tGlobal steps: 557,\\t'\n",
      " 'Iteration: 558,\\tLoss: 0.15393,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:05] Train,\\tTimestamp: 1614896825631,\\tGlobal steps: 558,\\t'\n",
      " 'Iteration: 559,\\tLoss: 0.15372,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:05] Train,\\tTimestamp: 1614896825834,\\tGlobal steps: 559,\\t'\n",
      " 'Iteration: 560,\\tLoss: 0.15351,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:06] Train,\\tTimestamp: 1614896826055,\\tGlobal steps: 560,\\t'\n",
      " 'Iteration: 561,\\tLoss: 0.1533,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:06] Train,\\tTimestamp: 1614896826259,\\tGlobal steps: 561,\\t'\n",
      " 'Iteration: 562,\\tLoss: 0.15309,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:06] Train,\\tTimestamp: 1614896826462,\\tGlobal steps: 562,\\t'\n",
      " 'Iteration: 563,\\tLoss: 0.15289,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:06] Train,\\tTimestamp: 1614896826665,\\tGlobal steps: 563,\\t'\n",
      " 'Iteration: 564,\\tLoss: 0.15268,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:06] Train,\\tTimestamp: 1614896826868,\\tGlobal steps: 564,\\t'\n",
      " 'Iteration: 565,\\tLoss: 0.15247,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:07] Train,\\tTimestamp: 1614896827071,\\tGlobal steps: 565,\\t'\n",
      " 'Iteration: 566,\\tLoss: 0.15227,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:07] Train,\\tTimestamp: 1614896827274,\\tGlobal steps: 566,\\t'\n",
      " 'Iteration: 567,\\tLoss: 0.15206,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:07] Train,\\tTimestamp: 1614896827478,\\tGlobal steps: 567,\\t'\n",
      " 'Iteration: 568,\\tLoss: 0.15186,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:07] Train,\\tTimestamp: 1614896827681,\\tGlobal steps: 568,\\t'\n",
      " 'Iteration: 569,\\tLoss: 0.15165,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:07] Train,\\tTimestamp: 1614896827883,\\tGlobal steps: 569,\\t'\n",
      " 'Iteration: 570,\\tLoss: 0.15145,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:08] Train,\\tTimestamp: 1614896828087,\\tGlobal steps: 570,\\t'\n",
      " 'Iteration: 571,\\tLoss: 0.15125,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:08] Train,\\tTimestamp: 1614896828289,\\tGlobal steps: 571,\\t'\n",
      " 'Iteration: 572,\\tLoss: 0.15105,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:08] Train,\\tTimestamp: 1614896828492,\\tGlobal steps: 572,\\t'\n",
      " 'Iteration: 573,\\tLoss: 0.15085,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:08] Train,\\tTimestamp: 1614896828695,\\tGlobal steps: 573,\\t'\n",
      " 'Iteration: 574,\\tLoss: 0.15065,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:08] Train,\\tTimestamp: 1614896828897,\\tGlobal steps: 574,\\t'\n",
      " 'Iteration: 575,\\tLoss: 0.15045,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:09] Train,\\tTimestamp: 1614896829099,\\tGlobal steps: 575,\\t'\n",
      " 'Iteration: 576,\\tLoss: 0.15025,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:09] Train,\\tTimestamp: 1614896829336,\\tGlobal steps: 576,\\t'\n",
      " 'Iteration: 577,\\tLoss: 0.15005,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:09] Train,\\tTimestamp: 1614896829538,\\tGlobal steps: 577,\\t'\n",
      " 'Iteration: 578,\\tLoss: 0.14986,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:09] Train,\\tTimestamp: 1614896829744,\\tGlobal steps: 578,\\t'\n",
      " 'Iteration: 579,\\tLoss: 0.14966,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:09] Train,\\tTimestamp: 1614896829947,\\tGlobal steps: 579,\\t'\n",
      " 'Iteration: 580,\\tLoss: 0.14947,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:10] Train,\\tTimestamp: 1614896830148,\\tGlobal steps: 580,\\t'\n",
      " 'Iteration: 581,\\tLoss: 0.14927,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:10] Train,\\tTimestamp: 1614896830350,\\tGlobal steps: 581,\\t'\n",
      " 'Iteration: 582,\\tLoss: 0.14908,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:10] Train,\\tTimestamp: 1614896830552,\\tGlobal steps: 582,\\t'\n",
      " 'Iteration: 583,\\tLoss: 0.14888,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:10] Train,\\tTimestamp: 1614896830754,\\tGlobal steps: 583,\\t'\n",
      " 'Iteration: 584,\\tLoss: 0.14869,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:10] Train,\\tTimestamp: 1614896830955,\\tGlobal steps: 584,\\t'\n",
      " 'Iteration: 585,\\tLoss: 0.1485,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:11] Train,\\tTimestamp: 1614896831156,\\tGlobal steps: 585,\\t'\n",
      " 'Iteration: 586,\\tLoss: 0.14831,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:11] Train,\\tTimestamp: 1614896831359,\\tGlobal steps: 586,\\t'\n",
      " 'Iteration: 587,\\tLoss: 0.14812,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:11] Train,\\tTimestamp: 1614896831562,\\tGlobal steps: 587,\\t'\n",
      " 'Iteration: 588,\\tLoss: 0.14793,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:11] Train,\\tTimestamp: 1614896831766,\\tGlobal steps: 588,\\t'\n",
      " 'Iteration: 589,\\tLoss: 0.14774,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:11] Train,\\tTimestamp: 1614896831968,\\tGlobal steps: 589,\\t'\n",
      " 'Iteration: 590,\\tLoss: 0.14755,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:12] Train,\\tTimestamp: 1614896832171,\\tGlobal steps: 590,\\t'\n",
      " 'Iteration: 591,\\tLoss: 0.14736,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:12] Train,\\tTimestamp: 1614896832373,\\tGlobal steps: 591,\\t'\n",
      " 'Iteration: 592,\\tLoss: 0.14717,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:12] Train,\\tTimestamp: 1614896832595,\\tGlobal steps: 592,\\t'\n",
      " 'Iteration: 593,\\tLoss: 0.14699,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:12] Train,\\tTimestamp: 1614896832798,\\tGlobal steps: 593,\\t'\n",
      " 'Iteration: 594,\\tLoss: 0.1468,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:12] Train,\\tTimestamp: 1614896833000,\\tGlobal steps: 594,\\t'\n",
      " 'Iteration: 595,\\tLoss: 0.14662,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:13] Train,\\tTimestamp: 1614896833201,\\tGlobal steps: 595,\\t'\n",
      " 'Iteration: 596,\\tLoss: 0.14643,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:13] Train,\\tTimestamp: 1614896833402,\\tGlobal steps: 596,\\t'\n",
      " 'Iteration: 597,\\tLoss: 0.14625,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:13] Train,\\tTimestamp: 1614896833604,\\tGlobal steps: 597,\\t'\n",
      " 'Iteration: 598,\\tLoss: 0.14607,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:13] Train,\\tTimestamp: 1614896833806,\\tGlobal steps: 598,\\t'\n",
      " 'Iteration: 599,\\tLoss: 0.14589,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:14] Train,\\tTimestamp: 1614896834006,\\tGlobal steps: 599,\\t'\n",
      " 'Iteration: 600,\\tLoss: 0.1457,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:14] Train,\\tTimestamp: 1614896834207,\\tGlobal steps: 600,\\t'\n",
      " 'Iteration: 601,\\tLoss: 0.14552,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:14] Train,\\tTimestamp: 1614896834419,\\tGlobal steps: 601,\\t'\n",
      " 'Iteration: 602,\\tLoss: 0.14534,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:14] Train,\\tTimestamp: 1614896834620,\\tGlobal steps: 602,\\t'\n",
      " 'Iteration: 603,\\tLoss: 0.14516,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:14] Train,\\tTimestamp: 1614896834822,\\tGlobal steps: 603,\\t'\n",
      " 'Iteration: 604,\\tLoss: 0.14498,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:15] Train,\\tTimestamp: 1614896835023,\\tGlobal steps: 604,\\t'\n",
      " 'Iteration: 605,\\tLoss: 0.1448,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:15] Train,\\tTimestamp: 1614896835225,\\tGlobal steps: 605,\\t'\n",
      " 'Iteration: 606,\\tLoss: 0.14463,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:15] Train,\\tTimestamp: 1614896835427,\\tGlobal steps: 606,\\t'\n",
      " 'Iteration: 607,\\tLoss: 0.14445,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:15] Train,\\tTimestamp: 1614896835629,\\tGlobal steps: 607,\\t'\n",
      " 'Iteration: 608,\\tLoss: 0.14427,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:15] Train,\\tTimestamp: 1614896835850,\\tGlobal steps: 608,\\t'\n",
      " 'Iteration: 609,\\tLoss: 0.1441,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:16] Train,\\tTimestamp: 1614896836052,\\tGlobal steps: 609,\\t'\n",
      " 'Iteration: 610,\\tLoss: 0.14392,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:16] Train,\\tTimestamp: 1614896836253,\\tGlobal steps: 610,\\t'\n",
      " 'Iteration: 611,\\tLoss: 0.14375,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:16] Train,\\tTimestamp: 1614896836456,\\tGlobal steps: 611,\\t'\n",
      " 'Iteration: 612,\\tLoss: 0.14357,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:16] Train,\\tTimestamp: 1614896836659,\\tGlobal steps: 612,\\t'\n",
      " 'Iteration: 613,\\tLoss: 0.1434,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:16] Train,\\tTimestamp: 1614896836860,\\tGlobal steps: 613,\\t'\n",
      " 'Iteration: 614,\\tLoss: 0.14322,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:17] Train,\\tTimestamp: 1614896837063,\\tGlobal steps: 614,\\t'\n",
      " 'Iteration: 615,\\tLoss: 0.14305,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:17] Train,\\tTimestamp: 1614896837264,\\tGlobal steps: 615,\\t'\n",
      " 'Iteration: 616,\\tLoss: 0.14288,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:17] Train,\\tTimestamp: 1614896837466,\\tGlobal steps: 616,\\t'\n",
      " 'Iteration: 617,\\tLoss: 0.14271,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:17] Train,\\tTimestamp: 1614896837667,\\tGlobal steps: 617,\\t'\n",
      " 'Iteration: 618,\\tLoss: 0.14254,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:17] Train,\\tTimestamp: 1614896837868,\\tGlobal steps: 618,\\t'\n",
      " 'Iteration: 619,\\tLoss: 0.14237,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:18] Train,\\tTimestamp: 1614896838070,\\tGlobal steps: 619,\\t'\n",
      " 'Iteration: 620,\\tLoss: 0.1422,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:18] Train,\\tTimestamp: 1614896838271,\\tGlobal steps: 620,\\t'\n",
      " 'Iteration: 621,\\tLoss: 0.14203,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:18] Train,\\tTimestamp: 1614896838474,\\tGlobal steps: 621,\\t'\n",
      " 'Iteration: 622,\\tLoss: 0.14186,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:18] Train,\\tTimestamp: 1614896838676,\\tGlobal steps: 622,\\t'\n",
      " 'Iteration: 623,\\tLoss: 0.14169,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:18] Train,\\tTimestamp: 1614896838880,\\tGlobal steps: 623,\\t'\n",
      " 'Iteration: 624,\\tLoss: 0.14152,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:19] Train,\\tTimestamp: 1614896839103,\\tGlobal steps: 624,\\t'\n",
      " 'Iteration: 625,\\tLoss: 0.14135,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:19] Train,\\tTimestamp: 1614896839305,\\tGlobal steps: 625,\\t'\n",
      " 'Iteration: 626,\\tLoss: 0.14119,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:19] Train,\\tTimestamp: 1614896839512,\\tGlobal steps: 626,\\t'\n",
      " 'Iteration: 627,\\tLoss: 0.14102,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:19] Train,\\tTimestamp: 1614896839714,\\tGlobal steps: 627,\\t'\n",
      " 'Iteration: 628,\\tLoss: 0.14086,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:19] Train,\\tTimestamp: 1614896839916,\\tGlobal steps: 628,\\t'\n",
      " 'Iteration: 629,\\tLoss: 0.14069,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:20] Train,\\tTimestamp: 1614896840117,\\tGlobal steps: 629,\\t'\n",
      " 'Iteration: 630,\\tLoss: 0.14052,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:20] Train,\\tTimestamp: 1614896840319,\\tGlobal steps: 630,\\t'\n",
      " 'Iteration: 631,\\tLoss: 0.14036,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:20] Train,\\tTimestamp: 1614896840522,\\tGlobal steps: 631,\\t'\n",
      " 'Iteration: 632,\\tLoss: 0.1402,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:20] Train,\\tTimestamp: 1614896840724,\\tGlobal steps: 632,\\t'\n",
      " 'Iteration: 633,\\tLoss: 0.14004,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:20] Train,\\tTimestamp: 1614896840925,\\tGlobal steps: 633,\\t'\n",
      " 'Iteration: 634,\\tLoss: 0.13987,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:21] Train,\\tTimestamp: 1614896841128,\\tGlobal steps: 634,\\t'\n",
      " 'Iteration: 635,\\tLoss: 0.13971,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:21] Train,\\tTimestamp: 1614896841330,\\tGlobal steps: 635,\\t'\n",
      " 'Iteration: 636,\\tLoss: 0.13955,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:21] Train,\\tTimestamp: 1614896841532,\\tGlobal steps: 636,\\t'\n",
      " 'Iteration: 637,\\tLoss: 0.13939,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:21] Train,\\tTimestamp: 1614896841733,\\tGlobal steps: 637,\\t'\n",
      " 'Iteration: 638,\\tLoss: 0.13923,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:21] Train,\\tTimestamp: 1614896841935,\\tGlobal steps: 638,\\t'\n",
      " 'Iteration: 639,\\tLoss: 0.13907,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:22] Train,\\tTimestamp: 1614896842138,\\tGlobal steps: 639,\\t'\n",
      " 'Iteration: 640,\\tLoss: 0.13891,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:22] Train,\\tTimestamp: 1614896842365,\\tGlobal steps: 640,\\t'\n",
      " 'Iteration: 641,\\tLoss: 0.13875,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:22] Train,\\tTimestamp: 1614896842567,\\tGlobal steps: 641,\\t'\n",
      " 'Iteration: 642,\\tLoss: 0.13859,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:22] Train,\\tTimestamp: 1614896842770,\\tGlobal steps: 642,\\t'\n",
      " 'Iteration: 643,\\tLoss: 0.13843,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:22] Train,\\tTimestamp: 1614896842973,\\tGlobal steps: 643,\\t'\n",
      " 'Iteration: 644,\\tLoss: 0.13828,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:23] Train,\\tTimestamp: 1614896843176,\\tGlobal steps: 644,\\t'\n",
      " 'Iteration: 645,\\tLoss: 0.13813,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:23] Train,\\tTimestamp: 1614896843379,\\tGlobal steps: 645,\\t'\n",
      " 'Iteration: 646,\\tLoss: 0.13797,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:23] Train,\\tTimestamp: 1614896843583,\\tGlobal steps: 646,\\t'\n",
      " 'Iteration: 647,\\tLoss: 0.13781,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:23] Train,\\tTimestamp: 1614896843786,\\tGlobal steps: 647,\\t'\n",
      " 'Iteration: 648,\\tLoss: 0.13766,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:23] Train,\\tTimestamp: 1614896843989,\\tGlobal steps: 648,\\t'\n",
      " 'Iteration: 649,\\tLoss: 0.1375,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:24] Train,\\tTimestamp: 1614896844193,\\tGlobal steps: 649,\\t'\n",
      " 'Iteration: 650,\\tLoss: 0.13735,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:24] Train,\\tTimestamp: 1614896844396,\\tGlobal steps: 650,\\t'\n",
      " 'Iteration: 651,\\tLoss: 0.13719,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:24] Train,\\tTimestamp: 1614896844606,\\tGlobal steps: 651,\\t'\n",
      " 'Iteration: 652,\\tLoss: 0.13704,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:24] Train,\\tTimestamp: 1614896844809,\\tGlobal steps: 652,\\t'\n",
      " 'Iteration: 653,\\tLoss: 0.13688,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:25] Train,\\tTimestamp: 1614896845013,\\tGlobal steps: 653,\\t'\n",
      " 'Iteration: 654,\\tLoss: 0.13673,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:25] Train,\\tTimestamp: 1614896845215,\\tGlobal steps: 654,\\t'\n",
      " 'Iteration: 655,\\tLoss: 0.13658,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:25] Train,\\tTimestamp: 1614896845417,\\tGlobal steps: 655,\\t'\n",
      " 'Iteration: 656,\\tLoss: 0.13643,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:25] Train,\\tTimestamp: 1614896845638,\\tGlobal steps: 656,\\t'\n",
      " 'Iteration: 657,\\tLoss: 0.13628,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:25] Train,\\tTimestamp: 1614896845840,\\tGlobal steps: 657,\\t'\n",
      " 'Iteration: 658,\\tLoss: 0.13612,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:26] Train,\\tTimestamp: 1614896846042,\\tGlobal steps: 658,\\t'\n",
      " 'Iteration: 659,\\tLoss: 0.13597,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:26] Train,\\tTimestamp: 1614896846243,\\tGlobal steps: 659,\\t'\n",
      " 'Iteration: 660,\\tLoss: 0.13582,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:26] Train,\\tTimestamp: 1614896846444,\\tGlobal steps: 660,\\t'\n",
      " 'Iteration: 661,\\tLoss: 0.13567,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:26] Train,\\tTimestamp: 1614896846646,\\tGlobal steps: 661,\\t'\n",
      " 'Iteration: 662,\\tLoss: 0.13552,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:26] Train,\\tTimestamp: 1614896846848,\\tGlobal steps: 662,\\t'\n",
      " 'Iteration: 663,\\tLoss: 0.13537,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:27] Train,\\tTimestamp: 1614896847049,\\tGlobal steps: 663,\\t'\n",
      " 'Iteration: 664,\\tLoss: 0.13522,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:27] Train,\\tTimestamp: 1614896847251,\\tGlobal steps: 664,\\t'\n",
      " 'Iteration: 665,\\tLoss: 0.13507,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:27] Train,\\tTimestamp: 1614896847452,\\tGlobal steps: 665,\\t'\n",
      " 'Iteration: 666,\\tLoss: 0.13493,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:29] Train,\\tTimestamp: 1614896849748,\\tGlobal steps: 666,\\t'\n",
      " 'Iteration: 668,\\tLoss: 0.13478,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:29] Train,\\tTimestamp: 1614896849954,\\tGlobal steps: 667,\\t'\n",
      " 'Iteration: 669,\\tLoss: 0.13463,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:30] Train,\\tTimestamp: 1614896850183,\\tGlobal steps: 668,\\t'\n",
      " 'Iteration: 670,\\tLoss: 0.13448,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:30] Train,\\tTimestamp: 1614896850423,\\tGlobal steps: 669,\\t'\n",
      " 'Iteration: 671,\\tLoss: 0.13434,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:30] Train,\\tTimestamp: 1614896850664,\\tGlobal steps: 670,\\t'\n",
      " 'Iteration: 672,\\tLoss: 0.13419,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:33] Train,\\tTimestamp: 1614896853167,\\tGlobal steps: 671,\\t'\n",
      " 'Iteration: 674,\\tLoss: 0.13405,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:33] Train,\\tTimestamp: 1614896853172,\\tGlobal steps: 672,\\t'\n",
      " 'Iteration: 674,\\tLoss: 0.1339,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:33] Train,\\tTimestamp: 1614896853369,\\tGlobal steps: 673,\\t'\n",
      " 'Iteration: 675,\\tLoss: 0.13376,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:33] Train,\\tTimestamp: 1614896853574,\\tGlobal steps: 674,\\t'\n",
      " 'Iteration: 676,\\tLoss: 0.13361,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:33] Train,\\tTimestamp: 1614896853725,\\tGlobal steps: 675,\\t'\n",
      " 'Iteration: 677,\\tLoss: 0.13347,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:33] Train,\\tTimestamp: 1614896853775,\\tGlobal steps: 676,\\t'\n",
      " 'Iteration: 678,\\tLoss: 0.13333,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:33] Train,\\tTimestamp: 1614896853976,\\tGlobal steps: 677,\\t'\n",
      " 'Iteration: 679,\\tLoss: 0.13318,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:34] Train,\\tTimestamp: 1614896854177,\\tGlobal steps: 678,\\t'\n",
      " 'Iteration: 680,\\tLoss: 0.13304,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:34] Train,\\tTimestamp: 1614896854291,\\tGlobal steps: 679,\\t'\n",
      " 'Iteration: 681,\\tLoss: 0.1329,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:34] Train,\\tTimestamp: 1614896854396,\\tGlobal steps: 680,\\t'\n",
      " 'Iteration: 682,\\tLoss: 0.13276,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:34] Train,\\tTimestamp: 1614896854598,\\tGlobal steps: 681,\\t'\n",
      " 'Iteration: 683,\\tLoss: 0.13261,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:27:34] Train,\\tTimestamp: 1614896854799,\\tGlobal steps: 682,\\t'\n",
      " 'Iteration: 684,\\tLoss: 0.13247,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:34] Train,\\tTimestamp: 1614896854845,\\tGlobal steps: 683,\\t'\n",
      " 'Iteration: 685,\\tLoss: 0.13233,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:35] Train,\\tTimestamp: 1614896855001,\\tGlobal steps: 684,\\t'\n",
      " 'Iteration: 686,\\tLoss: 0.13219,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:35] Train,\\tTimestamp: 1614896855203,\\tGlobal steps: 685,\\t'\n",
      " 'Iteration: 687,\\tLoss: 0.13205,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:35] Train,\\tTimestamp: 1614896855404,\\tGlobal steps: 686,\\t'\n",
      " 'Iteration: 688,\\tLoss: 0.13191,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:35] Train,\\tTimestamp: 1614896855649,\\tGlobal steps: 687,\\t'\n",
      " 'Iteration: 690,\\tLoss: 0.13177,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:35] Train,\\tTimestamp: 1614896855653,\\tGlobal steps: 688,\\t'\n",
      " 'Iteration: 690,\\tLoss: 0.13163,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:35] Train,\\tTimestamp: 1614896855855,\\tGlobal steps: 689,\\t'\n",
      " 'Iteration: 691,\\tLoss: 0.1315,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:36] Train,\\tTimestamp: 1614896856057,\\tGlobal steps: 690,\\t'\n",
      " 'Iteration: 692,\\tLoss: 0.13136,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:36] Train,\\tTimestamp: 1614896856210,\\tGlobal steps: 691,\\t'\n",
      " 'Iteration: 693,\\tLoss: 0.13122,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:36] Train,\\tTimestamp: 1614896856259,\\tGlobal steps: 692,\\t'\n",
      " 'Iteration: 694,\\tLoss: 0.13108,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:36] Train,\\tTimestamp: 1614896856461,\\tGlobal steps: 693,\\t'\n",
      " 'Iteration: 695,\\tLoss: 0.13095,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:36] Train,\\tTimestamp: 1614896856662,\\tGlobal steps: 694,\\t'\n",
      " 'Iteration: 696,\\tLoss: 0.13081,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:36] Train,\\tTimestamp: 1614896856771,\\tGlobal steps: 695,\\t'\n",
      " 'Iteration: 697,\\tLoss: 0.13067,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:36] Train,\\tTimestamp: 1614896856864,\\tGlobal steps: 696,\\t'\n",
      " 'Iteration: 698,\\tLoss: 0.13054,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:37] Train,\\tTimestamp: 1614896857066,\\tGlobal steps: 697,\\t'\n",
      " 'Iteration: 699,\\tLoss: 0.1304,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:37] Train,\\tTimestamp: 1614896857268,\\tGlobal steps: 698,\\t'\n",
      " 'Iteration: 700,\\tLoss: 0.13027,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:37] Train,\\tTimestamp: 1614896857327,\\tGlobal steps: 699,\\t'\n",
      " 'Iteration: 701,\\tLoss: 0.13013,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:37] Train,\\tTimestamp: 1614896857471,\\tGlobal steps: 700,\\t'\n",
      " 'Iteration: 702,\\tLoss: 0.13,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:37] Train,\\tTimestamp: 1614896857673,\\tGlobal steps: 701,\\t'\n",
      " 'Iteration: 703,\\tLoss: 0.12987,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:37] Train,\\tTimestamp: 1614896857874,\\tGlobal steps: 702,\\t'\n",
      " 'Iteration: 704,\\tLoss: 0.12973,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:38] Train,\\tTimestamp: 1614896858126,\\tGlobal steps: 703,\\t'\n",
      " 'Iteration: 706,\\tLoss: 0.1296,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:38] Train,\\tTimestamp: 1614896858130,\\tGlobal steps: 704,\\t'\n",
      " 'Iteration: 706,\\tLoss: 0.12947,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:38] Train,\\tTimestamp: 1614896858332,\\tGlobal steps: 705,\\t'\n",
      " 'Iteration: 707,\\tLoss: 0.12934,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:38] Train,\\tTimestamp: 1614896858533,\\tGlobal steps: 706,\\t'\n",
      " 'Iteration: 708,\\tLoss: 0.1292,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:38] Train,\\tTimestamp: 1614896858685,\\tGlobal steps: 707,\\t'\n",
      " 'Iteration: 709,\\tLoss: 0.12907,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:38] Train,\\tTimestamp: 1614896858735,\\tGlobal steps: 708,\\t'\n",
      " 'Iteration: 710,\\tLoss: 0.12894,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:38] Train,\\tTimestamp: 1614896858938,\\tGlobal steps: 709,\\t'\n",
      " 'Iteration: 711,\\tLoss: 0.12881,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:39] Train,\\tTimestamp: 1614896859139,\\tGlobal steps: 710,\\t'\n",
      " 'Iteration: 712,\\tLoss: 0.12868,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:39] Train,\\tTimestamp: 1614896859249,\\tGlobal steps: 711,\\t'\n",
      " 'Iteration: 713,\\tLoss: 0.12855,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:39] Train,\\tTimestamp: 1614896859341,\\tGlobal steps: 712,\\t'\n",
      " 'Iteration: 714,\\tLoss: 0.12842,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:39] Train,\\tTimestamp: 1614896859542,\\tGlobal steps: 713,\\t'\n",
      " 'Iteration: 715,\\tLoss: 0.12829,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:39] Train,\\tTimestamp: 1614896859744,\\tGlobal steps: 714,\\t'\n",
      " 'Iteration: 716,\\tLoss: 0.12816,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:39] Train,\\tTimestamp: 1614896859809,\\tGlobal steps: 715,\\t'\n",
      " 'Iteration: 717,\\tLoss: 0.12804,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:39] Train,\\tTimestamp: 1614896859946,\\tGlobal steps: 716,\\t'\n",
      " 'Iteration: 718,\\tLoss: 0.12791,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:40] Train,\\tTimestamp: 1614896860147,\\tGlobal steps: 717,\\t'\n",
      " 'Iteration: 719,\\tLoss: 0.12778,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:40] Train,\\tTimestamp: 1614896860348,\\tGlobal steps: 718,\\t'\n",
      " 'Iteration: 720,\\tLoss: 0.12766,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:40] Train,\\tTimestamp: 1614896860611,\\tGlobal steps: 719,\\t'\n",
      " 'Iteration: 722,\\tLoss: 0.12753,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:40] Train,\\tTimestamp: 1614896860615,\\tGlobal steps: 720,\\t'\n",
      " 'Iteration: 722,\\tLoss: 0.1274,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:40] Train,\\tTimestamp: 1614896860816,\\tGlobal steps: 721,\\t'\n",
      " 'Iteration: 723,\\tLoss: 0.12728,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:41] Train,\\tTimestamp: 1614896861018,\\tGlobal steps: 722,\\t'\n",
      " 'Iteration: 724,\\tLoss: 0.12715,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:41] Train,\\tTimestamp: 1614896861174,\\tGlobal steps: 723,\\t'\n",
      " 'Iteration: 725,\\tLoss: 0.12703,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:41] Train,\\tTimestamp: 1614896861221,\\tGlobal steps: 724,\\t'\n",
      " 'Iteration: 726,\\tLoss: 0.1269,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:41] Train,\\tTimestamp: 1614896861424,\\tGlobal steps: 725,\\t'\n",
      " 'Iteration: 727,\\tLoss: 0.12677,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:41] Train,\\tTimestamp: 1614896861625,\\tGlobal steps: 726,\\t'\n",
      " 'Iteration: 728,\\tLoss: 0.12665,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:41] Train,\\tTimestamp: 1614896861737,\\tGlobal steps: 727,\\t'\n",
      " 'Iteration: 729,\\tLoss: 0.12653,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:41] Train,\\tTimestamp: 1614896861827,\\tGlobal steps: 728,\\t'\n",
      " 'Iteration: 730,\\tLoss: 0.1264,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:42] Train,\\tTimestamp: 1614896862028,\\tGlobal steps: 729,\\t'\n",
      " 'Iteration: 731,\\tLoss: 0.12628,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:42] Train,\\tTimestamp: 1614896862229,\\tGlobal steps: 730,\\t'\n",
      " 'Iteration: 732,\\tLoss: 0.12615,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:42] Train,\\tTimestamp: 1614896862295,\\tGlobal steps: 731,\\t'\n",
      " 'Iteration: 733,\\tLoss: 0.12603,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:42] Train,\\tTimestamp: 1614896862432,\\tGlobal steps: 732,\\t'\n",
      " 'Iteration: 734,\\tLoss: 0.12591,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:42] Train,\\tTimestamp: 1614896862633,\\tGlobal steps: 733,\\t'\n",
      " 'Iteration: 735,\\tLoss: 0.12579,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:42] Train,\\tTimestamp: 1614896862834,\\tGlobal steps: 734,\\t'\n",
      " 'Iteration: 736,\\tLoss: 0.12567,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:43] Train,\\tTimestamp: 1614896863099,\\tGlobal steps: 735,\\t'\n",
      " 'Iteration: 738,\\tLoss: 0.12554,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:43] Train,\\tTimestamp: 1614896863102,\\tGlobal steps: 736,\\t'\n",
      " 'Iteration: 738,\\tLoss: 0.12542,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:43] Train,\\tTimestamp: 1614896863302,\\tGlobal steps: 737,\\t'\n",
      " 'Iteration: 739,\\tLoss: 0.1253,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:43] Train,\\tTimestamp: 1614896863504,\\tGlobal steps: 738,\\t'\n",
      " 'Iteration: 740,\\tLoss: 0.12519,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:43] Train,\\tTimestamp: 1614896863657,\\tGlobal steps: 739,\\t'\n",
      " 'Iteration: 741,\\tLoss: 0.12507,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:43] Train,\\tTimestamp: 1614896863705,\\tGlobal steps: 740,\\t'\n",
      " 'Iteration: 742,\\tLoss: 0.12495,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:43] Train,\\tTimestamp: 1614896863906,\\tGlobal steps: 741,\\t'\n",
      " 'Iteration: 743,\\tLoss: 0.12483,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:44] Train,\\tTimestamp: 1614896864107,\\tGlobal steps: 742,\\t'\n",
      " 'Iteration: 744,\\tLoss: 0.12471,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:44] Train,\\tTimestamp: 1614896864219,\\tGlobal steps: 743,\\t'\n",
      " 'Iteration: 745,\\tLoss: 0.12459,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:44] Train,\\tTimestamp: 1614896864309,\\tGlobal steps: 744,\\t'\n",
      " 'Iteration: 746,\\tLoss: 0.12447,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:44] Train,\\tTimestamp: 1614896864525,\\tGlobal steps: 745,\\t'\n",
      " 'Iteration: 747,\\tLoss: 0.12436,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:44] Train,\\tTimestamp: 1614896864725,\\tGlobal steps: 746,\\t'\n",
      " 'Iteration: 748,\\tLoss: 0.12424,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:44] Train,\\tTimestamp: 1614896864782,\\tGlobal steps: 747,\\t'\n",
      " 'Iteration: 749,\\tLoss: 0.12412,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:44] Train,\\tTimestamp: 1614896864927,\\tGlobal steps: 748,\\t'\n",
      " 'Iteration: 750,\\tLoss: 0.124,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:45] Train,\\tTimestamp: 1614896865128,\\tGlobal steps: 749,\\t'\n",
      " 'Iteration: 751,\\tLoss: 0.12388,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:45] Train,\\tTimestamp: 1614896865329,\\tGlobal steps: 750,\\t'\n",
      " 'Iteration: 752,\\tLoss: 0.12377,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:45] Train,\\tTimestamp: 1614896865580,\\tGlobal steps: 751,\\t'\n",
      " 'Iteration: 754,\\tLoss: 0.12365,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:45] Train,\\tTimestamp: 1614896865583,\\tGlobal steps: 752,\\t'\n",
      " 'Iteration: 754,\\tLoss: 0.12353,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:27:45] Train,\\tTimestamp: 1614896865784,\\tGlobal steps: 753,\\t'\n",
      " 'Iteration: 755,\\tLoss: 0.12342,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:45] Train,\\tTimestamp: 1614896865986,\\tGlobal steps: 754,\\t'\n",
      " 'Iteration: 756,\\tLoss: 0.1233,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:46] Train,\\tTimestamp: 1614896866143,\\tGlobal steps: 755,\\t'\n",
      " 'Iteration: 757,\\tLoss: 0.12319,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:46] Train,\\tTimestamp: 1614896866188,\\tGlobal steps: 756,\\t'\n",
      " 'Iteration: 758,\\tLoss: 0.12311,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:46] Train,\\tTimestamp: 1614896866389,\\tGlobal steps: 757,\\t'\n",
      " 'Iteration: 759,\\tLoss: 0.123,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:46] Train,\\tTimestamp: 1614896866592,\\tGlobal steps: 758,\\t'\n",
      " 'Iteration: 760,\\tLoss: 0.12288,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:46] Train,\\tTimestamp: 1614896866709,\\tGlobal steps: 759,\\t'\n",
      " 'Iteration: 761,\\tLoss: 0.12277,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:46] Train,\\tTimestamp: 1614896866794,\\tGlobal steps: 760,\\t'\n",
      " 'Iteration: 762,\\tLoss: 0.12265,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:46] Train,\\tTimestamp: 1614896866997,\\tGlobal steps: 761,\\t'\n",
      " 'Iteration: 763,\\tLoss: 0.12254,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:47] Train,\\tTimestamp: 1614896867196,\\tGlobal steps: 762,\\t'\n",
      " 'Iteration: 764,\\tLoss: 0.12243,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:47] Train,\\tTimestamp: 1614896867267,\\tGlobal steps: 763,\\t'\n",
      " 'Iteration: 765,\\tLoss: 0.12231,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:47] Train,\\tTimestamp: 1614896867395,\\tGlobal steps: 764,\\t'\n",
      " 'Iteration: 766,\\tLoss: 0.1222,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:47] Train,\\tTimestamp: 1614896867592,\\tGlobal steps: 765,\\t'\n",
      " 'Iteration: 767,\\tLoss: 0.12209,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:47] Train,\\tTimestamp: 1614896867790,\\tGlobal steps: 766,\\t'\n",
      " 'Iteration: 768,\\tLoss: 0.12198,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:48] Train,\\tTimestamp: 1614896868072,\\tGlobal steps: 767,\\t'\n",
      " 'Iteration: 770,\\tLoss: 0.12187,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:48] Train,\\tTimestamp: 1614896868075,\\tGlobal steps: 768,\\t'\n",
      " 'Iteration: 770,\\tLoss: 0.12176,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:48] Train,\\tTimestamp: 1614896868272,\\tGlobal steps: 769,\\t'\n",
      " 'Iteration: 771,\\tLoss: 0.12165,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:48] Train,\\tTimestamp: 1614896868635,\\tGlobal steps: 770,\\t'\n",
      " 'Iteration: 772,\\tLoss: 0.12154,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:49] Train,\\tTimestamp: 1614896869201,\\tGlobal steps: 771,\\t'\n",
      " 'Iteration: 773,\\tLoss: 0.12143,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:49] Train,\\tTimestamp: 1614896869762,\\tGlobal steps: 772,\\t'\n",
      " 'Iteration: 774,\\tLoss: 0.12132,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:50] Train,\\tTimestamp: 1614896870327,\\tGlobal steps: 773,\\t'\n",
      " 'Iteration: 775,\\tLoss: 0.12121,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:50] Train,\\tTimestamp: 1614896870891,\\tGlobal steps: 774,\\t'\n",
      " 'Iteration: 776,\\tLoss: 0.1211,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:51] Train,\\tTimestamp: 1614896871453,\\tGlobal steps: 775,\\t'\n",
      " 'Iteration: 777,\\tLoss: 0.12099,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:52] Train,\\tTimestamp: 1614896872013,\\tGlobal steps: 776,\\t'\n",
      " 'Iteration: 778,\\tLoss: 0.12087,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:52] Train,\\tTimestamp: 1614896872578,\\tGlobal steps: 777,\\t'\n",
      " 'Iteration: 779,\\tLoss: 0.12077,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:53] Train,\\tTimestamp: 1614896873137,\\tGlobal steps: 778,\\t'\n",
      " 'Iteration: 780,\\tLoss: 0.12066,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:53] Train,\\tTimestamp: 1614896873698,\\tGlobal steps: 779,\\t'\n",
      " 'Iteration: 781,\\tLoss: 0.12055,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:27:54] Train,\\tTimestamp: 1614896874363,\\tGlobal steps: 780,\\t'\n",
      " 'Iteration: 0,\\tLoss: 0.03955,\\tAccuracy: 0.0\\n'\n",
      " '[2021-03-04 17:27:54] Train,\\tTimestamp: 1614896874370,\\tGlobal steps: 781,\\t'\n",
      " 'Iteration: 1,\\tLoss: 0.03786,\\tAccuracy: 0.00039\\n'\n",
      " '[2021-03-04 17:27:54] Train,\\tTimestamp: 1614896874736,\\tGlobal steps: 782,\\t'\n",
      " 'Iteration: 3,\\tLoss: 0.03713,\\tAccuracy: 0.00057\\n'\n",
      " '[2021-03-04 17:27:54] Train,\\tTimestamp: 1614896874937,\\tGlobal steps: 783,\\t'\n",
      " 'Iteration: 4,\\tLoss: 0.03691,\\tAccuracy: 0.00059\\n'\n",
      " '[2021-03-04 17:27:55] Train,\\tTimestamp: 1614896875081,\\tGlobal steps: 784,\\t'\n",
      " 'Iteration: 5,\\tLoss: 0.03682,\\tAccuracy: 0.00056\\n'\n",
      " '[2021-03-04 17:27:55] Train,\\tTimestamp: 1614896875139,\\tGlobal steps: 785,\\t'\n",
      " 'Iteration: 6,\\tLoss: 0.03668,\\tAccuracy: 0.0007\\n'\n",
      " '[2021-03-04 17:27:55] Train,\\tTimestamp: 1614896875340,\\tGlobal steps: 786,\\t'\n",
      " 'Iteration: 7,\\tLoss: 0.03666,\\tAccuracy: 0.00071\\n'\n",
      " '[2021-03-04 17:27:55] Train,\\tTimestamp: 1614896875541,\\tGlobal steps: 787,\\t'\n",
      " 'Iteration: 8,\\tLoss: 0.0366,\\tAccuracy: 0.00074\\n'\n",
      " '[2021-03-04 17:27:55] Train,\\tTimestamp: 1614896875647,\\tGlobal steps: 788,\\t'\n",
      " 'Iteration: 9,\\tLoss: 0.03654,\\tAccuracy: 0.00078\\n'\n",
      " '[2021-03-04 17:27:55] Train,\\tTimestamp: 1614896875743,\\tGlobal steps: 789,\\t'\n",
      " 'Iteration: 10,\\tLoss: 0.03648,\\tAccuracy: 0.00077\\n'\n",
      " '[2021-03-04 17:27:55] Train,\\tTimestamp: 1614896875945,\\tGlobal steps: 790,\\t'\n",
      " 'Iteration: 11,\\tLoss: 0.03643,\\tAccuracy: 0.0008\\n'\n",
      " '[2021-03-04 17:27:56] Train,\\tTimestamp: 1614896876145,\\tGlobal steps: 791,\\t'\n",
      " 'Iteration: 12,\\tLoss: 0.03636,\\tAccuracy: 0.00083\\n'\n",
      " '[2021-03-04 17:27:56] Train,\\tTimestamp: 1614896876206,\\tGlobal steps: 792,\\t'\n",
      " 'Iteration: 13,\\tLoss: 0.03633,\\tAccuracy: 0.00085\\n'\n",
      " '[2021-03-04 17:27:56] Train,\\tTimestamp: 1614896876347,\\tGlobal steps: 793,\\t'\n",
      " 'Iteration: 14,\\tLoss: 0.03634,\\tAccuracy: 0.00086\\n'\n",
      " '[2021-03-04 17:27:56] Train,\\tTimestamp: 1614896876548,\\tGlobal steps: 794,\\t'\n",
      " 'Iteration: 15,\\tLoss: 0.03631,\\tAccuracy: 0.00089\\n'\n",
      " '[2021-03-04 17:27:56] Train,\\tTimestamp: 1614896876748,\\tGlobal steps: 795,\\t'\n",
      " 'Iteration: 16,\\tLoss: 0.03631,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:27:57] Train,\\tTimestamp: 1614896877008,\\tGlobal steps: 796,\\t'\n",
      " 'Iteration: 18,\\tLoss: 0.03631,\\tAccuracy: 0.00089\\n'\n",
      " '[2021-03-04 17:27:57] Train,\\tTimestamp: 1614896877010,\\tGlobal steps: 797,\\t'\n",
      " 'Iteration: 18,\\tLoss: 0.03632,\\tAccuracy: 0.00089\\n'\n",
      " '[2021-03-04 17:27:57] Train,\\tTimestamp: 1614896877210,\\tGlobal steps: 798,\\t'\n",
      " 'Iteration: 19,\\tLoss: 0.03631,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:27:57] Train,\\tTimestamp: 1614896877411,\\tGlobal steps: 799,\\t'\n",
      " 'Iteration: 20,\\tLoss: 0.0363,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:27:57] Train,\\tTimestamp: 1614896877569,\\tGlobal steps: 800,\\t'\n",
      " 'Iteration: 21,\\tLoss: 0.03631,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:27:57] Train,\\tTimestamp: 1614896877611,\\tGlobal steps: 801,\\t'\n",
      " 'Iteration: 22,\\tLoss: 0.0363,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:27:57] Train,\\tTimestamp: 1614896877813,\\tGlobal steps: 802,\\t'\n",
      " 'Iteration: 23,\\tLoss: 0.0363,\\tAccuracy: 0.00087\\n'\n",
      " '[2021-03-04 17:27:58] Train,\\tTimestamp: 1614896878015,\\tGlobal steps: 803,\\t'\n",
      " 'Iteration: 24,\\tLoss: 0.03629,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:27:58] Train,\\tTimestamp: 1614896878139,\\tGlobal steps: 804,\\t'\n",
      " 'Iteration: 25,\\tLoss: 0.03628,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:27:58] Train,\\tTimestamp: 1614896878216,\\tGlobal steps: 805,\\t'\n",
      " 'Iteration: 26,\\tLoss: 0.03627,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:27:58] Train,\\tTimestamp: 1614896878418,\\tGlobal steps: 806,\\t'\n",
      " 'Iteration: 27,\\tLoss: 0.03627,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:27:58] Train,\\tTimestamp: 1614896878619,\\tGlobal steps: 807,\\t'\n",
      " 'Iteration: 28,\\tLoss: 0.03628,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:27:58] Train,\\tTimestamp: 1614896878702,\\tGlobal steps: 808,\\t'\n",
      " 'Iteration: 29,\\tLoss: 0.03627,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:27:58] Train,\\tTimestamp: 1614896878820,\\tGlobal steps: 809,\\t'\n",
      " 'Iteration: 30,\\tLoss: 0.03626,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:27:59] Train,\\tTimestamp: 1614896879021,\\tGlobal steps: 810,\\t'\n",
      " 'Iteration: 31,\\tLoss: 0.03627,\\tAccuracy: 0.00089\\n'\n",
      " '[2021-03-04 17:27:59] Train,\\tTimestamp: 1614896879222,\\tGlobal steps: 811,\\t'\n",
      " 'Iteration: 32,\\tLoss: 0.03628,\\tAccuracy: 0.00088\\n'\n",
      " '[2021-03-04 17:27:59] Train,\\tTimestamp: 1614896879504,\\tGlobal steps: 812,\\t'\n",
      " 'Iteration: 33,\\tLoss: 0.03626,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:27:59] Train,\\tTimestamp: 1614896879509,\\tGlobal steps: 813,\\t'\n",
      " 'Iteration: 34,\\tLoss: 0.03626,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:27:59] Train,\\tTimestamp: 1614896879705,\\tGlobal steps: 814,\\t'\n",
      " 'Iteration: 35,\\tLoss: 0.03628,\\tAccuracy: 0.0009\\n'\n",
      " '[2021-03-04 17:27:59] Train,\\tTimestamp: 1614896879906,\\tGlobal steps: 815,\\t'\n",
      " 'Iteration: 36,\\tLoss: 0.03628,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:28:00] Train,\\tTimestamp: 1614896880070,\\tGlobal steps: 816,\\t'\n",
      " 'Iteration: 37,\\tLoss: 0.03628,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:00] Train,\\tTimestamp: 1614896880108,\\tGlobal steps: 817,\\t'\n",
      " 'Iteration: 38,\\tLoss: 0.03628,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:00] Train,\\tTimestamp: 1614896880311,\\tGlobal steps: 818,\\t'\n",
      " 'Iteration: 39,\\tLoss: 0.03627,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:00] Train,\\tTimestamp: 1614896880515,\\tGlobal steps: 819,\\t'\n",
      " 'Iteration: 40,\\tLoss: 0.03627,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:00] Train,\\tTimestamp: 1614896880637,\\tGlobal steps: 820,\\t'\n",
      " 'Iteration: 41,\\tLoss: 0.03627,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:00] Train,\\tTimestamp: 1614896880716,\\tGlobal steps: 821,\\t'\n",
      " 'Iteration: 42,\\tLoss: 0.03628,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:00] Train,\\tTimestamp: 1614896880918,\\tGlobal steps: 822,\\t'\n",
      " 'Iteration: 43,\\tLoss: 0.03628,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:01] Train,\\tTimestamp: 1614896881120,\\tGlobal steps: 823,\\t'\n",
      " 'Iteration: 44,\\tLoss: 0.03628,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:01] Train,\\tTimestamp: 1614896881200,\\tGlobal steps: 824,\\t'\n",
      " 'Iteration: 45,\\tLoss: 0.03626,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:01] Train,\\tTimestamp: 1614896881321,\\tGlobal steps: 825,\\t'\n",
      " 'Iteration: 46,\\tLoss: 0.03625,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:01] Train,\\tTimestamp: 1614896881522,\\tGlobal steps: 826,\\t'\n",
      " 'Iteration: 47,\\tLoss: 0.03625,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:01] Train,\\tTimestamp: 1614896881724,\\tGlobal steps: 827,\\t'\n",
      " 'Iteration: 48,\\tLoss: 0.03626,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:02] Train,\\tTimestamp: 1614896882007,\\tGlobal steps: 828,\\t'\n",
      " 'Iteration: 50,\\tLoss: 0.03625,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:02] Train,\\tTimestamp: 1614896882009,\\tGlobal steps: 829,\\t'\n",
      " 'Iteration: 50,\\tLoss: 0.03625,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:02] Train,\\tTimestamp: 1614896882209,\\tGlobal steps: 830,\\t'\n",
      " 'Iteration: 51,\\tLoss: 0.03625,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:02] Train,\\tTimestamp: 1614896882410,\\tGlobal steps: 831,\\t'\n",
      " 'Iteration: 52,\\tLoss: 0.03624,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:02] Train,\\tTimestamp: 1614896882573,\\tGlobal steps: 832,\\t'\n",
      " 'Iteration: 53,\\tLoss: 0.03623,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:02] Train,\\tTimestamp: 1614896882612,\\tGlobal steps: 833,\\t'\n",
      " 'Iteration: 54,\\tLoss: 0.03623,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:02] Train,\\tTimestamp: 1614896882814,\\tGlobal steps: 834,\\t'\n",
      " 'Iteration: 55,\\tLoss: 0.03623,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:03] Train,\\tTimestamp: 1614896883015,\\tGlobal steps: 835,\\t'\n",
      " 'Iteration: 56,\\tLoss: 0.03623,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:03] Train,\\tTimestamp: 1614896883138,\\tGlobal steps: 836,\\t'\n",
      " 'Iteration: 57,\\tLoss: 0.03622,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:03] Train,\\tTimestamp: 1614896883218,\\tGlobal steps: 837,\\t'\n",
      " 'Iteration: 58,\\tLoss: 0.03621,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:03] Train,\\tTimestamp: 1614896883420,\\tGlobal steps: 838,\\t'\n",
      " 'Iteration: 59,\\tLoss: 0.03622,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:03] Train,\\tTimestamp: 1614896883621,\\tGlobal steps: 839,\\t'\n",
      " 'Iteration: 60,\\tLoss: 0.03621,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:03] Train,\\tTimestamp: 1614896883701,\\tGlobal steps: 840,\\t'\n",
      " 'Iteration: 61,\\tLoss: 0.0362,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:03] Train,\\tTimestamp: 1614896883822,\\tGlobal steps: 841,\\t'\n",
      " 'Iteration: 62,\\tLoss: 0.0362,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:04] Train,\\tTimestamp: 1614896884023,\\tGlobal steps: 842,\\t'\n",
      " 'Iteration: 63,\\tLoss: 0.03619,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:04] Train,\\tTimestamp: 1614896884225,\\tGlobal steps: 843,\\t'\n",
      " 'Iteration: 64,\\tLoss: 0.0362,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:04] Train,\\tTimestamp: 1614896884507,\\tGlobal steps: 844,\\t'\n",
      " 'Iteration: 66,\\tLoss: 0.0362,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:28:04] Train,\\tTimestamp: 1614896884510,\\tGlobal steps: 845,\\t'\n",
      " 'Iteration: 66,\\tLoss: 0.0362,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:04] Train,\\tTimestamp: 1614896884711,\\tGlobal steps: 846,\\t'\n",
      " 'Iteration: 67,\\tLoss: 0.03619,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:28:04] Train,\\tTimestamp: 1614896884912,\\tGlobal steps: 847,\\t'\n",
      " 'Iteration: 68,\\tLoss: 0.03619,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:28:05] Train,\\tTimestamp: 1614896885071,\\tGlobal steps: 848,\\t'\n",
      " 'Iteration: 69,\\tLoss: 0.03619,\\tAccuracy: 0.00091\\n'\n",
      " '[2021-03-04 17:28:05] Train,\\tTimestamp: 1614896885113,\\tGlobal steps: 849,\\t'\n",
      " 'Iteration: 70,\\tLoss: 0.03618,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:05] Train,\\tTimestamp: 1614896885315,\\tGlobal steps: 850,\\t'\n",
      " 'Iteration: 71,\\tLoss: 0.03618,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:05] Train,\\tTimestamp: 1614896885516,\\tGlobal steps: 851,\\t'\n",
      " 'Iteration: 72,\\tLoss: 0.03618,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:05] Train,\\tTimestamp: 1614896885638,\\tGlobal steps: 852,\\t'\n",
      " 'Iteration: 73,\\tLoss: 0.03618,\\tAccuracy: 0.00092\\n'\n",
      " '[2021-03-04 17:28:05] Train,\\tTimestamp: 1614896885717,\\tGlobal steps: 853,\\t'\n",
      " 'Iteration: 74,\\tLoss: 0.03618,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:05] Train,\\tTimestamp: 1614896885919,\\tGlobal steps: 854,\\t'\n",
      " 'Iteration: 75,\\tLoss: 0.03617,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:06] Train,\\tTimestamp: 1614896886121,\\tGlobal steps: 855,\\t'\n",
      " 'Iteration: 76,\\tLoss: 0.03618,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:06] Train,\\tTimestamp: 1614896886207,\\tGlobal steps: 856,\\t'\n",
      " 'Iteration: 77,\\tLoss: 0.03617,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:06] Train,\\tTimestamp: 1614896886322,\\tGlobal steps: 857,\\t'\n",
      " 'Iteration: 78,\\tLoss: 0.03617,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:06] Train,\\tTimestamp: 1614896886524,\\tGlobal steps: 858,\\t'\n",
      " 'Iteration: 79,\\tLoss: 0.03618,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:06] Train,\\tTimestamp: 1614896886725,\\tGlobal steps: 859,\\t'\n",
      " 'Iteration: 80,\\tLoss: 0.03618,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:07] Train,\\tTimestamp: 1614896887011,\\tGlobal steps: 860,\\t'\n",
      " 'Iteration: 82,\\tLoss: 0.03618,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:07] Train,\\tTimestamp: 1614896887014,\\tGlobal steps: 861,\\t'\n",
      " 'Iteration: 82,\\tLoss: 0.03618,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:07] Train,\\tTimestamp: 1614896887214,\\tGlobal steps: 862,\\t'\n",
      " 'Iteration: 83,\\tLoss: 0.03618,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:07] Train,\\tTimestamp: 1614896887415,\\tGlobal steps: 863,\\t'\n",
      " 'Iteration: 84,\\tLoss: 0.03618,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:07] Train,\\tTimestamp: 1614896887573,\\tGlobal steps: 864,\\t'\n",
      " 'Iteration: 85,\\tLoss: 0.03618,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:07] Train,\\tTimestamp: 1614896887616,\\tGlobal steps: 865,\\t'\n",
      " 'Iteration: 86,\\tLoss: 0.03619,\\tAccuracy: 0.00093\\n'\n",
      " '[2021-03-04 17:28:07] Train,\\tTimestamp: 1614896887817,\\tGlobal steps: 866,\\t'\n",
      " 'Iteration: 87,\\tLoss: 0.03619,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:08] Train,\\tTimestamp: 1614896888021,\\tGlobal steps: 867,\\t'\n",
      " 'Iteration: 88,\\tLoss: 0.03618,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:08] Train,\\tTimestamp: 1614896888142,\\tGlobal steps: 868,\\t'\n",
      " 'Iteration: 89,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:08] Train,\\tTimestamp: 1614896888223,\\tGlobal steps: 869,\\t'\n",
      " 'Iteration: 90,\\tLoss: 0.03618,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:08] Train,\\tTimestamp: 1614896888425,\\tGlobal steps: 870,\\t'\n",
      " 'Iteration: 91,\\tLoss: 0.03618,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:08] Train,\\tTimestamp: 1614896888626,\\tGlobal steps: 871,\\t'\n",
      " 'Iteration: 92,\\tLoss: 0.03618,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:08] Train,\\tTimestamp: 1614896888703,\\tGlobal steps: 872,\\t'\n",
      " 'Iteration: 93,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:08] Train,\\tTimestamp: 1614896888827,\\tGlobal steps: 873,\\t'\n",
      " 'Iteration: 94,\\tLoss: 0.03619,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:09] Train,\\tTimestamp: 1614896889029,\\tGlobal steps: 874,\\t'\n",
      " 'Iteration: 95,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:09] Train,\\tTimestamp: 1614896889231,\\tGlobal steps: 875,\\t'\n",
      " 'Iteration: 96,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:09] Train,\\tTimestamp: 1614896889509,\\tGlobal steps: 876,\\t'\n",
      " 'Iteration: 98,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:09] Train,\\tTimestamp: 1614896889513,\\tGlobal steps: 877,\\t'\n",
      " 'Iteration: 98,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:09] Train,\\tTimestamp: 1614896889713,\\tGlobal steps: 878,\\t'\n",
      " 'Iteration: 99,\\tLoss: 0.03619,\\tAccuracy: 0.00094\\n'\n",
      " '[2021-03-04 17:28:09] Train,\\tTimestamp: 1614896889914,\\tGlobal steps: 879,\\t'\n",
      " 'Iteration: 100,\\tLoss: 0.03618,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:10] Train,\\tTimestamp: 1614896890074,\\tGlobal steps: 880,\\t'\n",
      " 'Iteration: 101,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:10] Train,\\tTimestamp: 1614896890115,\\tGlobal steps: 881,\\t'\n",
      " 'Iteration: 102,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:10] Train,\\tTimestamp: 1614896890317,\\tGlobal steps: 882,\\t'\n",
      " 'Iteration: 103,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:10] Train,\\tTimestamp: 1614896890518,\\tGlobal steps: 883,\\t'\n",
      " 'Iteration: 104,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:10] Train,\\tTimestamp: 1614896890644,\\tGlobal steps: 884,\\t'\n",
      " 'Iteration: 105,\\tLoss: 0.03619,\\tAccuracy: 0.00095\\n'\n",
      " '[2021-03-04 17:28:10] Train,\\tTimestamp: 1614896890720,\\tGlobal steps: 885,\\t'\n",
      " 'Iteration: 106,\\tLoss: 0.0362,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:28:10] Train,\\tTimestamp: 1614896890922,\\tGlobal steps: 886,\\t'\n",
      " 'Iteration: 107,\\tLoss: 0.03621,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:28:11] Train,\\tTimestamp: 1614896891123,\\tGlobal steps: 887,\\t'\n",
      " 'Iteration: 108,\\tLoss: 0.03621,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:28:11] Train,\\tTimestamp: 1614896891205,\\tGlobal steps: 888,\\t'\n",
      " 'Iteration: 109,\\tLoss: 0.03621,\\tAccuracy: 0.00097\\n'\n",
      " '[2021-03-04 17:28:11] Train,\\tTimestamp: 1614896891325,\\tGlobal steps: 889,\\t'\n",
      " 'Iteration: 110,\\tLoss: 0.03621,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:28:11] Train,\\tTimestamp: 1614896891526,\\tGlobal steps: 890,\\t'\n",
      " 'Iteration: 111,\\tLoss: 0.03621,\\tAccuracy: 0.00096\\n'\n",
      " '[2021-03-04 17:28:11] Train,\\tTimestamp: 1614896891728,\\tGlobal steps: 891,\\t'\n",
      " 'Iteration: 112,\\tLoss: 0.03621,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:28:12] Train,\\tTimestamp: 1614896892012,\\tGlobal steps: 892,\\t'\n",
      " 'Iteration: 114,\\tLoss: 0.0362,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:28:12] Train,\\tTimestamp: 1614896892016,\\tGlobal steps: 893,\\t'\n",
      " 'Iteration: 114,\\tLoss: 0.03621,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:28:12] Train,\\tTimestamp: 1614896892219,\\tGlobal steps: 894,\\t'\n",
      " 'Iteration: 115,\\tLoss: 0.03621,\\tAccuracy: 0.00098\\n'\n",
      " '[2021-03-04 17:28:12] Train,\\tTimestamp: 1614896892421,\\tGlobal steps: 895,\\t'\n",
      " 'Iteration: 116,\\tLoss: 0.03621,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:28:12] Train,\\tTimestamp: 1614896892577,\\tGlobal steps: 896,\\t'\n",
      " 'Iteration: 117,\\tLoss: 0.03621,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:28:12] Train,\\tTimestamp: 1614896892623,\\tGlobal steps: 897,\\t'\n",
      " 'Iteration: 118,\\tLoss: 0.0362,\\tAccuracy: 0.00099\\n'\n",
      " '[2021-03-04 17:28:12] Train,\\tTimestamp: 1614896892826,\\tGlobal steps: 898,\\t'\n",
      " 'Iteration: 119,\\tLoss: 0.0362,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:28:13] Train,\\tTimestamp: 1614896893028,\\tGlobal steps: 899,\\t'\n",
      " 'Iteration: 120,\\tLoss: 0.0362,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:28:13] Train,\\tTimestamp: 1614896893147,\\tGlobal steps: 900,\\t'\n",
      " 'Iteration: 121,\\tLoss: 0.0362,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:28:13] Train,\\tTimestamp: 1614896893230,\\tGlobal steps: 901,\\t'\n",
      " 'Iteration: 122,\\tLoss: 0.0362,\\tAccuracy: 0.001\\n'\n",
      " '[2021-03-04 17:28:13] Train,\\tTimestamp: 1614896893432,\\tGlobal steps: 902,\\t'\n",
      " 'Iteration: 123,\\tLoss: 0.0362,\\tAccuracy: 0.00101\\n'\n",
      " '[2021-03-04 17:28:13] Train,\\tTimestamp: 1614896893633,\\tGlobal steps: 903,\\t'\n",
      " 'Iteration: 124,\\tLoss: 0.03619,\\tAccuracy: 0.00101\\n'\n",
      " '[2021-03-04 17:28:13] Train,\\tTimestamp: 1614896893711,\\tGlobal steps: 904,\\t'\n",
      " 'Iteration: 125,\\tLoss: 0.03619,\\tAccuracy: 0.00101\\n'\n",
      " '[2021-03-04 17:28:13] Train,\\tTimestamp: 1614896893836,\\tGlobal steps: 905,\\t'\n",
      " 'Iteration: 126,\\tLoss: 0.03619,\\tAccuracy: 0.00101\\n'\n",
      " '[2021-03-04 17:28:14] Train,\\tTimestamp: 1614896894038,\\tGlobal steps: 906,\\t'\n",
      " 'Iteration: 127,\\tLoss: 0.03619,\\tAccuracy: 0.00101\\n'\n",
      " '[2021-03-04 17:28:14] Train,\\tTimestamp: 1614896894239,\\tGlobal steps: 907,\\t'\n",
      " 'Iteration: 128,\\tLoss: 0.03619,\\tAccuracy: 0.00101\\n'\n",
      " '[2021-03-04 17:28:14] Train,\\tTimestamp: 1614896894520,\\tGlobal steps: 908,\\t'\n",
      " 'Iteration: 130,\\tLoss: 0.03619,\\tAccuracy: 0.00101\\n'\n",
      " '[2021-03-04 17:28:14] Train,\\tTimestamp: 1614896894522,\\tGlobal steps: 909,\\t'\n",
      " 'Iteration: 130,\\tLoss: 0.03619,\\tAccuracy: 0.00102\\n'\n",
      " '[2021-03-04 17:28:14] Train,\\tTimestamp: 1614896894722,\\tGlobal steps: 910,\\t'\n",
      " 'Iteration: 131,\\tLoss: 0.03619,\\tAccuracy: 0.00102\\n'\n",
      " '[2021-03-04 17:28:14] Train,\\tTimestamp: 1614896894922,\\tGlobal steps: 911,\\t'\n",
      " 'Iteration: 132,\\tLoss: 0.03619,\\tAccuracy: 0.00102\\n'\n",
      " '[2021-03-04 17:28:15] Train,\\tTimestamp: 1614896895083,\\tGlobal steps: 912,\\t'\n",
      " 'Iteration: 133,\\tLoss: 0.03618,\\tAccuracy: 0.00102\\n'\n",
      " '[2021-03-04 17:28:15] Train,\\tTimestamp: 1614896895123,\\tGlobal steps: 913,\\t'\n",
      " 'Iteration: 134,\\tLoss: 0.03618,\\tAccuracy: 0.00102\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '[2021-03-04 17:28:15] Train,\\tTimestamp: 1614896895325,\\tGlobal steps: 914,\\t'\n",
      " 'Iteration: 135,\\tLoss: 0.03618,\\tAccuracy: 0.00103\\n'\n",
      " '[2021-03-04 17:28:15] Train,\\tTimestamp: 1614896895528,\\tGlobal steps: 915,\\t'\n",
      " 'Iteration: 136,\\tLoss: 0.03618,\\tAccuracy: 0.00103\\n'\n",
      " '[2021-03-04 17:28:15] Train,\\tTimestamp: 1614896895653,\\tGlobal steps: 916,\\t'\n",
      " 'Iteration: 137,\\tLoss: 0.03618,\\tAccuracy: 0.00103\\n'\n",
      " '[2021-03-04 17:28:15] Train,\\tTimestamp: 1614896895730,\\tGlobal steps: 917,\\t'\n",
      " 'Iteration: 138,\\tLoss: 0.03618,\\tAccuracy: 0.00103\\n'\n",
      " '[2021-03-04 17:28:15] Train,\\tTimestamp: 1614896895932,\\tGlobal steps: 918,\\t'\n",
      " 'Iteration: 139,\\tLoss: 0.03618,\\tAccuracy: 0.00103\\n'\n",
      " '[2021-03-04 17:28:16] Train,\\tTimestamp: 1614896896134,\\tGlobal steps: 919,\\t'\n",
      " 'Iteration: 140,\\tLoss: 0.03618,\\tAccuracy: 0.00103\\n'\n",
      " '[2021-03-04 17:28:16] Train,\\tTimestamp: 1614896896217,\\tGlobal steps: 920,\\t'\n",
      " 'Iteration: 141,\\tLoss: 0.03617,\\tAccuracy: 0.00103\\n'\n",
      " '[2021-03-04 17:28:16] Train,\\tTimestamp: 1614896896335,\\tGlobal steps: 921,\\t'\n",
      " 'Iteration: 142,\\tLoss: 0.03617,\\tAccuracy: 0.00104\\n'\n",
      " '[2021-03-04 17:28:16] Train,\\tTimestamp: 1614896896537,\\tGlobal steps: 922,\\t'\n",
      " 'Iteration: 143,\\tLoss: 0.03617,\\tAccuracy: 0.00104\\n'\n",
      " '[2021-03-04 17:28:16] Train,\\tTimestamp: 1614896896739,\\tGlobal steps: 923,\\t'\n",
      " 'Iteration: 144,\\tLoss: 0.03617,\\tAccuracy: 0.00104\\n'\n",
      " '[2021-03-04 17:28:17] Train,\\tTimestamp: 1614896897022,\\tGlobal steps: 924,\\t'\n",
      " 'Iteration: 146,\\tLoss: 0.03617,\\tAccuracy: 0.00104\\n'\n",
      " '[2021-03-04 17:28:17] Train,\\tTimestamp: 1614896897026,\\tGlobal steps: 925,\\t'\n",
      " 'Iteration: 146,\\tLoss: 0.03616,\\tAccuracy: 0.00104\\n'\n",
      " '[2021-03-04 17:28:17] Train,\\tTimestamp: 1614896897227,\\tGlobal steps: 926,\\t'\n",
      " 'Iteration: 147,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:17] Train,\\tTimestamp: 1614896897428,\\tGlobal steps: 927,\\t'\n",
      " 'Iteration: 148,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:17] Train,\\tTimestamp: 1614896897588,\\tGlobal steps: 928,\\t'\n",
      " 'Iteration: 149,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:17] Train,\\tTimestamp: 1614896897630,\\tGlobal steps: 929,\\t'\n",
      " 'Iteration: 150,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:17] Train,\\tTimestamp: 1614896897832,\\tGlobal steps: 930,\\t'\n",
      " 'Iteration: 151,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:18] Train,\\tTimestamp: 1614896898035,\\tGlobal steps: 931,\\t'\n",
      " 'Iteration: 152,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:18] Train,\\tTimestamp: 1614896898158,\\tGlobal steps: 932,\\t'\n",
      " 'Iteration: 153,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:18] Train,\\tTimestamp: 1614896898237,\\tGlobal steps: 933,\\t'\n",
      " 'Iteration: 154,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:18] Train,\\tTimestamp: 1614896898440,\\tGlobal steps: 934,\\t'\n",
      " 'Iteration: 155,\\tLoss: 0.03618,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:18] Train,\\tTimestamp: 1614896898641,\\tGlobal steps: 935,\\t'\n",
      " 'Iteration: 156,\\tLoss: 0.03618,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:18] Train,\\tTimestamp: 1614896898724,\\tGlobal steps: 936,\\t'\n",
      " 'Iteration: 157,\\tLoss: 0.03618,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:18] Train,\\tTimestamp: 1614896898843,\\tGlobal steps: 937,\\t'\n",
      " 'Iteration: 158,\\tLoss: 0.03618,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:19] Train,\\tTimestamp: 1614896899046,\\tGlobal steps: 938,\\t'\n",
      " 'Iteration: 159,\\tLoss: 0.03619,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:19] Train,\\tTimestamp: 1614896899248,\\tGlobal steps: 939,\\t'\n",
      " 'Iteration: 160,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:19] Train,\\tTimestamp: 1614896899530,\\tGlobal steps: 940,\\t'\n",
      " 'Iteration: 162,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:19] Train,\\tTimestamp: 1614896899533,\\tGlobal steps: 941,\\t'\n",
      " 'Iteration: 162,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:19] Train,\\tTimestamp: 1614896899733,\\tGlobal steps: 942,\\t'\n",
      " 'Iteration: 163,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:19] Train,\\tTimestamp: 1614896899935,\\tGlobal steps: 943,\\t'\n",
      " 'Iteration: 164,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:20] Train,\\tTimestamp: 1614896900097,\\tGlobal steps: 944,\\t'\n",
      " 'Iteration: 165,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:20] Train,\\tTimestamp: 1614896900135,\\tGlobal steps: 945,\\t'\n",
      " 'Iteration: 166,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:20] Train,\\tTimestamp: 1614896900336,\\tGlobal steps: 946,\\t'\n",
      " 'Iteration: 167,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:20] Train,\\tTimestamp: 1614896900538,\\tGlobal steps: 947,\\t'\n",
      " 'Iteration: 168,\\tLoss: 0.03618,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:20] Train,\\tTimestamp: 1614896900671,\\tGlobal steps: 948,\\t'\n",
      " 'Iteration: 169,\\tLoss: 0.03618,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:20] Train,\\tTimestamp: 1614896900739,\\tGlobal steps: 949,\\t'\n",
      " 'Iteration: 170,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:20] Train,\\tTimestamp: 1614896900941,\\tGlobal steps: 950,\\t'\n",
      " 'Iteration: 171,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:21] Train,\\tTimestamp: 1614896901142,\\tGlobal steps: 951,\\t'\n",
      " 'Iteration: 172,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:21] Train,\\tTimestamp: 1614896901236,\\tGlobal steps: 952,\\t'\n",
      " 'Iteration: 173,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:21] Train,\\tTimestamp: 1614896901344,\\tGlobal steps: 953,\\t'\n",
      " 'Iteration: 174,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:21] Train,\\tTimestamp: 1614896901546,\\tGlobal steps: 954,\\t'\n",
      " 'Iteration: 175,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:21] Train,\\tTimestamp: 1614896901748,\\tGlobal steps: 955,\\t'\n",
      " 'Iteration: 176,\\tLoss: 0.03617,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:22] Train,\\tTimestamp: 1614896902044,\\tGlobal steps: 956,\\t'\n",
      " 'Iteration: 178,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:22] Train,\\tTimestamp: 1614896902047,\\tGlobal steps: 957,\\t'\n",
      " 'Iteration: 178,\\tLoss: 0.03618,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:22] Train,\\tTimestamp: 1614896902247,\\tGlobal steps: 958,\\t'\n",
      " 'Iteration: 179,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:22] Train,\\tTimestamp: 1614896902449,\\tGlobal steps: 959,\\t'\n",
      " 'Iteration: 180,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:22] Train,\\tTimestamp: 1614896902611,\\tGlobal steps: 960,\\t'\n",
      " 'Iteration: 181,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:22] Train,\\tTimestamp: 1614896902651,\\tGlobal steps: 961,\\t'\n",
      " 'Iteration: 182,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:22] Train,\\tTimestamp: 1614896902853,\\tGlobal steps: 962,\\t'\n",
      " 'Iteration: 183,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:23] Train,\\tTimestamp: 1614896903055,\\tGlobal steps: 963,\\t'\n",
      " 'Iteration: 184,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:23] Train,\\tTimestamp: 1614896903182,\\tGlobal steps: 964,\\t'\n",
      " 'Iteration: 185,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:23] Train,\\tTimestamp: 1614896903256,\\tGlobal steps: 965,\\t'\n",
      " 'Iteration: 186,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:23] Train,\\tTimestamp: 1614896903459,\\tGlobal steps: 966,\\t'\n",
      " 'Iteration: 187,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:23] Train,\\tTimestamp: 1614896903661,\\tGlobal steps: 967,\\t'\n",
      " 'Iteration: 188,\\tLoss: 0.03617,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:23] Train,\\tTimestamp: 1614896903747,\\tGlobal steps: 968,\\t'\n",
      " 'Iteration: 189,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:23] Train,\\tTimestamp: 1614896903862,\\tGlobal steps: 969,\\t'\n",
      " 'Iteration: 190,\\tLoss: 0.03617,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:24] Train,\\tTimestamp: 1614896904063,\\tGlobal steps: 970,\\t'\n",
      " 'Iteration: 191,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:24] Train,\\tTimestamp: 1614896904266,\\tGlobal steps: 971,\\t'\n",
      " 'Iteration: 192,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:24] Train,\\tTimestamp: 1614896904559,\\tGlobal steps: 972,\\t'\n",
      " 'Iteration: 194,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:24] Train,\\tTimestamp: 1614896904562,\\tGlobal steps: 973,\\t'\n",
      " 'Iteration: 194,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:24] Train,\\tTimestamp: 1614896904762,\\tGlobal steps: 974,\\t'\n",
      " 'Iteration: 195,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:24] Train,\\tTimestamp: 1614896904963,\\tGlobal steps: 975,\\t'\n",
      " 'Iteration: 196,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:25] Train,\\tTimestamp: 1614896905124,\\tGlobal steps: 976,\\t'\n",
      " 'Iteration: 197,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:25] Train,\\tTimestamp: 1614896905164,\\tGlobal steps: 977,\\t'\n",
      " 'Iteration: 198,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:25] Train,\\tTimestamp: 1614896905366,\\tGlobal steps: 978,\\t'\n",
      " 'Iteration: 199,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:25] Train,\\tTimestamp: 1614896905566,\\tGlobal steps: 979,\\t'\n",
      " 'Iteration: 200,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:25] Train,\\tTimestamp: 1614896905696,\\tGlobal steps: 980,\\t'\n",
      " 'Iteration: 201,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:25] Train,\\tTimestamp: 1614896905768,\\tGlobal steps: 981,\\t'\n",
      " 'Iteration: 202,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:25] Train,\\tTimestamp: 1614896905970,\\tGlobal steps: 982,\\t'\n",
      " 'Iteration: 203,\\tLoss: 0.03616,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:26] Train,\\tTimestamp: 1614896906172,\\tGlobal steps: 983,\\t'\n",
      " 'Iteration: 204,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:26] Train,\\tTimestamp: 1614896906260,\\tGlobal steps: 984,\\t'\n",
      " 'Iteration: 205,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:26] Train,\\tTimestamp: 1614896906376,\\tGlobal steps: 985,\\t'\n",
      " 'Iteration: 206,\\tLoss: 0.03616,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:26] Train,\\tTimestamp: 1614896906578,\\tGlobal steps: 986,\\t'\n",
      " 'Iteration: 207,\\tLoss: 0.03616,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:26] Train,\\tTimestamp: 1614896906780,\\tGlobal steps: 987,\\t'\n",
      " 'Iteration: 208,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:27] Train,\\tTimestamp: 1614896907072,\\tGlobal steps: 988,\\t'\n",
      " 'Iteration: 210,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:27] Train,\\tTimestamp: 1614896907074,\\tGlobal steps: 989,\\t'\n",
      " 'Iteration: 210,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:27] Train,\\tTimestamp: 1614896907275,\\tGlobal steps: 990,\\t'\n",
      " 'Iteration: 211,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:27] Train,\\tTimestamp: 1614896907477,\\tGlobal steps: 991,\\t'\n",
      " 'Iteration: 212,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:27] Train,\\tTimestamp: 1614896907638,\\tGlobal steps: 992,\\t'\n",
      " 'Iteration: 213,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:27] Train,\\tTimestamp: 1614896907678,\\tGlobal steps: 993,\\t'\n",
      " 'Iteration: 214,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:27] Train,\\tTimestamp: 1614896907880,\\tGlobal steps: 994,\\t'\n",
      " 'Iteration: 215,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:28] Train,\\tTimestamp: 1614896908082,\\tGlobal steps: 995,\\t'\n",
      " 'Iteration: 216,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:28] Train,\\tTimestamp: 1614896908213,\\tGlobal steps: 996,\\t'\n",
      " 'Iteration: 217,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:28] Train,\\tTimestamp: 1614896908284,\\tGlobal steps: 997,\\t'\n",
      " 'Iteration: 218,\\tLoss: 0.03615,\\tAccuracy: 0.00108\\n'\n",
      " '[2021-03-04 17:28:28] Train,\\tTimestamp: 1614896908486,\\tGlobal steps: 998,\\t'\n",
      " 'Iteration: 219,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:28] Train,\\tTimestamp: 1614896908688,\\tGlobal steps: 999,\\t'\n",
      " 'Iteration: 220,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:28] Train,\\tTimestamp: 1614896908779,\\tGlobal steps: '\n",
      " '1000,\\tIteration: 221,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:28] Train,\\tTimestamp: 1614896908890,\\tGlobal steps: '\n",
      " '1001,\\tIteration: 222,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:29] Train,\\tTimestamp: 1614896909092,\\tGlobal steps: '\n",
      " '1002,\\tIteration: 223,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:29] Train,\\tTimestamp: 1614896909294,\\tGlobal steps: '\n",
      " '1003,\\tIteration: 224,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:29] Train,\\tTimestamp: 1614896909588,\\tGlobal steps: '\n",
      " '1004,\\tIteration: 226,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:29] Train,\\tTimestamp: 1614896909590,\\tGlobal steps: '\n",
      " '1005,\\tIteration: 226,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:29] Train,\\tTimestamp: 1614896909791,\\tGlobal steps: '\n",
      " '1006,\\tIteration: 227,\\tLoss: 0.03614,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:29] Train,\\tTimestamp: 1614896909992,\\tGlobal steps: '\n",
      " '1007,\\tIteration: 228,\\tLoss: 0.03614,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:30] Train,\\tTimestamp: 1614896910153,\\tGlobal steps: '\n",
      " '1008,\\tIteration: 229,\\tLoss: 0.03614,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:30] Train,\\tTimestamp: 1614896910194,\\tGlobal steps: '\n",
      " '1009,\\tIteration: 230,\\tLoss: 0.03614,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:30] Train,\\tTimestamp: 1614896910398,\\tGlobal steps: '\n",
      " '1010,\\tIteration: 231,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:30] Train,\\tTimestamp: 1614896910601,\\tGlobal steps: '\n",
      " '1011,\\tIteration: 232,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:30] Train,\\tTimestamp: 1614896910727,\\tGlobal steps: '\n",
      " '1012,\\tIteration: 233,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:30] Train,\\tTimestamp: 1614896910804,\\tGlobal steps: '\n",
      " '1013,\\tIteration: 234,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:31] Train,\\tTimestamp: 1614896911006,\\tGlobal steps: '\n",
      " '1014,\\tIteration: 235,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:31] Train,\\tTimestamp: 1614896911207,\\tGlobal steps: '\n",
      " '1015,\\tIteration: 236,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:31] Train,\\tTimestamp: 1614896911293,\\tGlobal steps: '\n",
      " '1016,\\tIteration: 237,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:31] Train,\\tTimestamp: 1614896911409,\\tGlobal steps: '\n",
      " '1017,\\tIteration: 238,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:31] Train,\\tTimestamp: 1614896911610,\\tGlobal steps: '\n",
      " '1018,\\tIteration: 239,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:31] Train,\\tTimestamp: 1614896911812,\\tGlobal steps: '\n",
      " '1019,\\tIteration: 240,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:32] Train,\\tTimestamp: 1614896912103,\\tGlobal steps: '\n",
      " '1020,\\tIteration: 242,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:32] Train,\\tTimestamp: 1614896912106,\\tGlobal steps: '\n",
      " '1021,\\tIteration: 242,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:32] Train,\\tTimestamp: 1614896912307,\\tGlobal steps: '\n",
      " '1022,\\tIteration: 243,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:32] Train,\\tTimestamp: 1614896912509,\\tGlobal steps: '\n",
      " '1023,\\tIteration: 244,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:32] Train,\\tTimestamp: 1614896912671,\\tGlobal steps: '\n",
      " '1024,\\tIteration: 245,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:32] Train,\\tTimestamp: 1614896912711,\\tGlobal steps: '\n",
      " '1025,\\tIteration: 246,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:32] Train,\\tTimestamp: 1614896912914,\\tGlobal steps: '\n",
      " '1026,\\tIteration: 247,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:33] Train,\\tTimestamp: 1614896913116,\\tGlobal steps: '\n",
      " '1027,\\tIteration: 248,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:33] Train,\\tTimestamp: 1614896913243,\\tGlobal steps: '\n",
      " '1028,\\tIteration: 249,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:33] Train,\\tTimestamp: 1614896913317,\\tGlobal steps: '\n",
      " '1029,\\tIteration: 250,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:33] Train,\\tTimestamp: 1614896913520,\\tGlobal steps: '\n",
      " '1030,\\tIteration: 251,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:33] Train,\\tTimestamp: 1614896913723,\\tGlobal steps: '\n",
      " '1031,\\tIteration: 252,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:33] Train,\\tTimestamp: 1614896913809,\\tGlobal steps: '\n",
      " '1032,\\tIteration: 253,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:33] Train,\\tTimestamp: 1614896913925,\\tGlobal steps: '\n",
      " '1033,\\tIteration: 254,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:34] Train,\\tTimestamp: 1614896914128,\\tGlobal steps: '\n",
      " '1034,\\tIteration: 255,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:34] Train,\\tTimestamp: 1614896914330,\\tGlobal steps: '\n",
      " '1035,\\tIteration: 256,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:34] Train,\\tTimestamp: 1614896914617,\\tGlobal steps: '\n",
      " '1036,\\tIteration: 258,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:34] Train,\\tTimestamp: 1614896914621,\\tGlobal steps: '\n",
      " '1037,\\tIteration: 258,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:34] Train,\\tTimestamp: 1614896914821,\\tGlobal steps: '\n",
      " '1038,\\tIteration: 259,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:35] Train,\\tTimestamp: 1614896915023,\\tGlobal steps: '\n",
      " '1039,\\tIteration: 260,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:35] Train,\\tTimestamp: 1614896915186,\\tGlobal steps: '\n",
      " '1040,\\tIteration: 261,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:35] Train,\\tTimestamp: 1614896915224,\\tGlobal steps: '\n",
      " '1041,\\tIteration: 262,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:35] Train,\\tTimestamp: 1614896915426,\\tGlobal steps: '\n",
      " '1042,\\tIteration: 263,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:35] Train,\\tTimestamp: 1614896915627,\\tGlobal steps: '\n",
      " '1043,\\tIteration: 264,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:35] Train,\\tTimestamp: 1614896915760,\\tGlobal steps: '\n",
      " '1044,\\tIteration: 265,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:35] Train,\\tTimestamp: 1614896915828,\\tGlobal steps: '\n",
      " '1045,\\tIteration: 266,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:36] Train,\\tTimestamp: 1614896916031,\\tGlobal steps: '\n",
      " '1046,\\tIteration: 267,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:36] Train,\\tTimestamp: 1614896916232,\\tGlobal steps: '\n",
      " '1047,\\tIteration: 268,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:36] Train,\\tTimestamp: 1614896916327,\\tGlobal steps: '\n",
      " '1048,\\tIteration: 269,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:36] Train,\\tTimestamp: 1614896916434,\\tGlobal steps: '\n",
      " '1049,\\tIteration: 270,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:36] Train,\\tTimestamp: 1614896916635,\\tGlobal steps: '\n",
      " '1050,\\tIteration: 271,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:36] Train,\\tTimestamp: 1614896916836,\\tGlobal steps: '\n",
      " '1051,\\tIteration: 272,\\tLoss: 0.03614,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:37] Train,\\tTimestamp: 1614896917138,\\tGlobal steps: '\n",
      " '1052,\\tIteration: 274,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:37] Train,\\tTimestamp: 1614896917141,\\tGlobal steps: '\n",
      " '1053,\\tIteration: 274,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:37] Train,\\tTimestamp: 1614896917343,\\tGlobal steps: '\n",
      " '1054,\\tIteration: 275,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:37] Train,\\tTimestamp: 1614896917543,\\tGlobal steps: '\n",
      " '1055,\\tIteration: 276,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:37] Train,\\tTimestamp: 1614896917703,\\tGlobal steps: '\n",
      " '1056,\\tIteration: 277,\\tLoss: 0.03614,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:37] Train,\\tTimestamp: 1614896917744,\\tGlobal steps: '\n",
      " '1057,\\tIteration: 278,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:37] Train,\\tTimestamp: 1614896917945,\\tGlobal steps: '\n",
      " '1058,\\tIteration: 279,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:38] Train,\\tTimestamp: 1614896918146,\\tGlobal steps: '\n",
      " '1059,\\tIteration: 280,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:38] Train,\\tTimestamp: 1614896918278,\\tGlobal steps: '\n",
      " '1060,\\tIteration: 281,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:38] Train,\\tTimestamp: 1614896918348,\\tGlobal steps: '\n",
      " '1061,\\tIteration: 282,\\tLoss: 0.03614,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:38] Train,\\tTimestamp: 1614896918550,\\tGlobal steps: '\n",
      " '1062,\\tIteration: 283,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:38] Train,\\tTimestamp: 1614896918752,\\tGlobal steps: '\n",
      " '1063,\\tIteration: 284,\\tLoss: 0.03615,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:38] Train,\\tTimestamp: 1614896918844,\\tGlobal steps: '\n",
      " '1064,\\tIteration: 285,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:38] Train,\\tTimestamp: 1614896918954,\\tGlobal steps: '\n",
      " '1065,\\tIteration: 286,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:39] Train,\\tTimestamp: 1614896919156,\\tGlobal steps: '\n",
      " '1066,\\tIteration: 287,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:39] Train,\\tTimestamp: 1614896919358,\\tGlobal steps: '\n",
      " '1067,\\tIteration: 288,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:39] Train,\\tTimestamp: 1614896919656,\\tGlobal steps: '\n",
      " '1068,\\tIteration: 290,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:39] Train,\\tTimestamp: 1614896919658,\\tGlobal steps: '\n",
      " '1069,\\tIteration: 290,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:39] Train,\\tTimestamp: 1614896919859,\\tGlobal steps: '\n",
      " '1070,\\tIteration: 291,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:40] Train,\\tTimestamp: 1614896920061,\\tGlobal steps: '\n",
      " '1071,\\tIteration: 292,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:40] Train,\\tTimestamp: 1614896920223,\\tGlobal steps: '\n",
      " '1072,\\tIteration: 293,\\tLoss: 0.03617,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:40] Train,\\tTimestamp: 1614896920262,\\tGlobal steps: '\n",
      " '1073,\\tIteration: 294,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:40] Train,\\tTimestamp: 1614896920464,\\tGlobal steps: '\n",
      " '1074,\\tIteration: 295,\\tLoss: 0.03617,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:40] Train,\\tTimestamp: 1614896920667,\\tGlobal steps: '\n",
      " '1075,\\tIteration: 296,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:40] Train,\\tTimestamp: 1614896920798,\\tGlobal steps: '\n",
      " '1076,\\tIteration: 297,\\tLoss: 0.03617,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:40] Train,\\tTimestamp: 1614896920868,\\tGlobal steps: '\n",
      " '1077,\\tIteration: 298,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:41] Train,\\tTimestamp: 1614896921069,\\tGlobal steps: '\n",
      " '1078,\\tIteration: 299,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:41] Train,\\tTimestamp: 1614896921271,\\tGlobal steps: '\n",
      " '1079,\\tIteration: 300,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:41] Train,\\tTimestamp: 1614896921365,\\tGlobal steps: '\n",
      " '1080,\\tIteration: 301,\\tLoss: 0.03616,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:28:41] Train,\\tTimestamp: 1614896921475,\\tGlobal steps: '\n",
      " '1081,\\tIteration: 302,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:41] Train,\\tTimestamp: 1614896921677,\\tGlobal steps: '\n",
      " '1082,\\tIteration: 303,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:41] Train,\\tTimestamp: 1614896921880,\\tGlobal steps: '\n",
      " '1083,\\tIteration: 304,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:42] Train,\\tTimestamp: 1614896922176,\\tGlobal steps: '\n",
      " '1084,\\tIteration: 306,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:42] Train,\\tTimestamp: 1614896922179,\\tGlobal steps: '\n",
      " '1085,\\tIteration: 306,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:42] Train,\\tTimestamp: 1614896922380,\\tGlobal steps: '\n",
      " '1086,\\tIteration: 307,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:42] Train,\\tTimestamp: 1614896922582,\\tGlobal steps: '\n",
      " '1087,\\tIteration: 308,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:42] Train,\\tTimestamp: 1614896922745,\\tGlobal steps: '\n",
      " '1088,\\tIteration: 309,\\tLoss: 0.03617,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:42] Train,\\tTimestamp: 1614896922783,\\tGlobal steps: '\n",
      " '1089,\\tIteration: 310,\\tLoss: 0.03617,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:42] Train,\\tTimestamp: 1614896922984,\\tGlobal steps: '\n",
      " '1090,\\tIteration: 311,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:43] Train,\\tTimestamp: 1614896923184,\\tGlobal steps: '\n",
      " '1091,\\tIteration: 312,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:43] Train,\\tTimestamp: 1614896923317,\\tGlobal steps: '\n",
      " '1092,\\tIteration: 313,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:43] Train,\\tTimestamp: 1614896923386,\\tGlobal steps: '\n",
      " '1093,\\tIteration: 314,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:43] Train,\\tTimestamp: 1614896923587,\\tGlobal steps: '\n",
      " '1094,\\tIteration: 315,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:43] Train,\\tTimestamp: 1614896923789,\\tGlobal steps: '\n",
      " '1095,\\tIteration: 316,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:43] Train,\\tTimestamp: 1614896923884,\\tGlobal steps: '\n",
      " '1096,\\tIteration: 317,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:43] Train,\\tTimestamp: 1614896923992,\\tGlobal steps: '\n",
      " '1097,\\tIteration: 318,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:44] Train,\\tTimestamp: 1614896924195,\\tGlobal steps: '\n",
      " '1098,\\tIteration: 319,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:44] Train,\\tTimestamp: 1614896924398,\\tGlobal steps: '\n",
      " '1099,\\tIteration: 320,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:44] Train,\\tTimestamp: 1614896924694,\\tGlobal steps: '\n",
      " '1100,\\tIteration: 321,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:44] Train,\\tTimestamp: 1614896924700,\\tGlobal steps: '\n",
      " '1101,\\tIteration: 322,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:44] Train,\\tTimestamp: 1614896924903,\\tGlobal steps: '\n",
      " '1102,\\tIteration: 323,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:45] Train,\\tTimestamp: 1614896925107,\\tGlobal steps: '\n",
      " '1103,\\tIteration: 324,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:45] Train,\\tTimestamp: 1614896925262,\\tGlobal steps: '\n",
      " '1104,\\tIteration: 325,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:45] Train,\\tTimestamp: 1614896925310,\\tGlobal steps: '\n",
      " '1105,\\tIteration: 326,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:45] Train,\\tTimestamp: 1614896925512,\\tGlobal steps: '\n",
      " '1106,\\tIteration: 327,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:45] Train,\\tTimestamp: 1614896925715,\\tGlobal steps: '\n",
      " '1107,\\tIteration: 328,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:45] Train,\\tTimestamp: 1614896925836,\\tGlobal steps: '\n",
      " '1108,\\tIteration: 329,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:45] Train,\\tTimestamp: 1614896925918,\\tGlobal steps: '\n",
      " '1109,\\tIteration: 330,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:46] Train,\\tTimestamp: 1614896926122,\\tGlobal steps: '\n",
      " '1110,\\tIteration: 331,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:46] Train,\\tTimestamp: 1614896926324,\\tGlobal steps: '\n",
      " '1111,\\tIteration: 332,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:46] Train,\\tTimestamp: 1614896926403,\\tGlobal steps: '\n",
      " '1112,\\tIteration: 333,\\tLoss: 0.03616,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:46] Train,\\tTimestamp: 1614896926527,\\tGlobal steps: '\n",
      " '1113,\\tIteration: 334,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:46] Train,\\tTimestamp: 1614896926731,\\tGlobal steps: '\n",
      " '1114,\\tIteration: 335,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:46] Train,\\tTimestamp: 1614896926934,\\tGlobal steps: '\n",
      " '1115,\\tIteration: 336,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:47] Train,\\tTimestamp: 1614896927212,\\tGlobal steps: '\n",
      " '1116,\\tIteration: 337,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:47] Train,\\tTimestamp: 1614896927218,\\tGlobal steps: '\n",
      " '1117,\\tIteration: 338,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:47] Train,\\tTimestamp: 1614896927422,\\tGlobal steps: '\n",
      " '1118,\\tIteration: 339,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:47] Train,\\tTimestamp: 1614896927624,\\tGlobal steps: '\n",
      " '1119,\\tIteration: 340,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:47] Train,\\tTimestamp: 1614896927779,\\tGlobal steps: '\n",
      " '1120,\\tIteration: 341,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:47] Train,\\tTimestamp: 1614896927826,\\tGlobal steps: '\n",
      " '1121,\\tIteration: 342,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:48] Train,\\tTimestamp: 1614896928030,\\tGlobal steps: '\n",
      " '1122,\\tIteration: 343,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:48] Train,\\tTimestamp: 1614896928232,\\tGlobal steps: '\n",
      " '1123,\\tIteration: 344,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:48] Train,\\tTimestamp: 1614896928353,\\tGlobal steps: '\n",
      " '1124,\\tIteration: 345,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:48] Train,\\tTimestamp: 1614896928436,\\tGlobal steps: '\n",
      " '1125,\\tIteration: 346,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:48] Train,\\tTimestamp: 1614896928639,\\tGlobal steps: '\n",
      " '1126,\\tIteration: 347,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:48] Train,\\tTimestamp: 1614896928842,\\tGlobal steps: '\n",
      " '1127,\\tIteration: 348,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:48] Train,\\tTimestamp: 1614896928918,\\tGlobal steps: '\n",
      " '1128,\\tIteration: 349,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:49] Train,\\tTimestamp: 1614896929048,\\tGlobal steps: '\n",
      " '1129,\\tIteration: 350,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:49] Train,\\tTimestamp: 1614896929250,\\tGlobal steps: '\n",
      " '1130,\\tIteration: 351,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:49] Train,\\tTimestamp: 1614896929452,\\tGlobal steps: '\n",
      " '1131,\\tIteration: 352,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:49] Train,\\tTimestamp: 1614896929731,\\tGlobal steps: '\n",
      " '1132,\\tIteration: 354,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:49] Train,\\tTimestamp: 1614896929734,\\tGlobal steps: '\n",
      " '1133,\\tIteration: 354,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:49] Train,\\tTimestamp: 1614896929935,\\tGlobal steps: '\n",
      " '1134,\\tIteration: 355,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:50] Train,\\tTimestamp: 1614896930137,\\tGlobal steps: '\n",
      " '1135,\\tIteration: 356,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:50] Train,\\tTimestamp: 1614896930296,\\tGlobal steps: '\n",
      " '1136,\\tIteration: 357,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:50] Train,\\tTimestamp: 1614896930342,\\tGlobal steps: '\n",
      " '1137,\\tIteration: 358,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:50] Train,\\tTimestamp: 1614896930545,\\tGlobal steps: '\n",
      " '1138,\\tIteration: 359,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:50] Train,\\tTimestamp: 1614896930752,\\tGlobal steps: '\n",
      " '1139,\\tIteration: 360,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:50] Train,\\tTimestamp: 1614896930871,\\tGlobal steps: '\n",
      " '1140,\\tIteration: 361,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:50] Train,\\tTimestamp: 1614896930953,\\tGlobal steps: '\n",
      " '1141,\\tIteration: 362,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:51] Train,\\tTimestamp: 1614896931157,\\tGlobal steps: '\n",
      " '1142,\\tIteration: 363,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:51] Train,\\tTimestamp: 1614896931359,\\tGlobal steps: '\n",
      " '1143,\\tIteration: 364,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:51] Train,\\tTimestamp: 1614896931441,\\tGlobal steps: '\n",
      " '1144,\\tIteration: 365,\\tLoss: 0.03616,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:51] Train,\\tTimestamp: 1614896931561,\\tGlobal steps: '\n",
      " '1145,\\tIteration: 366,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:51] Train,\\tTimestamp: 1614896931763,\\tGlobal steps: '\n",
      " '1146,\\tIteration: 367,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:51] Train,\\tTimestamp: 1614896931966,\\tGlobal steps: '\n",
      " '1147,\\tIteration: 368,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:52] Train,\\tTimestamp: 1614896932249,\\tGlobal steps: '\n",
      " '1148,\\tIteration: 370,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:52] Train,\\tTimestamp: 1614896932252,\\tGlobal steps: '\n",
      " '1149,\\tIteration: 370,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:52] Train,\\tTimestamp: 1614896932452,\\tGlobal steps: '\n",
      " '1150,\\tIteration: 371,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:52] Train,\\tTimestamp: 1614896932654,\\tGlobal steps: '\n",
      " '1151,\\tIteration: 372,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:52] Train,\\tTimestamp: 1614896932816,\\tGlobal steps: '\n",
      " '1152,\\tIteration: 373,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:52] Train,\\tTimestamp: 1614896932856,\\tGlobal steps: '\n",
      " '1153,\\tIteration: 374,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:53] Train,\\tTimestamp: 1614896933057,\\tGlobal steps: '\n",
      " '1154,\\tIteration: 375,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:53] Train,\\tTimestamp: 1614896933259,\\tGlobal steps: '\n",
      " '1155,\\tIteration: 376,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:53] Train,\\tTimestamp: 1614896933392,\\tGlobal steps: '\n",
      " '1156,\\tIteration: 377,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:53] Train,\\tTimestamp: 1614896933459,\\tGlobal steps: '\n",
      " '1157,\\tIteration: 378,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:53] Train,\\tTimestamp: 1614896933661,\\tGlobal steps: '\n",
      " '1158,\\tIteration: 379,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:53] Train,\\tTimestamp: 1614896933862,\\tGlobal steps: '\n",
      " '1159,\\tIteration: 380,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:53] Train,\\tTimestamp: 1614896933957,\\tGlobal steps: '\n",
      " '1160,\\tIteration: 381,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:54] Train,\\tTimestamp: 1614896934064,\\tGlobal steps: '\n",
      " '1161,\\tIteration: 382,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:54] Train,\\tTimestamp: 1614896934265,\\tGlobal steps: '\n",
      " '1162,\\tIteration: 383,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:54] Train,\\tTimestamp: 1614896934468,\\tGlobal steps: '\n",
      " '1163,\\tIteration: 384,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:54] Train,\\tTimestamp: 1614896934765,\\tGlobal steps: '\n",
      " '1164,\\tIteration: 386,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:54] Train,\\tTimestamp: 1614896934767,\\tGlobal steps: '\n",
      " '1165,\\tIteration: 386,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:54] Train,\\tTimestamp: 1614896934968,\\tGlobal steps: '\n",
      " '1166,\\tIteration: 387,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:55] Train,\\tTimestamp: 1614896935170,\\tGlobal steps: '\n",
      " '1167,\\tIteration: 388,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:55] Train,\\tTimestamp: 1614896935332,\\tGlobal steps: '\n",
      " '1168,\\tIteration: 389,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:55] Train,\\tTimestamp: 1614896935371,\\tGlobal steps: '\n",
      " '1169,\\tIteration: 390,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:55] Train,\\tTimestamp: 1614896935574,\\tGlobal steps: '\n",
      " '1170,\\tIteration: 391,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:55] Train,\\tTimestamp: 1614896935774,\\tGlobal steps: '\n",
      " '1171,\\tIteration: 392,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:55] Train,\\tTimestamp: 1614896935901,\\tGlobal steps: '\n",
      " '1172,\\tIteration: 393,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:55] Train,\\tTimestamp: 1614896935976,\\tGlobal steps: '\n",
      " '1173,\\tIteration: 394,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:56] Train,\\tTimestamp: 1614896936177,\\tGlobal steps: '\n",
      " '1174,\\tIteration: 395,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:56] Train,\\tTimestamp: 1614896936379,\\tGlobal steps: '\n",
      " '1175,\\tIteration: 396,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:56] Train,\\tTimestamp: 1614896936466,\\tGlobal steps: '\n",
      " '1176,\\tIteration: 397,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:56] Train,\\tTimestamp: 1614896936580,\\tGlobal steps: '\n",
      " '1177,\\tIteration: 398,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:56] Train,\\tTimestamp: 1614896936781,\\tGlobal steps: '\n",
      " '1178,\\tIteration: 399,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:56] Train,\\tTimestamp: 1614896936982,\\tGlobal steps: '\n",
      " '1179,\\tIteration: 400,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:57] Train,\\tTimestamp: 1614896937275,\\tGlobal steps: '\n",
      " '1180,\\tIteration: 402,\\tLoss: 0.03615,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:57] Train,\\tTimestamp: 1614896937278,\\tGlobal steps: '\n",
      " '1181,\\tIteration: 402,\\tLoss: 0.03615,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:57] Train,\\tTimestamp: 1614896937478,\\tGlobal steps: '\n",
      " '1182,\\tIteration: 403,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:57] Train,\\tTimestamp: 1614896937678,\\tGlobal steps: '\n",
      " '1183,\\tIteration: 404,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:57] Train,\\tTimestamp: 1614896937844,\\tGlobal steps: '\n",
      " '1184,\\tIteration: 405,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:57] Train,\\tTimestamp: 1614896937880,\\tGlobal steps: '\n",
      " '1185,\\tIteration: 406,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:58] Train,\\tTimestamp: 1614896938081,\\tGlobal steps: '\n",
      " '1186,\\tIteration: 407,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:28:58] Train,\\tTimestamp: 1614896938282,\\tGlobal steps: '\n",
      " '1187,\\tIteration: 408,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:58] Train,\\tTimestamp: 1614896938418,\\tGlobal steps: '\n",
      " '1188,\\tIteration: 409,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:58] Train,\\tTimestamp: 1614896938484,\\tGlobal steps: '\n",
      " '1189,\\tIteration: 410,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:58] Train,\\tTimestamp: 1614896938685,\\tGlobal steps: '\n",
      " '1190,\\tIteration: 411,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:58] Train,\\tTimestamp: 1614896938886,\\tGlobal steps: '\n",
      " '1191,\\tIteration: 412,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:58] Train,\\tTimestamp: 1614896938985,\\tGlobal steps: '\n",
      " '1192,\\tIteration: 413,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:59] Train,\\tTimestamp: 1614896939088,\\tGlobal steps: '\n",
      " '1193,\\tIteration: 414,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:59] Train,\\tTimestamp: 1614896939290,\\tGlobal steps: '\n",
      " '1194,\\tIteration: 415,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:59] Train,\\tTimestamp: 1614896939491,\\tGlobal steps: '\n",
      " '1195,\\tIteration: 416,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:59] Train,\\tTimestamp: 1614896939793,\\tGlobal steps: '\n",
      " '1196,\\tIteration: 418,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:59] Train,\\tTimestamp: 1614896939795,\\tGlobal steps: '\n",
      " '1197,\\tIteration: 418,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:28:59] Train,\\tTimestamp: 1614896939996,\\tGlobal steps: '\n",
      " '1198,\\tIteration: 419,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:00] Train,\\tTimestamp: 1614896940201,\\tGlobal steps: '\n",
      " '1199,\\tIteration: 420,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:00] Train,\\tTimestamp: 1614896940362,\\tGlobal steps: '\n",
      " '1200,\\tIteration: 421,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:00] Train,\\tTimestamp: 1614896940403,\\tGlobal steps: '\n",
      " '1201,\\tIteration: 422,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:00] Train,\\tTimestamp: 1614896940604,\\tGlobal steps: '\n",
      " '1202,\\tIteration: 423,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:00] Train,\\tTimestamp: 1614896940806,\\tGlobal steps: '\n",
      " '1203,\\tIteration: 424,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:00] Train,\\tTimestamp: 1614896940937,\\tGlobal steps: '\n",
      " '1204,\\tIteration: 425,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:01] Train,\\tTimestamp: 1614896941006,\\tGlobal steps: '\n",
      " '1205,\\tIteration: 426,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:01] Train,\\tTimestamp: 1614896941208,\\tGlobal steps: '\n",
      " '1206,\\tIteration: 427,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:01] Train,\\tTimestamp: 1614896941409,\\tGlobal steps: '\n",
      " '1207,\\tIteration: 428,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:01] Train,\\tTimestamp: 1614896941503,\\tGlobal steps: '\n",
      " '1208,\\tIteration: 429,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:01] Train,\\tTimestamp: 1614896941610,\\tGlobal steps: '\n",
      " '1209,\\tIteration: 430,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:01] Train,\\tTimestamp: 1614896941811,\\tGlobal steps: '\n",
      " '1210,\\tIteration: 431,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:02] Train,\\tTimestamp: 1614896942012,\\tGlobal steps: '\n",
      " '1211,\\tIteration: 432,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:02] Train,\\tTimestamp: 1614896942315,\\tGlobal steps: '\n",
      " '1212,\\tIteration: 434,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:02] Train,\\tTimestamp: 1614896942317,\\tGlobal steps: '\n",
      " '1213,\\tIteration: 434,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:02] Train,\\tTimestamp: 1614896942517,\\tGlobal steps: '\n",
      " '1214,\\tIteration: 435,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:02] Train,\\tTimestamp: 1614896942720,\\tGlobal steps: '\n",
      " '1215,\\tIteration: 436,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:02] Train,\\tTimestamp: 1614896942880,\\tGlobal steps: '\n",
      " '1216,\\tIteration: 437,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:02] Train,\\tTimestamp: 1614896942921,\\tGlobal steps: '\n",
      " '1217,\\tIteration: 438,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:03] Train,\\tTimestamp: 1614896943123,\\tGlobal steps: '\n",
      " '1218,\\tIteration: 439,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:03] Train,\\tTimestamp: 1614896943324,\\tGlobal steps: '\n",
      " '1219,\\tIteration: 440,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:03] Train,\\tTimestamp: 1614896943451,\\tGlobal steps: '\n",
      " '1220,\\tIteration: 441,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:03] Train,\\tTimestamp: 1614896943526,\\tGlobal steps: '\n",
      " '1221,\\tIteration: 442,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:03] Train,\\tTimestamp: 1614896943727,\\tGlobal steps: '\n",
      " '1222,\\tIteration: 443,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:03] Train,\\tTimestamp: 1614896943929,\\tGlobal steps: '\n",
      " '1223,\\tIteration: 444,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:04] Train,\\tTimestamp: 1614896944016,\\tGlobal steps: '\n",
      " '1224,\\tIteration: 445,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:04] Train,\\tTimestamp: 1614896944131,\\tGlobal steps: '\n",
      " '1225,\\tIteration: 446,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:04] Train,\\tTimestamp: 1614896944333,\\tGlobal steps: '\n",
      " '1226,\\tIteration: 447,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:04] Train,\\tTimestamp: 1614896944551,\\tGlobal steps: '\n",
      " '1227,\\tIteration: 448,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:04] Train,\\tTimestamp: 1614896944824,\\tGlobal steps: '\n",
      " '1228,\\tIteration: 450,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:04] Train,\\tTimestamp: 1614896944826,\\tGlobal steps: '\n",
      " '1229,\\tIteration: 450,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:05] Train,\\tTimestamp: 1614896945026,\\tGlobal steps: '\n",
      " '1230,\\tIteration: 451,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:05] Train,\\tTimestamp: 1614896945228,\\tGlobal steps: '\n",
      " '1231,\\tIteration: 452,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:05] Train,\\tTimestamp: 1614896945393,\\tGlobal steps: '\n",
      " '1232,\\tIteration: 453,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:05] Train,\\tTimestamp: 1614896945430,\\tGlobal steps: '\n",
      " '1233,\\tIteration: 454,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:05] Train,\\tTimestamp: 1614896945635,\\tGlobal steps: '\n",
      " '1234,\\tIteration: 455,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:05] Train,\\tTimestamp: 1614896945837,\\tGlobal steps: '\n",
      " '1235,\\tIteration: 456,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:05] Train,\\tTimestamp: 1614896945966,\\tGlobal steps: '\n",
      " '1236,\\tIteration: 457,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:06] Train,\\tTimestamp: 1614896946038,\\tGlobal steps: '\n",
      " '1237,\\tIteration: 458,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:06] Train,\\tTimestamp: 1614896946239,\\tGlobal steps: '\n",
      " '1238,\\tIteration: 459,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:06] Train,\\tTimestamp: 1614896946440,\\tGlobal steps: '\n",
      " '1239,\\tIteration: 460,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:06] Train,\\tTimestamp: 1614896946531,\\tGlobal steps: '\n",
      " '1240,\\tIteration: 461,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:06] Train,\\tTimestamp: 1614896946642,\\tGlobal steps: '\n",
      " '1241,\\tIteration: 462,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:06] Train,\\tTimestamp: 1614896946844,\\tGlobal steps: '\n",
      " '1242,\\tIteration: 463,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:07] Train,\\tTimestamp: 1614896947045,\\tGlobal steps: '\n",
      " '1243,\\tIteration: 464,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:07] Train,\\tTimestamp: 1614896947341,\\tGlobal steps: '\n",
      " '1244,\\tIteration: 466,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:07] Train,\\tTimestamp: 1614896947344,\\tGlobal steps: '\n",
      " '1245,\\tIteration: 466,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:07] Train,\\tTimestamp: 1614896947545,\\tGlobal steps: '\n",
      " '1246,\\tIteration: 467,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:07] Train,\\tTimestamp: 1614896947745,\\tGlobal steps: '\n",
      " '1247,\\tIteration: 468,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:07] Train,\\tTimestamp: 1614896947909,\\tGlobal steps: '\n",
      " '1248,\\tIteration: 469,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:07] Train,\\tTimestamp: 1614896947946,\\tGlobal steps: '\n",
      " '1249,\\tIteration: 470,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:08] Train,\\tTimestamp: 1614896948147,\\tGlobal steps: '\n",
      " '1250,\\tIteration: 471,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:08] Train,\\tTimestamp: 1614896948348,\\tGlobal steps: '\n",
      " '1251,\\tIteration: 472,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:08] Train,\\tTimestamp: 1614896948485,\\tGlobal steps: '\n",
      " '1252,\\tIteration: 473,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:08] Train,\\tTimestamp: 1614896948551,\\tGlobal steps: '\n",
      " '1253,\\tIteration: 474,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:08] Train,\\tTimestamp: 1614896948752,\\tGlobal steps: '\n",
      " '1254,\\tIteration: 475,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:08] Train,\\tTimestamp: 1614896948952,\\tGlobal steps: '\n",
      " '1255,\\tIteration: 476,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:09] Train,\\tTimestamp: 1614896949058,\\tGlobal steps: '\n",
      " '1256,\\tIteration: 477,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:09] Train,\\tTimestamp: 1614896949154,\\tGlobal steps: '\n",
      " '1257,\\tIteration: 478,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:09] Train,\\tTimestamp: 1614896949356,\\tGlobal steps: '\n",
      " '1258,\\tIteration: 479,\\tLoss: 0.03613,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:09] Train,\\tTimestamp: 1614896949574,\\tGlobal steps: '\n",
      " '1259,\\tIteration: 480,\\tLoss: 0.03613,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:09] Train,\\tTimestamp: 1614896949865,\\tGlobal steps: '\n",
      " '1260,\\tIteration: 482,\\tLoss: 0.03613,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:09] Train,\\tTimestamp: 1614896949869,\\tGlobal steps: '\n",
      " '1261,\\tIteration: 482,\\tLoss: 0.03613,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:10] Train,\\tTimestamp: 1614896950071,\\tGlobal steps: '\n",
      " '1262,\\tIteration: 483,\\tLoss: 0.03614,\\tAccuracy: 0.00105\\n'\n",
      " '[2021-03-04 17:29:10] Train,\\tTimestamp: 1614896950272,\\tGlobal steps: '\n",
      " '1263,\\tIteration: 484,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:10] Train,\\tTimestamp: 1614896950432,\\tGlobal steps: '\n",
      " '1264,\\tIteration: 485,\\tLoss: 0.03614,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:10] Train,\\tTimestamp: 1614896950473,\\tGlobal steps: '\n",
      " '1265,\\tIteration: 486,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:10] Train,\\tTimestamp: 1614896950676,\\tGlobal steps: '\n",
      " '1266,\\tIteration: 487,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:10] Train,\\tTimestamp: 1614896950879,\\tGlobal steps: '\n",
      " '1267,\\tIteration: 488,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:11] Train,\\tTimestamp: 1614896951010,\\tGlobal steps: '\n",
      " '1268,\\tIteration: 489,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:11] Train,\\tTimestamp: 1614896951081,\\tGlobal steps: '\n",
      " '1269,\\tIteration: 490,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:11] Train,\\tTimestamp: 1614896951282,\\tGlobal steps: '\n",
      " '1270,\\tIteration: 491,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:11] Train,\\tTimestamp: 1614896951486,\\tGlobal steps: '\n",
      " '1271,\\tIteration: 492,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:11] Train,\\tTimestamp: 1614896951577,\\tGlobal steps: '\n",
      " '1272,\\tIteration: 493,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:11] Train,\\tTimestamp: 1614896951689,\\tGlobal steps: '\n",
      " '1273,\\tIteration: 494,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:11] Train,\\tTimestamp: 1614896951895,\\tGlobal steps: '\n",
      " '1274,\\tIteration: 495,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:12] Train,\\tTimestamp: 1614896952098,\\tGlobal steps: '\n",
      " '1275,\\tIteration: 496,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:12] Train,\\tTimestamp: 1614896952388,\\tGlobal steps: '\n",
      " '1276,\\tIteration: 498,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:12] Train,\\tTimestamp: 1614896952390,\\tGlobal steps: '\n",
      " '1277,\\tIteration: 498,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:12] Train,\\tTimestamp: 1614896952592,\\tGlobal steps: '\n",
      " '1278,\\tIteration: 499,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:12] Train,\\tTimestamp: 1614896952794,\\tGlobal steps: '\n",
      " '1279,\\tIteration: 500,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:12] Train,\\tTimestamp: 1614896952954,\\tGlobal steps: '\n",
      " '1280,\\tIteration: 501,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:12] Train,\\tTimestamp: 1614896952996,\\tGlobal steps: '\n",
      " '1281,\\tIteration: 502,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:13] Train,\\tTimestamp: 1614896953198,\\tGlobal steps: '\n",
      " '1282,\\tIteration: 503,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:13] Train,\\tTimestamp: 1614896953398,\\tGlobal steps: '\n",
      " '1283,\\tIteration: 504,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:13] Train,\\tTimestamp: 1614896953528,\\tGlobal steps: '\n",
      " '1284,\\tIteration: 505,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:13] Train,\\tTimestamp: 1614896953599,\\tGlobal steps: '\n",
      " '1285,\\tIteration: 506,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:13] Train,\\tTimestamp: 1614896953800,\\tGlobal steps: '\n",
      " '1286,\\tIteration: 507,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:14] Train,\\tTimestamp: 1614896954003,\\tGlobal steps: '\n",
      " '1287,\\tIteration: 508,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:14] Train,\\tTimestamp: 1614896954095,\\tGlobal steps: '\n",
      " '1288,\\tIteration: 509,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:14] Train,\\tTimestamp: 1614896954205,\\tGlobal steps: '\n",
      " '1289,\\tIteration: 510,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:14] Train,\\tTimestamp: 1614896954407,\\tGlobal steps: '\n",
      " '1290,\\tIteration: 511,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:14] Train,\\tTimestamp: 1614896954622,\\tGlobal steps: '\n",
      " '1291,\\tIteration: 512,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:14] Train,\\tTimestamp: 1614896954904,\\tGlobal steps: '\n",
      " '1292,\\tIteration: 514,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:14] Train,\\tTimestamp: 1614896954906,\\tGlobal steps: '\n",
      " '1293,\\tIteration: 514,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:15] Train,\\tTimestamp: 1614896955107,\\tGlobal steps: '\n",
      " '1294,\\tIteration: 515,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:15] Train,\\tTimestamp: 1614896955308,\\tGlobal steps: '\n",
      " '1295,\\tIteration: 516,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:15] Train,\\tTimestamp: 1614896955470,\\tGlobal steps: '\n",
      " '1296,\\tIteration: 517,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:15] Train,\\tTimestamp: 1614896955510,\\tGlobal steps: '\n",
      " '1297,\\tIteration: 518,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:15] Train,\\tTimestamp: 1614896955711,\\tGlobal steps: '\n",
      " '1298,\\tIteration: 519,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:15] Train,\\tTimestamp: 1614896955912,\\tGlobal steps: '\n",
      " '1299,\\tIteration: 520,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:16] Train,\\tTimestamp: 1614896956043,\\tGlobal steps: '\n",
      " '1300,\\tIteration: 521,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:16] Train,\\tTimestamp: 1614896956114,\\tGlobal steps: '\n",
      " '1301,\\tIteration: 522,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:16] Train,\\tTimestamp: 1614896956320,\\tGlobal steps: '\n",
      " '1302,\\tIteration: 523,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:16] Train,\\tTimestamp: 1614896956522,\\tGlobal steps: '\n",
      " '1303,\\tIteration: 524,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:16] Train,\\tTimestamp: 1614896956610,\\tGlobal steps: '\n",
      " '1304,\\tIteration: 525,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:16] Train,\\tTimestamp: 1614896956724,\\tGlobal steps: '\n",
      " '1305,\\tIteration: 526,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:16] Train,\\tTimestamp: 1614896956927,\\tGlobal steps: '\n",
      " '1306,\\tIteration: 527,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:17] Train,\\tTimestamp: 1614896957128,\\tGlobal steps: '\n",
      " '1307,\\tIteration: 528,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:17] Train,\\tTimestamp: 1614896957418,\\tGlobal steps: '\n",
      " '1308,\\tIteration: 530,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:17] Train,\\tTimestamp: 1614896957421,\\tGlobal steps: '\n",
      " '1309,\\tIteration: 530,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:17] Train,\\tTimestamp: 1614896957621,\\tGlobal steps: '\n",
      " '1310,\\tIteration: 531,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:17] Train,\\tTimestamp: 1614896957822,\\tGlobal steps: '\n",
      " '1311,\\tIteration: 532,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:17] Train,\\tTimestamp: 1614896957987,\\tGlobal steps: '\n",
      " '1312,\\tIteration: 533,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:18] Train,\\tTimestamp: 1614896958023,\\tGlobal steps: '\n",
      " '1313,\\tIteration: 534,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:18] Train,\\tTimestamp: 1614896958225,\\tGlobal steps: '\n",
      " '1314,\\tIteration: 535,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:18] Train,\\tTimestamp: 1614896958425,\\tGlobal steps: '\n",
      " '1315,\\tIteration: 536,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:18] Train,\\tTimestamp: 1614896958559,\\tGlobal steps: '\n",
      " '1316,\\tIteration: 537,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:18] Train,\\tTimestamp: 1614896958629,\\tGlobal steps: '\n",
      " '1317,\\tIteration: 538,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:18] Train,\\tTimestamp: 1614896958833,\\tGlobal steps: '\n",
      " '1318,\\tIteration: 539,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:19] Train,\\tTimestamp: 1614896959035,\\tGlobal steps: '\n",
      " '1319,\\tIteration: 540,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:19] Train,\\tTimestamp: 1614896959127,\\tGlobal steps: '\n",
      " '1320,\\tIteration: 541,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:19] Train,\\tTimestamp: 1614896959240,\\tGlobal steps: '\n",
      " '1321,\\tIteration: 542,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:19] Train,\\tTimestamp: 1614896959443,\\tGlobal steps: '\n",
      " '1322,\\tIteration: 543,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:19] Train,\\tTimestamp: 1614896959645,\\tGlobal steps: '\n",
      " '1323,\\tIteration: 544,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:19] Train,\\tTimestamp: 1614896959934,\\tGlobal steps: '\n",
      " '1324,\\tIteration: 546,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:19] Train,\\tTimestamp: 1614896959936,\\tGlobal steps: '\n",
      " '1325,\\tIteration: 546,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:20] Train,\\tTimestamp: 1614896960136,\\tGlobal steps: '\n",
      " '1326,\\tIteration: 547,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:20] Train,\\tTimestamp: 1614896960338,\\tGlobal steps: '\n",
      " '1327,\\tIteration: 548,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:20] Train,\\tTimestamp: 1614896960502,\\tGlobal steps: '\n",
      " '1328,\\tIteration: 549,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:20] Train,\\tTimestamp: 1614896960539,\\tGlobal steps: '\n",
      " '1329,\\tIteration: 550,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:20] Train,\\tTimestamp: 1614896960740,\\tGlobal steps: '\n",
      " '1330,\\tIteration: 551,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:20] Train,\\tTimestamp: 1614896960940,\\tGlobal steps: '\n",
      " '1331,\\tIteration: 552,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:21] Train,\\tTimestamp: 1614896961075,\\tGlobal steps: '\n",
      " '1332,\\tIteration: 553,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:21] Train,\\tTimestamp: 1614896961144,\\tGlobal steps: '\n",
      " '1333,\\tIteration: 554,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:21] Train,\\tTimestamp: 1614896961347,\\tGlobal steps: '\n",
      " '1334,\\tIteration: 555,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:21] Train,\\tTimestamp: 1614896961550,\\tGlobal steps: '\n",
      " '1335,\\tIteration: 556,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:21] Train,\\tTimestamp: 1614896961641,\\tGlobal steps: '\n",
      " '1336,\\tIteration: 557,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:21] Train,\\tTimestamp: 1614896961755,\\tGlobal steps: '\n",
      " '1337,\\tIteration: 558,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:21] Train,\\tTimestamp: 1614896961959,\\tGlobal steps: '\n",
      " '1338,\\tIteration: 559,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:22] Train,\\tTimestamp: 1614896962161,\\tGlobal steps: '\n",
      " '1339,\\tIteration: 560,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:22] Train,\\tTimestamp: 1614896962452,\\tGlobal steps: '\n",
      " '1340,\\tIteration: 562,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:22] Train,\\tTimestamp: 1614896962454,\\tGlobal steps: '\n",
      " '1341,\\tIteration: 562,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:22] Train,\\tTimestamp: 1614896962656,\\tGlobal steps: '\n",
      " '1342,\\tIteration: 563,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:22] Train,\\tTimestamp: 1614896962858,\\tGlobal steps: '\n",
      " '1343,\\tIteration: 564,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:23] Train,\\tTimestamp: 1614896963020,\\tGlobal steps: '\n",
      " '1344,\\tIteration: 565,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:23] Train,\\tTimestamp: 1614896963060,\\tGlobal steps: '\n",
      " '1345,\\tIteration: 566,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:23] Train,\\tTimestamp: 1614896963263,\\tGlobal steps: '\n",
      " '1346,\\tIteration: 567,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:23] Train,\\tTimestamp: 1614896963465,\\tGlobal steps: '\n",
      " '1347,\\tIteration: 568,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:23] Train,\\tTimestamp: 1614896963594,\\tGlobal steps: '\n",
      " '1348,\\tIteration: 569,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:23] Train,\\tTimestamp: 1614896963668,\\tGlobal steps: '\n",
      " '1349,\\tIteration: 570,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:23] Train,\\tTimestamp: 1614896963870,\\tGlobal steps: '\n",
      " '1350,\\tIteration: 571,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:24] Train,\\tTimestamp: 1614896964071,\\tGlobal steps: '\n",
      " '1351,\\tIteration: 572,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:24] Train,\\tTimestamp: 1614896964160,\\tGlobal steps: '\n",
      " '1352,\\tIteration: 573,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:24] Train,\\tTimestamp: 1614896964273,\\tGlobal steps: '\n",
      " '1353,\\tIteration: 574,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:24] Train,\\tTimestamp: 1614896964475,\\tGlobal steps: '\n",
      " '1354,\\tIteration: 575,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:24] Train,\\tTimestamp: 1614896964676,\\tGlobal steps: '\n",
      " '1355,\\tIteration: 576,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:24] Train,\\tTimestamp: 1614896964971,\\tGlobal steps: '\n",
      " '1356,\\tIteration: 578,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:24] Train,\\tTimestamp: 1614896964973,\\tGlobal steps: '\n",
      " '1357,\\tIteration: 578,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:25] Train,\\tTimestamp: 1614896965173,\\tGlobal steps: '\n",
      " '1358,\\tIteration: 579,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:25] Train,\\tTimestamp: 1614896965374,\\tGlobal steps: '\n",
      " '1359,\\tIteration: 580,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:25] Train,\\tTimestamp: 1614896965539,\\tGlobal steps: '\n",
      " '1360,\\tIteration: 581,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:25] Train,\\tTimestamp: 1614896965575,\\tGlobal steps: '\n",
      " '1361,\\tIteration: 582,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:25] Train,\\tTimestamp: 1614896965777,\\tGlobal steps: '\n",
      " '1362,\\tIteration: 583,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:25] Train,\\tTimestamp: 1614896965978,\\tGlobal steps: '\n",
      " '1363,\\tIteration: 584,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:26] Train,\\tTimestamp: 1614896966119,\\tGlobal steps: '\n",
      " '1364,\\tIteration: 585,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:26] Train,\\tTimestamp: 1614896966180,\\tGlobal steps: '\n",
      " '1365,\\tIteration: 586,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:26] Train,\\tTimestamp: 1614896966382,\\tGlobal steps: '\n",
      " '1366,\\tIteration: 587,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:26] Train,\\tTimestamp: 1614896966583,\\tGlobal steps: '\n",
      " '1367,\\tIteration: 588,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:26] Train,\\tTimestamp: 1614896966686,\\tGlobal steps: '\n",
      " '1368,\\tIteration: 589,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:26] Train,\\tTimestamp: 1614896966786,\\tGlobal steps: '\n",
      " '1369,\\tIteration: 590,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:26] Train,\\tTimestamp: 1614896966990,\\tGlobal steps: '\n",
      " '1370,\\tIteration: 591,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:27] Train,\\tTimestamp: 1614896967193,\\tGlobal steps: '\n",
      " '1371,\\tIteration: 592,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:27] Train,\\tTimestamp: 1614896967498,\\tGlobal steps: '\n",
      " '1372,\\tIteration: 594,\\tLoss: 0.03612,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:27] Train,\\tTimestamp: 1614896967500,\\tGlobal steps: '\n",
      " '1373,\\tIteration: 594,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:27] Train,\\tTimestamp: 1614896967700,\\tGlobal steps: '\n",
      " '1374,\\tIteration: 595,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:27] Train,\\tTimestamp: 1614896967902,\\tGlobal steps: '\n",
      " '1375,\\tIteration: 596,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:28] Train,\\tTimestamp: 1614896968066,\\tGlobal steps: '\n",
      " '1376,\\tIteration: 597,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:28] Train,\\tTimestamp: 1614896968104,\\tGlobal steps: '\n",
      " '1377,\\tIteration: 598,\\tLoss: 0.03613,\\tAccuracy: 0.00107\\n'\n",
      " '[2021-03-04 17:29:28] Train,\\tTimestamp: 1614896968309,\\tGlobal steps: '\n",
      " '1378,\\tIteration: 599,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:28] Train,\\tTimestamp: 1614896968510,\\tGlobal steps: '\n",
      " '1379,\\tIteration: 600,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:28] Train,\\tTimestamp: 1614896968643,\\tGlobal steps: '\n",
      " '1380,\\tIteration: 601,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:28] Train,\\tTimestamp: 1614896968712,\\tGlobal steps: '\n",
      " '1381,\\tIteration: 602,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:28] Train,\\tTimestamp: 1614896968914,\\tGlobal steps: '\n",
      " '1382,\\tIteration: 603,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:29] Train,\\tTimestamp: 1614896969114,\\tGlobal steps: '\n",
      " '1383,\\tIteration: 604,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:29] Train,\\tTimestamp: 1614896969209,\\tGlobal steps: '\n",
      " '1384,\\tIteration: 605,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:29] Train,\\tTimestamp: 1614896969327,\\tGlobal steps: '\n",
      " '1385,\\tIteration: 606,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:29] Train,\\tTimestamp: 1614896969528,\\tGlobal steps: '\n",
      " '1386,\\tIteration: 607,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:29] Train,\\tTimestamp: 1614896969729,\\tGlobal steps: '\n",
      " '1387,\\tIteration: 608,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:30] Train,\\tTimestamp: 1614896970019,\\tGlobal steps: '\n",
      " '1388,\\tIteration: 610,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:30] Train,\\tTimestamp: 1614896970023,\\tGlobal steps: '\n",
      " '1389,\\tIteration: 610,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:30] Train,\\tTimestamp: 1614896970223,\\tGlobal steps: '\n",
      " '1390,\\tIteration: 611,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:30] Train,\\tTimestamp: 1614896970424,\\tGlobal steps: '\n",
      " '1391,\\tIteration: 612,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:30] Train,\\tTimestamp: 1614896970588,\\tGlobal steps: '\n",
      " '1392,\\tIteration: 613,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:30] Train,\\tTimestamp: 1614896970625,\\tGlobal steps: '\n",
      " '1393,\\tIteration: 614,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:30] Train,\\tTimestamp: 1614896970827,\\tGlobal steps: '\n",
      " '1394,\\tIteration: 615,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:31] Train,\\tTimestamp: 1614896971028,\\tGlobal steps: '\n",
      " '1395,\\tIteration: 616,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:31] Train,\\tTimestamp: 1614896971165,\\tGlobal steps: '\n",
      " '1396,\\tIteration: 617,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:31] Train,\\tTimestamp: 1614896971229,\\tGlobal steps: '\n",
      " '1397,\\tIteration: 618,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:31] Train,\\tTimestamp: 1614896971432,\\tGlobal steps: '\n",
      " '1398,\\tIteration: 619,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:31] Train,\\tTimestamp: 1614896971633,\\tGlobal steps: '\n",
      " '1399,\\tIteration: 620,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:31] Train,\\tTimestamp: 1614896971735,\\tGlobal steps: '\n",
      " '1400,\\tIteration: 621,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:31] Train,\\tTimestamp: 1614896971834,\\tGlobal steps: '\n",
      " '1401,\\tIteration: 622,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:32] Train,\\tTimestamp: 1614896972036,\\tGlobal steps: '\n",
      " '1402,\\tIteration: 623,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:32] Train,\\tTimestamp: 1614896972238,\\tGlobal steps: '\n",
      " '1403,\\tIteration: 624,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:32] Train,\\tTimestamp: 1614896972548,\\tGlobal steps: '\n",
      " '1404,\\tIteration: 626,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:32] Train,\\tTimestamp: 1614896972552,\\tGlobal steps: '\n",
      " '1405,\\tIteration: 626,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:32] Train,\\tTimestamp: 1614896972753,\\tGlobal steps: '\n",
      " '1406,\\tIteration: 627,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:32] Train,\\tTimestamp: 1614896972956,\\tGlobal steps: '\n",
      " '1407,\\tIteration: 628,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:33] Train,\\tTimestamp: 1614896973118,\\tGlobal steps: '\n",
      " '1408,\\tIteration: 629,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:33] Train,\\tTimestamp: 1614896973159,\\tGlobal steps: '\n",
      " '1409,\\tIteration: 630,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:33] Train,\\tTimestamp: 1614896973360,\\tGlobal steps: '\n",
      " '1410,\\tIteration: 631,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:33] Train,\\tTimestamp: 1614896973561,\\tGlobal steps: '\n",
      " '1411,\\tIteration: 632,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:33] Train,\\tTimestamp: 1614896973695,\\tGlobal steps: '\n",
      " '1412,\\tIteration: 633,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:33] Train,\\tTimestamp: 1614896973762,\\tGlobal steps: '\n",
      " '1413,\\tIteration: 634,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:33] Train,\\tTimestamp: 1614896973963,\\tGlobal steps: '\n",
      " '1414,\\tIteration: 635,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:34] Train,\\tTimestamp: 1614896974165,\\tGlobal steps: '\n",
      " '1415,\\tIteration: 636,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:34] Train,\\tTimestamp: 1614896974267,\\tGlobal steps: '\n",
      " '1416,\\tIteration: 637,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:34] Train,\\tTimestamp: 1614896974367,\\tGlobal steps: '\n",
      " '1417,\\tIteration: 638,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:34] Train,\\tTimestamp: 1614896974585,\\tGlobal steps: '\n",
      " '1418,\\tIteration: 639,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:34] Train,\\tTimestamp: 1614896974786,\\tGlobal steps: '\n",
      " '1419,\\tIteration: 640,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:35] Train,\\tTimestamp: 1614896975079,\\tGlobal steps: '\n",
      " '1420,\\tIteration: 642,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:35] Train,\\tTimestamp: 1614896975082,\\tGlobal steps: '\n",
      " '1421,\\tIteration: 642,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:35] Train,\\tTimestamp: 1614896975282,\\tGlobal steps: '\n",
      " '1422,\\tIteration: 643,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:35] Train,\\tTimestamp: 1614896975483,\\tGlobal steps: '\n",
      " '1423,\\tIteration: 644,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:35] Train,\\tTimestamp: 1614896975649,\\tGlobal steps: '\n",
      " '1424,\\tIteration: 645,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:35] Train,\\tTimestamp: 1614896975683,\\tGlobal steps: '\n",
      " '1425,\\tIteration: 646,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:35] Train,\\tTimestamp: 1614896975885,\\tGlobal steps: '\n",
      " '1426,\\tIteration: 647,\\tLoss: 0.03613,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:36] Train,\\tTimestamp: 1614896976086,\\tGlobal steps: '\n",
      " '1427,\\tIteration: 648,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:36] Train,\\tTimestamp: 1614896976224,\\tGlobal steps: '\n",
      " '1428,\\tIteration: 649,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:36] Train,\\tTimestamp: 1614896976287,\\tGlobal steps: '\n",
      " '1429,\\tIteration: 650,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:36] Train,\\tTimestamp: 1614896976489,\\tGlobal steps: '\n",
      " '1430,\\tIteration: 651,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:36] Train,\\tTimestamp: 1614896976690,\\tGlobal steps: '\n",
      " '1431,\\tIteration: 652,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:36] Train,\\tTimestamp: 1614896976792,\\tGlobal steps: '\n",
      " '1432,\\tIteration: 653,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:36] Train,\\tTimestamp: 1614896976891,\\tGlobal steps: '\n",
      " '1433,\\tIteration: 654,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:37] Train,\\tTimestamp: 1614896977093,\\tGlobal steps: '\n",
      " '1434,\\tIteration: 655,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:37] Train,\\tTimestamp: 1614896977294,\\tGlobal steps: '\n",
      " '1435,\\tIteration: 656,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:37] Train,\\tTimestamp: 1614896977603,\\tGlobal steps: '\n",
      " '1436,\\tIteration: 658,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:37] Train,\\tTimestamp: 1614896977605,\\tGlobal steps: '\n",
      " '1437,\\tIteration: 658,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:37] Train,\\tTimestamp: 1614896977806,\\tGlobal steps: '\n",
      " '1438,\\tIteration: 659,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:38] Train,\\tTimestamp: 1614896978006,\\tGlobal steps: '\n",
      " '1439,\\tIteration: 660,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:38] Train,\\tTimestamp: 1614896978172,\\tGlobal steps: '\n",
      " '1440,\\tIteration: 661,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:38] Train,\\tTimestamp: 1614896978207,\\tGlobal steps: '\n",
      " '1441,\\tIteration: 662,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:38] Train,\\tTimestamp: 1614896978408,\\tGlobal steps: '\n",
      " '1442,\\tIteration: 663,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:38] Train,\\tTimestamp: 1614896978611,\\tGlobal steps: '\n",
      " '1443,\\tIteration: 664,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:38] Train,\\tTimestamp: 1614896978748,\\tGlobal steps: '\n",
      " '1444,\\tIteration: 665,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:38] Train,\\tTimestamp: 1614896978812,\\tGlobal steps: '\n",
      " '1445,\\tIteration: 666,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:39] Train,\\tTimestamp: 1614896979013,\\tGlobal steps: '\n",
      " '1446,\\tIteration: 667,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:39] Train,\\tTimestamp: 1614896979215,\\tGlobal steps: '\n",
      " '1447,\\tIteration: 668,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:39] Train,\\tTimestamp: 1614896979318,\\tGlobal steps: '\n",
      " '1448,\\tIteration: 669,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:39] Train,\\tTimestamp: 1614896979417,\\tGlobal steps: '\n",
      " '1449,\\tIteration: 670,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:39] Train,\\tTimestamp: 1614896979632,\\tGlobal steps: '\n",
      " '1450,\\tIteration: 671,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:39] Train,\\tTimestamp: 1614896979834,\\tGlobal steps: '\n",
      " '1451,\\tIteration: 672,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:40] Train,\\tTimestamp: 1614896980128,\\tGlobal steps: '\n",
      " '1452,\\tIteration: 674,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:40] Train,\\tTimestamp: 1614896980130,\\tGlobal steps: '\n",
      " '1453,\\tIteration: 674,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:40] Train,\\tTimestamp: 1614896980330,\\tGlobal steps: '\n",
      " '1454,\\tIteration: 675,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:40] Train,\\tTimestamp: 1614896980531,\\tGlobal steps: '\n",
      " '1455,\\tIteration: 676,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:40] Train,\\tTimestamp: 1614896980698,\\tGlobal steps: '\n",
      " '1456,\\tIteration: 677,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:40] Train,\\tTimestamp: 1614896980733,\\tGlobal steps: '\n",
      " '1457,\\tIteration: 678,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:40] Train,\\tTimestamp: 1614896980934,\\tGlobal steps: '\n",
      " '1458,\\tIteration: 679,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:41] Train,\\tTimestamp: 1614896981135,\\tGlobal steps: '\n",
      " '1459,\\tIteration: 680,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:41] Train,\\tTimestamp: 1614896981275,\\tGlobal steps: '\n",
      " '1460,\\tIteration: 681,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:41] Train,\\tTimestamp: 1614896981337,\\tGlobal steps: '\n",
      " '1461,\\tIteration: 682,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:41] Train,\\tTimestamp: 1614896981539,\\tGlobal steps: '\n",
      " '1462,\\tIteration: 683,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:41] Train,\\tTimestamp: 1614896981740,\\tGlobal steps: '\n",
      " '1463,\\tIteration: 684,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:41] Train,\\tTimestamp: 1614896981843,\\tGlobal steps: '\n",
      " '1464,\\tIteration: 685,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:41] Train,\\tTimestamp: 1614896981941,\\tGlobal steps: '\n",
      " '1465,\\tIteration: 686,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:42] Train,\\tTimestamp: 1614896982143,\\tGlobal steps: '\n",
      " '1466,\\tIteration: 687,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:42] Train,\\tTimestamp: 1614896982345,\\tGlobal steps: '\n",
      " '1467,\\tIteration: 688,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:42] Train,\\tTimestamp: 1614896982652,\\tGlobal steps: '\n",
      " '1468,\\tIteration: 690,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:42] Train,\\tTimestamp: 1614896982654,\\tGlobal steps: '\n",
      " '1469,\\tIteration: 690,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:42] Train,\\tTimestamp: 1614896982854,\\tGlobal steps: '\n",
      " '1470,\\tIteration: 691,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:43] Train,\\tTimestamp: 1614896983055,\\tGlobal steps: '\n",
      " '1471,\\tIteration: 692,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:43] Train,\\tTimestamp: 1614896983224,\\tGlobal steps: '\n",
      " '1472,\\tIteration: 693,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:43] Train,\\tTimestamp: 1614896983257,\\tGlobal steps: '\n",
      " '1473,\\tIteration: 694,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:43] Train,\\tTimestamp: 1614896983459,\\tGlobal steps: '\n",
      " '1474,\\tIteration: 695,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:43] Train,\\tTimestamp: 1614896983659,\\tGlobal steps: '\n",
      " '1475,\\tIteration: 696,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:43] Train,\\tTimestamp: 1614896983799,\\tGlobal steps: '\n",
      " '1476,\\tIteration: 697,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:43] Train,\\tTimestamp: 1614896983861,\\tGlobal steps: '\n",
      " '1477,\\tIteration: 698,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:44] Train,\\tTimestamp: 1614896984063,\\tGlobal steps: '\n",
      " '1478,\\tIteration: 699,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:44] Train,\\tTimestamp: 1614896984264,\\tGlobal steps: '\n",
      " '1479,\\tIteration: 700,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:44] Train,\\tTimestamp: 1614896984369,\\tGlobal steps: '\n",
      " '1480,\\tIteration: 701,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:44] Train,\\tTimestamp: 1614896984465,\\tGlobal steps: '\n",
      " '1481,\\tIteration: 702,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:44] Train,\\tTimestamp: 1614896984667,\\tGlobal steps: '\n",
      " '1482,\\tIteration: 703,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:44] Train,\\tTimestamp: 1614896984868,\\tGlobal steps: '\n",
      " '1483,\\tIteration: 704,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:45] Train,\\tTimestamp: 1614896985180,\\tGlobal steps: '\n",
      " '1484,\\tIteration: 706,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:45] Train,\\tTimestamp: 1614896985183,\\tGlobal steps: '\n",
      " '1485,\\tIteration: 706,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:45] Train,\\tTimestamp: 1614896985385,\\tGlobal steps: '\n",
      " '1486,\\tIteration: 707,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:45] Train,\\tTimestamp: 1614896985586,\\tGlobal steps: '\n",
      " '1487,\\tIteration: 708,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:45] Train,\\tTimestamp: 1614896985750,\\tGlobal steps: '\n",
      " '1488,\\tIteration: 709,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:45] Train,\\tTimestamp: 1614896985787,\\tGlobal steps: '\n",
      " '1489,\\tIteration: 710,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:45] Train,\\tTimestamp: 1614896985990,\\tGlobal steps: '\n",
      " '1490,\\tIteration: 711,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:46] Train,\\tTimestamp: 1614896986190,\\tGlobal steps: '\n",
      " '1491,\\tIteration: 712,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:46] Train,\\tTimestamp: 1614896986329,\\tGlobal steps: '\n",
      " '1492,\\tIteration: 713,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:46] Train,\\tTimestamp: 1614896986392,\\tGlobal steps: '\n",
      " '1493,\\tIteration: 714,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:46] Train,\\tTimestamp: 1614896986594,\\tGlobal steps: '\n",
      " '1494,\\tIteration: 715,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:46] Train,\\tTimestamp: 1614896986795,\\tGlobal steps: '\n",
      " '1495,\\tIteration: 716,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:46] Train,\\tTimestamp: 1614896986894,\\tGlobal steps: '\n",
      " '1496,\\tIteration: 717,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:46] Train,\\tTimestamp: 1614896986996,\\tGlobal steps: '\n",
      " '1497,\\tIteration: 718,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:47] Train,\\tTimestamp: 1614896987198,\\tGlobal steps: '\n",
      " '1498,\\tIteration: 719,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:47] Train,\\tTimestamp: 1614896987399,\\tGlobal steps: '\n",
      " '1499,\\tIteration: 720,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:47] Train,\\tTimestamp: 1614896987711,\\tGlobal steps: '\n",
      " '1500,\\tIteration: 722,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:47] Train,\\tTimestamp: 1614896987713,\\tGlobal steps: '\n",
      " '1501,\\tIteration: 722,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:47] Train,\\tTimestamp: 1614896987916,\\tGlobal steps: '\n",
      " '1502,\\tIteration: 723,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:48] Train,\\tTimestamp: 1614896988117,\\tGlobal steps: '\n",
      " '1503,\\tIteration: 724,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:48] Train,\\tTimestamp: 1614896988278,\\tGlobal steps: '\n",
      " '1504,\\tIteration: 725,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:48] Train,\\tTimestamp: 1614896988318,\\tGlobal steps: '\n",
      " '1505,\\tIteration: 726,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:48] Train,\\tTimestamp: 1614896988519,\\tGlobal steps: '\n",
      " '1506,\\tIteration: 727,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:48] Train,\\tTimestamp: 1614896988720,\\tGlobal steps: '\n",
      " '1507,\\tIteration: 728,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:48] Train,\\tTimestamp: 1614896988857,\\tGlobal steps: '\n",
      " '1508,\\tIteration: 729,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:48] Train,\\tTimestamp: 1614896988922,\\tGlobal steps: '\n",
      " '1509,\\tIteration: 730,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:49] Train,\\tTimestamp: 1614896989123,\\tGlobal steps: '\n",
      " '1510,\\tIteration: 731,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:49] Train,\\tTimestamp: 1614896989325,\\tGlobal steps: '\n",
      " '1511,\\tIteration: 732,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:49] Train,\\tTimestamp: 1614896989428,\\tGlobal steps: '\n",
      " '1512,\\tIteration: 733,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:49] Train,\\tTimestamp: 1614896989526,\\tGlobal steps: '\n",
      " '1513,\\tIteration: 734,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:49] Train,\\tTimestamp: 1614896989728,\\tGlobal steps: '\n",
      " '1514,\\tIteration: 735,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:49] Train,\\tTimestamp: 1614896989929,\\tGlobal steps: '\n",
      " '1515,\\tIteration: 736,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:50] Train,\\tTimestamp: 1614896990240,\\tGlobal steps: '\n",
      " '1516,\\tIteration: 738,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:50] Train,\\tTimestamp: 1614896990243,\\tGlobal steps: '\n",
      " '1517,\\tIteration: 738,\\tLoss: 0.03612,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:50] Train,\\tTimestamp: 1614896990443,\\tGlobal steps: '\n",
      " '1518,\\tIteration: 739,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:50] Train,\\tTimestamp: 1614896990644,\\tGlobal steps: '\n",
      " '1519,\\tIteration: 740,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:50] Train,\\tTimestamp: 1614896990810,\\tGlobal steps: '\n",
      " '1520,\\tIteration: 741,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:50] Train,\\tTimestamp: 1614896990845,\\tGlobal steps: '\n",
      " '1521,\\tIteration: 742,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:51] Train,\\tTimestamp: 1614896991045,\\tGlobal steps: '\n",
      " '1522,\\tIteration: 743,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:51] Train,\\tTimestamp: 1614896991247,\\tGlobal steps: '\n",
      " '1523,\\tIteration: 744,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:51] Train,\\tTimestamp: 1614896991388,\\tGlobal steps: '\n",
      " '1524,\\tIteration: 745,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:51] Train,\\tTimestamp: 1614896991448,\\tGlobal steps: '\n",
      " '1525,\\tIteration: 746,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:51] Train,\\tTimestamp: 1614896991650,\\tGlobal steps: '\n",
      " '1526,\\tIteration: 747,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:51] Train,\\tTimestamp: 1614896991851,\\tGlobal steps: '\n",
      " '1527,\\tIteration: 748,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:51] Train,\\tTimestamp: 1614896991955,\\tGlobal steps: '\n",
      " '1528,\\tIteration: 749,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:52] Train,\\tTimestamp: 1614896992053,\\tGlobal steps: '\n",
      " '1529,\\tIteration: 750,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:52] Train,\\tTimestamp: 1614896992255,\\tGlobal steps: '\n",
      " '1530,\\tIteration: 751,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:52] Train,\\tTimestamp: 1614896992457,\\tGlobal steps: '\n",
      " '1531,\\tIteration: 752,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:52] Train,\\tTimestamp: 1614896992768,\\tGlobal steps: '\n",
      " '1532,\\tIteration: 754,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:52] Train,\\tTimestamp: 1614896992771,\\tGlobal steps: '\n",
      " '1533,\\tIteration: 754,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:52] Train,\\tTimestamp: 1614896992972,\\tGlobal steps: '\n",
      " '1534,\\tIteration: 755,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:53] Train,\\tTimestamp: 1614896993174,\\tGlobal steps: '\n",
      " '1535,\\tIteration: 756,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:53] Train,\\tTimestamp: 1614896993339,\\tGlobal steps: '\n",
      " '1536,\\tIteration: 757,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:53] Train,\\tTimestamp: 1614896993375,\\tGlobal steps: '\n",
      " '1537,\\tIteration: 758,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:53] Train,\\tTimestamp: 1614896993576,\\tGlobal steps: '\n",
      " '1538,\\tIteration: 759,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:53] Train,\\tTimestamp: 1614896993777,\\tGlobal steps: '\n",
      " '1539,\\tIteration: 760,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:53] Train,\\tTimestamp: 1614896993916,\\tGlobal steps: '\n",
      " '1540,\\tIteration: 761,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:53] Train,\\tTimestamp: 1614896993979,\\tGlobal steps: '\n",
      " '1541,\\tIteration: 762,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:54] Train,\\tTimestamp: 1614896994182,\\tGlobal steps: '\n",
      " '1542,\\tIteration: 763,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:54] Train,\\tTimestamp: 1614896994382,\\tGlobal steps: '\n",
      " '1543,\\tIteration: 764,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:54] Train,\\tTimestamp: 1614896994488,\\tGlobal steps: '\n",
      " '1544,\\tIteration: 765,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:54] Train,\\tTimestamp: 1614896994580,\\tGlobal steps: '\n",
      " '1545,\\tIteration: 766,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:54] Train,\\tTimestamp: 1614896994788,\\tGlobal steps: '\n",
      " '1546,\\tIteration: 767,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:54] Train,\\tTimestamp: 1614896994986,\\tGlobal steps: '\n",
      " '1547,\\tIteration: 768,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:55] Train,\\tTimestamp: 1614896995299,\\tGlobal steps: '\n",
      " '1548,\\tIteration: 770,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:55] Train,\\tTimestamp: 1614896995302,\\tGlobal steps: '\n",
      " '1549,\\tIteration: 770,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:55] Train,\\tTimestamp: 1614896995498,\\tGlobal steps: '\n",
      " '1550,\\tIteration: 771,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:55] Train,\\tTimestamp: 1614896995873,\\tGlobal steps: '\n",
      " '1551,\\tIteration: 772,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:56] Train,\\tTimestamp: 1614896996450,\\tGlobal steps: '\n",
      " '1552,\\tIteration: 773,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:57] Train,\\tTimestamp: 1614896997018,\\tGlobal steps: '\n",
      " '1553,\\tIteration: 774,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:57] Train,\\tTimestamp: 1614896997595,\\tGlobal steps: '\n",
      " '1554,\\tIteration: 775,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:58] Train,\\tTimestamp: 1614896998166,\\tGlobal steps: '\n",
      " '1555,\\tIteration: 776,\\tLoss: 0.0361,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:58] Train,\\tTimestamp: 1614896998741,\\tGlobal steps: '\n",
      " '1556,\\tIteration: 777,\\tLoss: 0.03611,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:59] Train,\\tTimestamp: 1614896999308,\\tGlobal steps: '\n",
      " '1557,\\tIteration: 778,\\tLoss: 0.0361,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:29:59] Train,\\tTimestamp: 1614896999885,\\tGlobal steps: '\n",
      " '1558,\\tIteration: 779,\\tLoss: 0.0361,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:30:00] Train,\\tTimestamp: 1614897000452,\\tGlobal steps: '\n",
      " '1559,\\tIteration: 780,\\tLoss: 0.0361,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:30:01] Train,\\tTimestamp: 1614897001028,\\tGlobal steps: '\n",
      " '1560,\\tIteration: 781,\\tLoss: 0.0361,\\tAccuracy: 0.00106\\n'\n",
      " '[2021-03-04 17:30:01] Train,\\tTimestamp: 1614897001681,\\tGlobal steps: '\n",
      " '1561,\\tIteration: 0,\\tLoss: 0.03622,\\tAccuracy: 0.00063\\n'\n",
      " '[2021-03-04 17:30:01] Train,\\tTimestamp: 1614897001683,\\tGlobal steps: '\n",
      " '1562,\\tIteration: 0,\\tLoss: 0.03603,\\tAccuracy: 0.00109\\n')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "retrieve_stdout_url =  dl_rest_url + '/execs/'+exec_id+'/log?logType=' + 'outlog'\n",
    "print (\"retrieve_stdout_url: \" + str(retrieve_stdout_url))\n",
    "res = requests.get(retrieve_stdout_url, auth=myauth, headers={'Accept': 'application/json'}, verify=False)\n",
    "\n",
    "pp.pprint(res.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dlipy3-cpu/lib/python3.7/site-packages/urllib3/connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host '169.61.217.36'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load extra config from : /opt/yxzbj/workspace/dli_shared_fs//conf\n",
      "Setting up spark environment on node baremetal03.ibm-ibm-platform-lab-team-test-account.cloud\n",
      "DLI_EXTRA_CONF=export DLI_LOGGER_LEVEL=info;export DLI_IS_ELASTIC=true;export DLI_CS_DATASTORE_META=type=fs,data_path=pytorch-mnist;export DLI_EXECID=Admin-1337275417692279-1421306274;export FABRIC_HOME=/opt/yxzbj/workspace/dli_shared_fs//fabric/1.2.5;export DLI_WORK_DIR=/opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274/_submitted_code/resnet-wmla;export MODEL_PATH=/opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274/_submitted_code/resnet-wmla;export FRAME_WORK=edtPyTorch\n",
      "21/02/05 01:32:10 INFO Succeed to activate conda environment dlipy3\n",
      "21/02/05 01:32:10 INFO Conda variables: conda_home=/opt/anaconda3, conda_env=dlipy3, python=/opt/anaconda3/envs/dlipy3/bin/python\n",
      "21/02/05 01:32:10 INFO Checking python version\n",
      "3.7.9 (default, Aug 31 2020, 12:42:55) \n",
      "[GCC 7.3.0]\n",
      "21/02/05 01:32:10 ERROR DLI_DATA_FS is not defined\n",
      "Exposed the environment variable DATA_DIR to: \n",
      "Exposed the environment variable RESULT_DIR to: /opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274\n",
      "Exposed the environment variable LOG_DIR to: /opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274/log/driver-20210205013209-0018-c5d3cc24-8e5d-40da-bcf9-d9dba85a2468.baremetal03.ibm-ibm-platform-lab-team-test-account.cloud\n",
      "Exposed the environment variable SAVED_MODEL_DIR to: /opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274/model\n",
      "/opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/logs/driver-20210205013209-0018-c5d3cc24-8e5d-40da-bcf9-d9dba85a2468.baremetal03.ibm-ibm-platform-lab-team-test-account.cloud/\n",
      "21/02/05 01:32:10 INFO Trace envrionment\n",
      "CUDA_VISIBLE_DEVICES=\n",
      "PATH=/opt/anaconda3/envs/dlipy3/bin:/opt/anaconda3/condabin:/usr/local/bin:/bin:/usr/bin:/opt/yxzbj/workspace/conductor2.5/3.9/linux-x86_64/bin:/opt/anaconda3/bin\n",
      "PYTHONPATH=:/opt/yxzbj/workspace/dli_shared_fs/tools/dli_utils/:/opt/yxzbj/workspace/dli_shared_fs//fabric/1.2.5/libs/fabric.zip:/opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274/_submitted_code/resnet-wmla\n",
      "LD_LIBRARY_PATH=/opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/work/root/754119::/opt/yxzbj/workspace/conductor2.5/3.9/linux-x86_64/lib:/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow:/opt/yxzbj/workspace/dli_shared_fs//fabric/1.2.5/native:/opt/yxzbj/workspace/dli_shared_fs//fabric/1.2.5/native/ce1.6:/opt/yxzbj/workspace/dli_shared_fs//fabric/1.2.5/native/ce1.7\n",
      "DL_NFS_PATH=/opt/yxzbj/workspace/dli_shared_fs\n",
      "DLI_DATA_FS=\n",
      "DLI_RESULT_FS=/opt/yxzbj/workspace/dli_result_fs\n",
      "DLI_IS_ELASTIC=true\n",
      "FABRIC_HOME=/opt/yxzbj/workspace/dli_shared_fs//fabric/1.2.5\n",
      "DLI_ELASTIC_CONF_DIR=/opt/yxzbj/workspace/conductor2.5/dli/1.2.5/fabric/conf\n",
      "RDMA_ENABLED=false\n",
      "RDMA_BUFFER_SIZE=1073741824\n",
      "RDMA_DEVICE_NAME=\n",
      "RDMA_DEVICE_PORT=\n",
      "DLI_EXECID=Admin-1337275417692279-1421306274\n",
      "DLI_RUN_AS_USERNAME=root\n",
      "DLI_RUN_AS_USERID=0\n",
      "DLI_RUN_AS_GROUPID=0\n",
      "DLI_LOGGER_LEVEL=info\n",
      "SPARK_LOG_STDOUTSTDERR_DIR=/opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/logs/driver-20210205013209-0018-c5d3cc24-8e5d-40da-bcf9-d9dba85a2468.baremetal03.ibm-ibm-platform-lab-team-test-account.cloud/\n",
      "DLI_DEFAULT_CONDA_HOME=/opt/anaconda3\n",
      "DLI_DEFAULT_CONDA_ENV_NAME=dlipy3\n",
      "DLI_CONDA_HOME=\n",
      "DLI_CONDA_ENV_NAME=\n",
      "HPO_PLUGIN_CONDA_HOME=\n",
      "HPO_PLUGIN_CONDA_ENV=\n",
      "21/02/05 01:32:10 INFO finish spark environment setup.\n",
      "<> RESULT_DIR /opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274\n",
      "<> LOG_DIR /opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274/log/driver-20210205013209-0018-c5d3cc24-8e5d-40da-bcf9-d9dba85a2468.baremetal03.ibm-ibm-platform-lab-team-test-account.cloud\n",
      "<> SAVED_MODEL_DIR /opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274/model\n",
      "Using pytorch backend.\n",
      "libunformatworker.so loaded successfully\n",
      "Using spark scheduler.\n",
      "DATA_DIR: /tmp\n",
      "==> Building model..resnet50\n",
      "Warining! Ignore invalid input for 'scaleup_function'. It should be a function with two input arguments: epoch and max_engine_num,and returns scaled max_engine_num\n",
      "/opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/python/lib/pyspark.zip/pyspark/context.py:248: RuntimeWarning: Failed to add file [.//fabric.zip] speficied in 'spark.submit.pyFiles' to Python path:\n",
      "  /opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/work/driver-20210205013209-0018-c5d3cc24-8e5d-40da-bcf9-d9dba85a2468\n",
      "  /tmp/c56f00f1-f4a2-4058-84f9-a81d45e1d885_root_root/spark-93ca463f-4a51-4d2d-bc57-d2d99d1daf00/userFiles-0d8e6f07-3235-43b3-96de-7b82c8391d73\n",
      "  /opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/work/driver-20210205013209-0018-c5d3cc24-8e5d-40da-bcf9-d9dba85a2468/fabric.zip\n",
      "  /opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/python/lib/pyspark.zip\n",
      "  /opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/python/lib/py4j-0.10.7-src.zip\n",
      "  /opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/jars/ego/spark-core_2.11-2.3.3.jar\n",
      "  /opt/yxzbj/workspace/sig/spark-2.3.3-hadoop-2.7/work/driver-20210205013209-0018-c5d3cc24-8e5d-40da-bcf9-d9dba85a2468\n",
      "  /opt/yxzbj/workspace/dli_shared_fs/tools/dli_utils\n",
      "  /opt/yxzbj/workspace/dli_shared_fs/fabric/1.2.5/libs/fabric.zip\n",
      "  /opt/yxzbj/workspace/dli_result_fs/root/batchworkdir/Admin-1337275417692279-1421306274/_submitted_code/resnet-wmla\n",
      "  /opt/anaconda3/envs/dlipy3/lib/python37.zip\n",
      "  /opt/anaconda3/envs/dlipy3/lib/python3.7\n",
      "  /opt/anaconda3/envs/dlipy3/lib/python3.7/lib-dynload\n",
      "  /opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages\n",
      "  /opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/IPython/extensions\n",
      "  /opt/yxzbj/workspace/dli_shared_fs//fabric/1.2.5/native\n",
      "  /opt/yxzbj/workspace/dli_shared_fs//fabric/1.2.5/native\n",
      "  RuntimeWarning)\n",
      "==> epochs:2, batchsize:64, engines_number:16\n",
      "start message server 169.61.217.36:44925\n",
      "2021-02-05 01:32:15,782 - root - INFO - client connect to server url: 169.61.217.36:44925\n",
      "2021-02-05 01:32:15,782 - root - INFO - client connect to server url: 169.61.217.36:44925\n",
      "--------START TRAINING--------\n",
      "Timestamp 1612510353178, Iteration 1\n",
      "batches :1 0.1147955134510994\n",
      "Iteration 2: tag train_accuracy, simple_value 0.0\n",
      "Iteration 2: tag train_loss, simple_value 0.1148\n",
      "Timestamp 1612510353487, Iteration 2\n",
      "batches :2 0.11446606367826462\n",
      "Iteration 3: tag train_accuracy, simple_value 0.0\n",
      "Iteration 3: tag train_loss, simple_value 0.11447\n",
      "Timestamp 1612510353797, Iteration 3\n",
      "batches :3 0.11369071900844574\n",
      "Iteration 4: tag train_accuracy, simple_value 0.0\n",
      "Iteration 4: tag train_loss, simple_value 0.11369\n",
      "Timestamp 1612510354105, Iteration 4\n",
      "batches :4 0.1136979479342699\n",
      "Iteration 5: tag train_accuracy, simple_value 0.0\n",
      "Iteration 5: tag train_loss, simple_value 0.1137\n",
      "Timestamp 1612510354412, Iteration 5\n",
      "batches :5 0.11346484422683716\n",
      "Iteration 6: tag train_accuracy, simple_value 0.0\n",
      "Iteration 6: tag train_loss, simple_value 0.11346\n",
      "Timestamp 1612510354722, Iteration 6\n",
      "batches :6 0.11369926730791728\n",
      "Iteration 7: tag train_accuracy, simple_value 0.0\n",
      "Iteration 7: tag train_loss, simple_value 0.1137\n",
      "Timestamp 1612510355032, Iteration 7\n",
      "batches :7 0.11367171044860568\n",
      "Iteration 8: tag train_accuracy, simple_value 0.0\n",
      "Iteration 8: tag train_loss, simple_value 0.11367\n",
      "Timestamp 1612510355341, Iteration 8\n",
      "batches :8 0.11386236641556025\n",
      "Iteration 9: tag train_accuracy, simple_value 0.0\n",
      "Iteration 9: tag train_loss, simple_value 0.11386\n",
      "Timestamp 1612510355650, Iteration 9\n",
      "batches :9 0.11394268025954564\n",
      "Iteration 10: tag train_accuracy, simple_value 0.0\n",
      "Iteration 10: tag train_loss, simple_value 0.11394\n",
      "Timestamp 1612510355958, Iteration 10\n",
      "batches :10 0.11390484869480133\n",
      "Iteration 11: tag train_accuracy, simple_value 0.0\n",
      "Iteration 11: tag train_loss, simple_value 0.1139\n",
      "Timestamp 1612510356267, Iteration 11\n",
      "batches :11 0.11398998376998035\n",
      "Iteration 12: tag train_accuracy, simple_value 0.0\n",
      "Iteration 12: tag train_loss, simple_value 0.11399\n",
      "Timestamp 1612510356575, Iteration 12\n",
      "batches :12 0.11395129126807053\n",
      "Iteration 13: tag train_accuracy, simple_value 0.0\n",
      "Iteration 13: tag train_loss, simple_value 0.11395\n",
      "Timestamp 1612510356883, Iteration 13\n",
      "batches :13 0.11399176315619396\n",
      "Iteration 14: tag train_accuracy, simple_value 0.0\n",
      "Iteration 14: tag train_loss, simple_value 0.11399\n",
      "Timestamp 1612510357191, Iteration 14\n",
      "batches :14 0.11404144710728101\n",
      "Iteration 15: tag train_accuracy, simple_value 0.0\n",
      "Iteration 15: tag train_loss, simple_value 0.11404\n",
      "Timestamp 1612510357499, Iteration 15\n",
      "batches :15 0.1140556126832962\n",
      "Iteration 16: tag train_accuracy, simple_value 0.0\n",
      "Iteration 16: tag train_loss, simple_value 0.11406\n",
      "Timestamp 1612510357945, Iteration 16\n",
      "batches :16 0.11402060417458415\n",
      "Iteration 17: tag train_accuracy, simple_value 0.0\n",
      "Iteration 17: tag train_loss, simple_value 0.11402\n",
      "Timestamp 1612510358255, Iteration 17\n",
      "batches :17 0.1171767838737544\n",
      "Iteration 18: tag train_accuracy, simple_value 0.00735\n",
      "Iteration 18: tag train_loss, simple_value 0.11718\n",
      "Timestamp 1612510358564, Iteration 18\n",
      "batches :18 0.12231104117300776\n",
      "Iteration 19: tag train_accuracy, simple_value 0.01389\n",
      "Iteration 19: tag train_loss, simple_value 0.12231\n",
      "Timestamp 1612510358872, Iteration 19\n",
      "batches :19 0.12483370500175577\n",
      "Iteration 20: tag train_accuracy, simple_value 0.0222\n",
      "Iteration 20: tag train_loss, simple_value 0.12483\n",
      "Timestamp 1612510359179, Iteration 20\n",
      "batches :20 0.1282954666763544\n",
      "Iteration 21: tag train_accuracy, simple_value 0.02891\n",
      "Iteration 21: tag train_loss, simple_value 0.1283\n",
      "Timestamp 1612510359487, Iteration 21\n",
      "batches :21 0.13141750047604242\n",
      "Iteration 22: tag train_accuracy, simple_value 0.03497\n",
      "Iteration 22: tag train_loss, simple_value 0.13142\n",
      "Timestamp 1612510359795, Iteration 22\n",
      "batches :22 0.1346382312476635\n",
      "Iteration 23: tag train_accuracy, simple_value 0.03551\n",
      "Iteration 23: tag train_loss, simple_value 0.13464\n",
      "Timestamp 1612510360104, Iteration 23\n",
      "batches :23 0.1374459477222484\n",
      "Iteration 24: tag train_accuracy, simple_value 0.03804\n",
      "Iteration 24: tag train_loss, simple_value 0.13745\n",
      "Timestamp 1612510360412, Iteration 24\n",
      "batches :24 0.14028347500910363\n",
      "Iteration 25: tag train_accuracy, simple_value 0.03841\n",
      "Iteration 25: tag train_loss, simple_value 0.14028\n",
      "Timestamp 1612510360721, Iteration 25\n",
      "batches :25 0.14390729933977128\n",
      "Iteration 26: tag train_accuracy, simple_value 0.04\n",
      "Iteration 26: tag train_loss, simple_value 0.14391\n",
      "Timestamp 1612510361028, Iteration 26\n",
      "batches :26 0.14535280460348496\n",
      "Iteration 27: tag train_accuracy, simple_value 0.04267\n",
      "Iteration 27: tag train_loss, simple_value 0.14535\n",
      "Timestamp 1612510361335, Iteration 27\n",
      "batches :27 0.14624663000857388\n",
      "Iteration 28: tag train_accuracy, simple_value 0.0434\n",
      "Iteration 28: tag train_loss, simple_value 0.14625\n",
      "Timestamp 1612510361643, Iteration 28\n",
      "batches :28 0.14811697522444384\n",
      "Iteration 29: tag train_accuracy, simple_value 0.04297\n",
      "Iteration 29: tag train_loss, simple_value 0.14812\n",
      "Timestamp 1612510361952, Iteration 29\n",
      "batches :29 0.1484805923597566\n",
      "Iteration 30: tag train_accuracy, simple_value 0.04634\n",
      "Iteration 30: tag train_loss, simple_value 0.14848\n",
      "Timestamp 1612510362259, Iteration 30\n",
      "batches :30 0.1501642681658268\n",
      "Iteration 31: tag train_accuracy, simple_value 0.04948\n",
      "Iteration 31: tag train_loss, simple_value 0.15016\n",
      "Timestamp 1612510362567, Iteration 31\n",
      "batches :31 0.15265965005082469\n",
      "Iteration 32: tag train_accuracy, simple_value 0.04889\n",
      "Iteration 32: tag train_loss, simple_value 0.15266\n",
      "Timestamp 1612510362892, Iteration 32\n",
      "batches :32 0.15402412717230618\n",
      "Iteration 33: tag train_accuracy, simple_value 0.05029\n",
      "Iteration 33: tag train_loss, simple_value 0.15402\n",
      "Timestamp 1612510363200, Iteration 33\n",
      "batches :33 0.16999201453996426\n",
      "Iteration 34: tag train_accuracy, simple_value 0.05161\n",
      "Iteration 34: tag train_loss, simple_value 0.16999\n",
      "Timestamp 1612510363508, Iteration 34\n",
      "batches :34 0.18567812201731346\n",
      "Iteration 35: tag train_accuracy, simple_value 0.05285\n",
      "Iteration 35: tag train_loss, simple_value 0.18568\n",
      "Timestamp 1612510363818, Iteration 35\n",
      "batches :35 0.19239273177725927\n",
      "Iteration 36: tag train_accuracy, simple_value 0.05402\n",
      "Iteration 36: tag train_loss, simple_value 0.19239\n",
      "Timestamp 1612510364126, Iteration 36\n",
      "batches :36 0.21269804342753357\n",
      "Iteration 37: tag train_accuracy, simple_value 0.05469\n",
      "Iteration 37: tag train_loss, simple_value 0.2127\n",
      "Timestamp 1612510364435, Iteration 37\n",
      "batches :37 0.2239366328796825\n",
      "Iteration 38: tag train_accuracy, simple_value 0.05659\n",
      "Iteration 38: tag train_loss, simple_value 0.22394\n",
      "Timestamp 1612510364744, Iteration 38\n",
      "batches :38 0.2523513414749974\n",
      "Iteration 39: tag train_accuracy, simple_value 0.05839\n",
      "Iteration 39: tag train_loss, simple_value 0.25235\n",
      "Timestamp 1612510365053, Iteration 39\n",
      "batches :39 0.2574817020541582\n",
      "Iteration 40: tag train_accuracy, simple_value 0.05809\n",
      "Iteration 40: tag train_loss, simple_value 0.25748\n",
      "Timestamp 1612510365362, Iteration 40\n",
      "batches :40 0.2585847781971097\n",
      "Iteration 41: tag train_accuracy, simple_value 0.05937\n",
      "Iteration 41: tag train_loss, simple_value 0.25858\n",
      "Timestamp 1612510365671, Iteration 41\n",
      "batches :41 0.27270431943782947\n",
      "Iteration 42: tag train_accuracy, simple_value 0.06098\n",
      "Iteration 42: tag train_loss, simple_value 0.2727\n",
      "Timestamp 1612510365980, Iteration 42\n",
      "batches :42 0.2799930645241624\n",
      "Iteration 43: tag train_accuracy, simple_value 0.06287\n",
      "Iteration 43: tag train_loss, simple_value 0.27999\n",
      "Timestamp 1612510366288, Iteration 43\n",
      "batches :43 0.2849880909157354\n",
      "Iteration 44: tag train_accuracy, simple_value 0.06432\n",
      "Iteration 44: tag train_loss, simple_value 0.28499\n",
      "Timestamp 1612510366598, Iteration 44\n",
      "batches :44 0.301976749334823\n",
      "Iteration 45: tag train_accuracy, simple_value 0.06428\n",
      "Iteration 45: tag train_loss, simple_value 0.30198\n",
      "Timestamp 1612510366906, Iteration 45\n",
      "batches :45 0.3113750360078282\n",
      "Iteration 46: tag train_accuracy, simple_value 0.06458\n",
      "Iteration 46: tag train_loss, simple_value 0.31138\n",
      "Timestamp 1612510367214, Iteration 46\n",
      "batches :46 0.31673133065519127\n",
      "Iteration 47: tag train_accuracy, simple_value 0.06624\n",
      "Iteration 47: tag train_loss, simple_value 0.31673\n",
      "Timestamp 1612510367525, Iteration 47\n",
      "batches :47 0.3256546000533916\n",
      "Iteration 48: tag train_accuracy, simple_value 0.06749\n",
      "Iteration 48: tag train_loss, simple_value 0.32565\n",
      "Timestamp 1612510367848, Iteration 48\n",
      "batches :48 0.3381644991847376\n",
      "Iteration 49: tag train_accuracy, simple_value 0.06836\n",
      "Iteration 49: tag train_loss, simple_value 0.33816\n",
      "Timestamp 1612510368155, Iteration 49\n",
      "batches :49 0.35846622212200746\n",
      "Iteration 50: tag train_accuracy, simple_value 0.06952\n",
      "Iteration 50: tag train_loss, simple_value 0.35847\n",
      "Timestamp 1612510368463, Iteration 50\n",
      "batches :50 0.3935530336201191\n",
      "Iteration 51: tag train_accuracy, simple_value 0.06906\n",
      "Iteration 51: tag train_loss, simple_value 0.39355\n",
      "Timestamp 1612510368770, Iteration 51\n",
      "batches :51 0.42318275001119166\n",
      "Iteration 52: tag train_accuracy, simple_value 0.06832\n",
      "Iteration 52: tag train_loss, simple_value 0.42318\n",
      "Timestamp 1612510369078, Iteration 52\n",
      "batches :52 0.4512629011789194\n",
      "Iteration 53: tag train_accuracy, simple_value 0.07001\n",
      "Iteration 53: tag train_loss, simple_value 0.45126\n",
      "Timestamp 1612510369387, Iteration 53\n",
      "batches :53 0.4799918415692617\n",
      "Iteration 54: tag train_accuracy, simple_value 0.06958\n",
      "Iteration 54: tag train_loss, simple_value 0.47999\n",
      "Timestamp 1612510369696, Iteration 54\n",
      "batches :54 0.5245670925963808\n",
      "Iteration 55: tag train_accuracy, simple_value 0.06944\n",
      "Iteration 55: tag train_loss, simple_value 0.52457\n",
      "Timestamp 1612510370003, Iteration 55\n",
      "batches :55 0.5498829306526618\n",
      "Iteration 56: tag train_accuracy, simple_value 0.06989\n",
      "Iteration 56: tag train_loss, simple_value 0.54988\n",
      "Timestamp 1612510370310, Iteration 56\n",
      "batches :56 0.5575121168845466\n",
      "Iteration 57: tag train_accuracy, simple_value 0.07087\n",
      "Iteration 57: tag train_loss, simple_value 0.55751\n",
      "Timestamp 1612510370619, Iteration 57\n",
      "batches :57 0.5675965167189899\n",
      "Iteration 58: tag train_accuracy, simple_value 0.071\n",
      "Iteration 58: tag train_loss, simple_value 0.5676\n",
      "Timestamp 1612510370930, Iteration 58\n",
      "batches :58 0.6029678517631416\n",
      "Iteration 59: tag train_accuracy, simple_value 0.07112\n",
      "Iteration 59: tag train_loss, simple_value 0.60297\n",
      "Timestamp 1612510371239, Iteration 59\n",
      "batches :59 0.6128324650354304\n",
      "Iteration 60: tag train_accuracy, simple_value 0.07097\n",
      "Iteration 60: tag train_loss, simple_value 0.61283\n",
      "Timestamp 1612510371550, Iteration 60\n",
      "batches :60 0.6423642830302317\n",
      "Iteration 61: tag train_accuracy, simple_value 0.07109\n",
      "Iteration 61: tag train_loss, simple_value 0.64236\n",
      "Timestamp 1612510371860, Iteration 61\n",
      "batches :61 0.6597890928387642\n",
      "Iteration 62: tag train_accuracy, simple_value 0.07172\n",
      "Iteration 62: tag train_loss, simple_value 0.65979\n",
      "Timestamp 1612510372169, Iteration 62\n",
      "batches :62 0.6853570819141404\n",
      "Iteration 63: tag train_accuracy, simple_value 0.07132\n",
      "Iteration 63: tag train_loss, simple_value 0.68536\n",
      "Timestamp 1612510372477, Iteration 63\n",
      "batches :63 0.7105729046558577\n",
      "Iteration 64: tag train_accuracy, simple_value 0.07192\n",
      "Iteration 64: tag train_loss, simple_value 0.71057\n",
      "Timestamp 1612510372801, Iteration 64\n",
      "batches :64 0.7316415045643225\n",
      "Iteration 65: tag train_accuracy, simple_value 0.07202\n",
      "Iteration 65: tag train_loss, simple_value 0.73164\n",
      "Timestamp 1612510373109, Iteration 65\n",
      "batches :65 0.7313242844664134\n",
      "Iteration 66: tag train_accuracy, simple_value 0.07284\n",
      "Iteration 66: tag train_loss, simple_value 0.73132\n",
      "Timestamp 1612510373417, Iteration 66\n",
      "batches :66 0.7374924752071048\n",
      "Iteration 67: tag train_accuracy, simple_value 0.07268\n",
      "Iteration 67: tag train_loss, simple_value 0.73749\n",
      "Timestamp 1612510373725, Iteration 67\n",
      "batches :67 0.737785817638262\n",
      "Iteration 68: tag train_accuracy, simple_value 0.07276\n",
      "Iteration 68: tag train_loss, simple_value 0.73779\n",
      "Timestamp 1612510374032, Iteration 68\n",
      "batches :68 0.7611020502141294\n",
      "Iteration 69: tag train_accuracy, simple_value 0.07238\n",
      "Iteration 69: tag train_loss, simple_value 0.7611\n",
      "Timestamp 1612510374339, Iteration 69\n",
      "batches :69 0.7583726658553317\n",
      "Iteration 70: tag train_accuracy, simple_value 0.07314\n",
      "Iteration 70: tag train_loss, simple_value 0.75837\n",
      "Timestamp 1612510374647, Iteration 70\n",
      "batches :70 0.7621258681373937\n",
      "Iteration 71: tag train_accuracy, simple_value 0.07299\n",
      "Iteration 71: tag train_loss, simple_value 0.76213\n",
      "Timestamp 1612510374954, Iteration 71\n",
      "batches :71 0.7648561644092412\n",
      "Iteration 72: tag train_accuracy, simple_value 0.07394\n",
      "Iteration 72: tag train_loss, simple_value 0.76486\n",
      "Timestamp 1612510375260, Iteration 72\n",
      "batches :72 0.7662764014676213\n",
      "Iteration 73: tag train_accuracy, simple_value 0.07422\n",
      "Iteration 73: tag train_loss, simple_value 0.76628\n",
      "Timestamp 1612510375566, Iteration 73\n",
      "batches :73 0.7696913257241249\n",
      "Iteration 74: tag train_accuracy, simple_value 0.07406\n",
      "Iteration 74: tag train_loss, simple_value 0.76969\n",
      "Timestamp 1612510375873, Iteration 74\n",
      "batches :74 0.780259345532269\n",
      "Iteration 75: tag train_accuracy, simple_value 0.0739\n",
      "Iteration 75: tag train_loss, simple_value 0.78026\n",
      "Timestamp 1612510376180, Iteration 75\n",
      "batches :75 0.8061875252922376\n",
      "Iteration 76: tag train_accuracy, simple_value 0.07396\n",
      "Iteration 76: tag train_loss, simple_value 0.80619\n",
      "Timestamp 1612510376486, Iteration 76\n",
      "batches :76 0.8046079267208514\n",
      "Iteration 77: tag train_accuracy, simple_value 0.07525\n",
      "Iteration 77: tag train_loss, simple_value 0.80461\n",
      "Timestamp 1612510376793, Iteration 77\n",
      "batches :77 0.7977284008225838\n",
      "Iteration 78: tag train_accuracy, simple_value 0.0765\n",
      "Iteration 78: tag train_loss, simple_value 0.79773\n",
      "Timestamp 1612510377102, Iteration 78\n",
      "batches :78 0.7961487511220651\n",
      "Iteration 79: tag train_accuracy, simple_value 0.07672\n",
      "Iteration 79: tag train_loss, simple_value 0.79615\n",
      "Timestamp 1612510377410, Iteration 79\n",
      "batches :79 0.81900632692666\n",
      "Iteration 80: tag train_accuracy, simple_value 0.07714\n",
      "Iteration 80: tag train_loss, simple_value 0.81901\n",
      "Timestamp 1612510377734, Iteration 80\n",
      "batches :80 0.8287572951056064\n",
      "Iteration 81: tag train_accuracy, simple_value 0.07676\n",
      "Iteration 81: tag train_loss, simple_value 0.82876\n",
      "Timestamp 1612510378040, Iteration 81\n",
      "batches :81 0.8260881474908487\n",
      "Iteration 82: tag train_accuracy, simple_value 0.07755\n",
      "Iteration 82: tag train_loss, simple_value 0.82609\n",
      "Timestamp 1612510378348, Iteration 82\n",
      "batches :82 0.8224373060937334\n",
      "Iteration 83: tag train_accuracy, simple_value 0.07793\n",
      "Iteration 83: tag train_loss, simple_value 0.82244\n",
      "Timestamp 1612510378656, Iteration 83\n",
      "batches :83 0.8190916088151644\n",
      "Iteration 84: tag train_accuracy, simple_value 0.07812\n",
      "Iteration 84: tag train_loss, simple_value 0.81909\n",
      "Timestamp 1612510378964, Iteration 84\n",
      "batches :84 0.816656493803575\n",
      "Iteration 85: tag train_accuracy, simple_value 0.07868\n",
      "Iteration 85: tag train_loss, simple_value 0.81666\n",
      "Timestamp 1612510379273, Iteration 85\n",
      "batches :85 0.8107353972161517\n",
      "Iteration 86: tag train_accuracy, simple_value 0.07868\n",
      "Iteration 86: tag train_loss, simple_value 0.81074\n",
      "Timestamp 1612510379582, Iteration 86\n",
      "batches :86 0.8196829132042652\n",
      "Iteration 87: tag train_accuracy, simple_value 0.0794\n",
      "Iteration 87: tag train_loss, simple_value 0.81968\n",
      "Timestamp 1612510379890, Iteration 87\n",
      "batches :87 0.8138622306715483\n",
      "Iteration 88: tag train_accuracy, simple_value 0.07992\n",
      "Iteration 88: tag train_loss, simple_value 0.81386\n",
      "Timestamp 1612510380198, Iteration 88\n",
      "batches :88 0.8138871911743825\n",
      "Iteration 89: tag train_accuracy, simple_value 0.08026\n",
      "Iteration 89: tag train_loss, simple_value 0.81389\n",
      "Timestamp 1612510380506, Iteration 89\n",
      "batches :89 0.8236935112750932\n",
      "Iteration 90: tag train_accuracy, simple_value 0.08058\n",
      "Iteration 90: tag train_loss, simple_value 0.82369\n",
      "Timestamp 1612510380815, Iteration 90\n",
      "batches :90 0.8173193458881644\n",
      "Iteration 91: tag train_accuracy, simple_value 0.08194\n",
      "Iteration 91: tag train_loss, simple_value 0.81732\n",
      "Timestamp 1612510381125, Iteration 91\n",
      "batches :91 0.8174869101289864\n",
      "Iteration 92: tag train_accuracy, simple_value 0.08225\n",
      "Iteration 92: tag train_loss, simple_value 0.81749\n",
      "Timestamp 1612510381434, Iteration 92\n",
      "batches :92 0.8144751234061044\n",
      "Iteration 93: tag train_accuracy, simple_value 0.0822\n",
      "Iteration 93: tag train_loss, simple_value 0.81448\n",
      "Timestamp 1612510381742, Iteration 93\n",
      "batches :93 0.810816846948157\n",
      "Iteration 94: tag train_accuracy, simple_value 0.08216\n",
      "Iteration 94: tag train_loss, simple_value 0.81082\n",
      "Timestamp 1612510382049, Iteration 94\n",
      "batches :94 0.8094044379255874\n",
      "Iteration 95: tag train_accuracy, simple_value 0.08211\n",
      "Iteration 95: tag train_loss, simple_value 0.8094\n",
      "Timestamp 1612510382356, Iteration 95\n",
      "batches :95 0.8074516083849104\n",
      "Iteration 96: tag train_accuracy, simple_value 0.08207\n",
      "Iteration 96: tag train_loss, simple_value 0.80745\n",
      "Timestamp 1612510382681, Iteration 96\n",
      "batches :96 0.8067645338208725\n",
      "Iteration 97: tag train_accuracy, simple_value 0.08187\n",
      "Iteration 97: tag train_loss, simple_value 0.80676\n",
      "Timestamp 1612510382989, Iteration 97\n",
      "batches :97 0.8036887898426694\n",
      "Iteration 98: tag train_accuracy, simple_value 0.08183\n",
      "Iteration 98: tag train_loss, simple_value 0.80369\n",
      "Timestamp 1612510383297, Iteration 98\n",
      "batches :98 0.7983663678929513\n",
      "Iteration 99: tag train_accuracy, simple_value 0.08131\n",
      "Iteration 99: tag train_loss, simple_value 0.79837\n",
      "Timestamp 1612510383605, Iteration 99\n",
      "batches :99 0.7936872449336629\n",
      "Iteration 100: tag train_accuracy, simple_value 0.0816\n",
      "Iteration 100: tag train_loss, simple_value 0.79369\n",
      "Timestamp 1612510383911, Iteration 100\n",
      "batches :100 0.7890960402041673\n",
      "Iteration 101: tag train_accuracy, simple_value 0.08141\n",
      "Iteration 101: tag train_loss, simple_value 0.7891\n",
      "Timestamp 1612510384219, Iteration 101\n",
      "batches :101 0.7844350754772083\n",
      "Iteration 102: tag train_accuracy, simple_value 0.08137\n",
      "Iteration 102: tag train_loss, simple_value 0.78444\n",
      "Timestamp 1612510384527, Iteration 102\n",
      "batches :102 0.7798640451156625\n",
      "Iteration 103: tag train_accuracy, simple_value 0.0815\n",
      "Iteration 103: tag train_loss, simple_value 0.77986\n",
      "Timestamp 1612510384834, Iteration 103\n",
      "batches :103 0.7776653480905931\n",
      "Iteration 104: tag train_accuracy, simple_value 0.08146\n",
      "Iteration 104: tag train_loss, simple_value 0.77767\n",
      "Timestamp 1612510385140, Iteration 104\n",
      "batches :104 0.7716142150501792\n",
      "Iteration 105: tag train_accuracy, simple_value 0.08203\n",
      "Iteration 105: tag train_loss, simple_value 0.77161\n",
      "Timestamp 1612510385448, Iteration 105\n",
      "batches :105 0.7717782201511519\n",
      "Iteration 106: tag train_accuracy, simple_value 0.08244\n",
      "Iteration 106: tag train_loss, simple_value 0.77178\n",
      "Timestamp 1612510385755, Iteration 106\n",
      "batches :106 0.7754762858011812\n",
      "Iteration 107: tag train_accuracy, simple_value 0.08225\n",
      "Iteration 107: tag train_loss, simple_value 0.77548\n",
      "Timestamp 1612510386065, Iteration 107\n",
      "batches :107 0.7733653326318642\n",
      "Iteration 108: tag train_accuracy, simple_value 0.08265\n",
      "Iteration 108: tag train_loss, simple_value 0.77337\n",
      "Timestamp 1612510386373, Iteration 108\n",
      "batches :108 0.7708008531481028\n",
      "Iteration 109: tag train_accuracy, simple_value 0.08261\n",
      "Iteration 109: tag train_loss, simple_value 0.7708\n",
      "Timestamp 1612510386686, Iteration 109\n",
      "batches :109 0.7677814497723492\n",
      "Iteration 110: tag train_accuracy, simple_value 0.08243\n",
      "Iteration 110: tag train_loss, simple_value 0.76778\n",
      "Timestamp 1612510386995, Iteration 110\n",
      "batches :110 0.7650276901369745\n",
      "Iteration 111: tag train_accuracy, simple_value 0.08224\n",
      "Iteration 111: tag train_loss, simple_value 0.76503\n",
      "Timestamp 1612510387303, Iteration 111\n",
      "batches :111 0.7613210763345968\n",
      "Iteration 112: tag train_accuracy, simple_value 0.08277\n",
      "Iteration 112: tag train_loss, simple_value 0.76132\n",
      "Timestamp 1612510387627, Iteration 112\n",
      "batches :112 0.7563167795139764\n",
      "Iteration 113: tag train_accuracy, simple_value 0.08287\n",
      "Iteration 113: tag train_loss, simple_value 0.75632\n",
      "Timestamp 1612510387935, Iteration 113\n",
      "batches :113 0.7533912210997227\n",
      "Iteration 114: tag train_accuracy, simple_value 0.08338\n",
      "Iteration 114: tag train_loss, simple_value 0.75339\n",
      "Timestamp 1612510388242, Iteration 114\n",
      "batches :114 0.7482851076413665\n",
      "Iteration 115: tag train_accuracy, simple_value 0.08333\n",
      "Iteration 115: tag train_loss, simple_value 0.74829\n",
      "Timestamp 1612510388550, Iteration 115\n",
      "batches :115 0.7436414274832477\n",
      "Iteration 116: tag train_accuracy, simple_value 0.08315\n",
      "Iteration 116: tag train_loss, simple_value 0.74364\n",
      "Timestamp 1612510388858, Iteration 116\n",
      "batches :116 0.7395324804017256\n",
      "Iteration 117: tag train_accuracy, simple_value 0.08338\n",
      "Iteration 117: tag train_loss, simple_value 0.73953\n",
      "Timestamp 1612510389165, Iteration 117\n",
      "batches :117 0.7366776157520775\n",
      "Iteration 118: tag train_accuracy, simple_value 0.08333\n",
      "Iteration 118: tag train_loss, simple_value 0.73668\n",
      "Timestamp 1612510389473, Iteration 118\n",
      "batches :118 0.7327402556725477\n",
      "Iteration 119: tag train_accuracy, simple_value 0.08369\n",
      "Iteration 119: tag train_loss, simple_value 0.73274\n",
      "Timestamp 1612510389783, Iteration 119\n",
      "batches :119 0.7280497496493724\n",
      "Iteration 120: tag train_accuracy, simple_value 0.0839\n",
      "Iteration 120: tag train_loss, simple_value 0.72805\n",
      "Timestamp 1612510390090, Iteration 120\n",
      "batches :120 0.7233947300041715\n",
      "Iteration 121: tag train_accuracy, simple_value 0.08411\n",
      "Iteration 121: tag train_loss, simple_value 0.72339\n",
      "Timestamp 1612510390397, Iteration 121\n",
      "batches :121 0.7191207221101138\n",
      "Iteration 122: tag train_accuracy, simple_value 0.08407\n",
      "Iteration 122: tag train_loss, simple_value 0.71912\n",
      "Timestamp 1612510390704, Iteration 122\n",
      "batches :122 0.7160722282944155\n",
      "Iteration 123: tag train_accuracy, simple_value 0.08363\n",
      "Iteration 123: tag train_loss, simple_value 0.71607\n",
      "Timestamp 1612510391013, Iteration 123\n",
      "batches :123 0.7181832288217739\n",
      "Iteration 124: tag train_accuracy, simple_value 0.08371\n",
      "Iteration 124: tag train_loss, simple_value 0.71818\n",
      "Timestamp 1612510391320, Iteration 124\n",
      "batches :124 0.7145721101832967\n",
      "Iteration 125: tag train_accuracy, simple_value 0.08417\n",
      "Iteration 125: tag train_loss, simple_value 0.71457\n",
      "Timestamp 1612510391627, Iteration 125\n",
      "batches :125 0.7100912762284279\n",
      "Iteration 126: tag train_accuracy, simple_value 0.08413\n",
      "Iteration 126: tag train_loss, simple_value 0.71009\n",
      "Timestamp 1612510391934, Iteration 126\n",
      "batches :126 0.7065122873182335\n",
      "Iteration 127: tag train_accuracy, simple_value 0.0842\n",
      "Iteration 127: tag train_loss, simple_value 0.70651\n",
      "Timestamp 1612510392241, Iteration 127\n",
      "batches :127 0.7083098989889378\n",
      "Iteration 128: tag train_accuracy, simple_value 0.08428\n",
      "Iteration 128: tag train_loss, simple_value 0.70831\n",
      "Timestamp 1612510392567, Iteration 128\n",
      "batches :128 0.7092726852861233\n",
      "Iteration 129: tag train_accuracy, simple_value 0.08423\n",
      "Iteration 129: tag train_loss, simple_value 0.70927\n",
      "Timestamp 1612510392875, Iteration 129\n",
      "batches :129 0.7047853741881459\n",
      "Iteration 130: tag train_accuracy, simple_value 0.08467\n",
      "Iteration 130: tag train_loss, simple_value 0.70479\n",
      "Timestamp 1612510393182, Iteration 130\n",
      "batches :130 0.7006545434204432\n",
      "Iteration 131: tag train_accuracy, simple_value 0.08534\n",
      "Iteration 131: tag train_loss, simple_value 0.70065\n",
      "Timestamp 1612510393491, Iteration 131\n",
      "batches :131 0.696251384629548\n",
      "Iteration 132: tag train_accuracy, simple_value 0.0854\n",
      "Iteration 132: tag train_loss, simple_value 0.69625\n",
      "Timestamp 1612510393800, Iteration 132\n",
      "batches :132 0.6920517173906168\n",
      "Iteration 133: tag train_accuracy, simple_value 0.08535\n",
      "Iteration 133: tag train_loss, simple_value 0.69205\n",
      "Timestamp 1612510394109, Iteration 133\n",
      "batches :133 0.6882661502612265\n",
      "Iteration 134: tag train_accuracy, simple_value 0.08529\n",
      "Iteration 134: tag train_loss, simple_value 0.68827\n",
      "Timestamp 1612510394418, Iteration 134\n",
      "batches :134 0.684109871400826\n",
      "Iteration 135: tag train_accuracy, simple_value 0.08559\n",
      "Iteration 135: tag train_loss, simple_value 0.68411\n",
      "Timestamp 1612510394726, Iteration 135\n",
      "batches :135 0.680383574631479\n",
      "Iteration 136: tag train_accuracy, simple_value 0.08565\n",
      "Iteration 136: tag train_loss, simple_value 0.68038\n",
      "Timestamp 1612510395033, Iteration 136\n",
      "batches :136 0.6762533929089413\n",
      "Iteration 137: tag train_accuracy, simple_value 0.08571\n",
      "Iteration 137: tag train_loss, simple_value 0.67625\n",
      "Timestamp 1612510395341, Iteration 137\n",
      "batches :137 0.6724119452338149\n",
      "Iteration 138: tag train_accuracy, simple_value 0.08565\n",
      "Iteration 138: tag train_loss, simple_value 0.67241\n",
      "Timestamp 1612510395648, Iteration 138\n",
      "batches :138 0.6688572880474554\n",
      "Iteration 139: tag train_accuracy, simple_value 0.08548\n",
      "Iteration 139: tag train_loss, simple_value 0.66886\n",
      "Timestamp 1612510395955, Iteration 139\n",
      "batches :139 0.6653906847182796\n",
      "Iteration 140: tag train_accuracy, simple_value 0.08532\n",
      "Iteration 140: tag train_loss, simple_value 0.66539\n",
      "Timestamp 1612510396261, Iteration 140\n",
      "batches :140 0.6630044358649424\n",
      "Iteration 141: tag train_accuracy, simple_value 0.0856\n",
      "Iteration 141: tag train_loss, simple_value 0.663\n",
      "Timestamp 1612510396567, Iteration 141\n",
      "batches :141 0.6591842947593818\n",
      "Iteration 142: tag train_accuracy, simple_value 0.0861\n",
      "Iteration 142: tag train_loss, simple_value 0.65918\n",
      "Timestamp 1612510396874, Iteration 142\n",
      "batches :142 0.6558930290941621\n",
      "Iteration 143: tag train_accuracy, simple_value 0.08671\n",
      "Iteration 143: tag train_loss, simple_value 0.65589\n",
      "Timestamp 1612510397180, Iteration 143\n",
      "batches :143 0.6526463748170779\n",
      "Iteration 144: tag train_accuracy, simple_value 0.08676\n",
      "Iteration 144: tag train_loss, simple_value 0.65265\n",
      "Timestamp 1612510397503, Iteration 144\n",
      "batches :144 0.6493371109892097\n",
      "Iteration 145: tag train_accuracy, simple_value 0.08702\n",
      "Iteration 145: tag train_loss, simple_value 0.64934\n",
      "Timestamp 1612510397811, Iteration 145\n",
      "batches :145 0.6461932665829001\n",
      "Iteration 146: tag train_accuracy, simple_value 0.08696\n",
      "Iteration 146: tag train_loss, simple_value 0.64619\n",
      "Timestamp 1612510398119, Iteration 146\n",
      "batches :146 0.64341204383806\n",
      "Iteration 147: tag train_accuracy, simple_value 0.08701\n",
      "Iteration 147: tag train_loss, simple_value 0.64341\n",
      "Timestamp 1612510398427, Iteration 147\n",
      "batches :147 0.6404074069087197\n",
      "Iteration 148: tag train_accuracy, simple_value 0.08705\n",
      "Iteration 148: tag train_loss, simple_value 0.64041\n",
      "Timestamp 1612510398735, Iteration 148\n",
      "batches :148 0.636742985027062\n",
      "Iteration 149: tag train_accuracy, simple_value 0.0871\n",
      "Iteration 149: tag train_loss, simple_value 0.63674\n",
      "Timestamp 1612510399042, Iteration 149\n",
      "batches :149 0.6357855175765569\n",
      "Iteration 150: tag train_accuracy, simple_value 0.08704\n",
      "Iteration 150: tag train_loss, simple_value 0.63579\n",
      "Timestamp 1612510399350, Iteration 150\n",
      "batches :150 0.6371416599551837\n",
      "Iteration 151: tag train_accuracy, simple_value 0.08698\n",
      "Iteration 151: tag train_loss, simple_value 0.63714\n",
      "Timestamp 1612510399658, Iteration 151\n",
      "batches :151 0.6336738539748634\n",
      "Iteration 152: tag train_accuracy, simple_value 0.08692\n",
      "Iteration 152: tag train_loss, simple_value 0.63367\n",
      "Timestamp 1612510399966, Iteration 152\n",
      "batches :152 0.6304223484996903\n",
      "Iteration 153: tag train_accuracy, simple_value 0.08666\n",
      "Iteration 153: tag train_loss, simple_value 0.63042\n",
      "Timestamp 1612510400272, Iteration 153\n",
      "batches :153 0.6292049563885515\n",
      "Iteration 154: tag train_accuracy, simple_value 0.0864\n",
      "Iteration 154: tag train_loss, simple_value 0.6292\n",
      "Timestamp 1612510400578, Iteration 154\n",
      "batches :154 0.6257271338496115\n",
      "Iteration 155: tag train_accuracy, simple_value 0.08675\n",
      "Iteration 155: tag train_loss, simple_value 0.62573\n",
      "Timestamp 1612510400886, Iteration 155\n",
      "batches :155 0.6224390181802935\n",
      "Iteration 156: tag train_accuracy, simple_value 0.0872\n",
      "Iteration 156: tag train_loss, simple_value 0.62244\n",
      "Timestamp 1612510401193, Iteration 156\n",
      "batches :156 0.6193405173909969\n",
      "Iteration 157: tag train_accuracy, simple_value 0.08704\n",
      "Iteration 157: tag train_loss, simple_value 0.61934\n",
      "Timestamp 1612510401502, Iteration 157\n",
      "batches :157 0.6164423699971218\n",
      "Iteration 158: tag train_accuracy, simple_value 0.08678\n",
      "Iteration 158: tag train_loss, simple_value 0.61644\n",
      "Timestamp 1612510401810, Iteration 158\n",
      "batches :158 0.6131922049047072\n",
      "Iteration 159: tag train_accuracy, simple_value 0.08703\n",
      "Iteration 159: tag train_loss, simple_value 0.61319\n",
      "Timestamp 1612510402117, Iteration 159\n",
      "batches :159 0.6121666542951416\n",
      "Iteration 160: tag train_accuracy, simple_value 0.08687\n",
      "Iteration 160: tag train_loss, simple_value 0.61217\n",
      "Timestamp 1612510402438, Iteration 160\n",
      "batches :160 0.6089134093374013\n",
      "Iteration 161: tag train_accuracy, simple_value 0.0873\n",
      "Iteration 161: tag train_loss, simple_value 0.60891\n",
      "Timestamp 1612510402746, Iteration 161\n",
      "batches :161 0.6059546526174368\n",
      "Iteration 162: tag train_accuracy, simple_value 0.08744\n",
      "Iteration 162: tag train_loss, simple_value 0.60595\n",
      "Timestamp 1612510403053, Iteration 162\n",
      "batches :162 0.6034897280695998\n",
      "Iteration 163: tag train_accuracy, simple_value 0.08729\n",
      "Iteration 163: tag train_loss, simple_value 0.60349\n",
      "Timestamp 1612510403361, Iteration 163\n",
      "batches :163 0.6003847704983196\n",
      "Iteration 164: tag train_accuracy, simple_value 0.08714\n",
      "Iteration 164: tag train_loss, simple_value 0.60038\n",
      "Timestamp 1612510403669, Iteration 164\n",
      "batches :164 0.5986223952708448\n",
      "Iteration 165: tag train_accuracy, simple_value 0.08708\n",
      "Iteration 165: tag train_loss, simple_value 0.59862\n",
      "Timestamp 1612510403976, Iteration 165\n",
      "batches :165 0.5957405387903705\n",
      "Iteration 166: tag train_accuracy, simple_value 0.08741\n",
      "Iteration 166: tag train_loss, simple_value 0.59574\n",
      "Timestamp 1612510404283, Iteration 166\n",
      "batches :166 0.5928644753813025\n",
      "Iteration 167: tag train_accuracy, simple_value 0.08707\n",
      "Iteration 167: tag train_loss, simple_value 0.59286\n",
      "Timestamp 1612510404590, Iteration 167\n",
      "batches :167 0.5904638607077256\n",
      "Iteration 168: tag train_accuracy, simple_value 0.08683\n",
      "Iteration 168: tag train_loss, simple_value 0.59046\n",
      "Timestamp 1612510404897, Iteration 168\n",
      "batches :168 0.5877534287227761\n",
      "Iteration 169: tag train_accuracy, simple_value 0.08696\n",
      "Iteration 169: tag train_loss, simple_value 0.58775\n",
      "Timestamp 1612510405205, Iteration 169\n",
      "batches :169 0.5848212305610702\n",
      "Iteration 170: tag train_accuracy, simple_value 0.08719\n",
      "Iteration 170: tag train_loss, simple_value 0.58482\n",
      "Timestamp 1612510405512, Iteration 170\n",
      "batches :170 0.5821338279282345\n",
      "Iteration 171: tag train_accuracy, simple_value 0.08713\n",
      "Iteration 171: tag train_loss, simple_value 0.58213\n",
      "Timestamp 1612510405818, Iteration 171\n",
      "batches :171 0.5795083270784009\n",
      "Iteration 172: tag train_accuracy, simple_value 0.08726\n",
      "Iteration 172: tag train_loss, simple_value 0.57951\n",
      "Timestamp 1612510406124, Iteration 172\n",
      "batches :172 0.5767708248350509\n",
      "Iteration 173: tag train_accuracy, simple_value 0.08739\n",
      "Iteration 173: tag train_loss, simple_value 0.57677\n",
      "Timestamp 1612510406431, Iteration 173\n",
      "batches :173 0.5741029590196003\n",
      "Iteration 174: tag train_accuracy, simple_value 0.08716\n",
      "Iteration 174: tag train_loss, simple_value 0.5741\n",
      "Timestamp 1612510406738, Iteration 174\n",
      "batches :174 0.5713788849012605\n",
      "Iteration 175: tag train_accuracy, simple_value 0.08728\n",
      "Iteration 175: tag train_loss, simple_value 0.57138\n",
      "Timestamp 1612510407045, Iteration 175\n",
      "batches :175 0.5688431263821465\n",
      "Iteration 176: tag train_accuracy, simple_value 0.08732\n",
      "Iteration 176: tag train_loss, simple_value 0.56884\n",
      "Timestamp 1612510407366, Iteration 176\n",
      "batches :176 0.566440767608583\n",
      "Iteration 177: tag train_accuracy, simple_value 0.08709\n",
      "Iteration 177: tag train_loss, simple_value 0.56644\n",
      "Timestamp 1612510407673, Iteration 177\n",
      "batches :177 0.5648702532052994\n",
      "Iteration 178: tag train_accuracy, simple_value 0.08722\n",
      "Iteration 178: tag train_loss, simple_value 0.56487\n",
      "Timestamp 1612510407979, Iteration 178\n",
      "batches :178 0.5621628882258796\n",
      "Iteration 179: tag train_accuracy, simple_value 0.08734\n",
      "Iteration 179: tag train_loss, simple_value 0.56216\n",
      "Timestamp 1612510408286, Iteration 179\n",
      "batches :179 0.5594369247971966\n",
      "Iteration 180: tag train_accuracy, simple_value 0.08738\n",
      "Iteration 180: tag train_loss, simple_value 0.55944\n",
      "Timestamp 1612510408594, Iteration 180\n",
      "batches :180 0.5568341264708174\n",
      "Iteration 181: tag train_accuracy, simple_value 0.0875\n",
      "Iteration 181: tag train_loss, simple_value 0.55683\n",
      "Timestamp 1612510408900, Iteration 181\n",
      "batches :181 0.5543131705229454\n",
      "Iteration 182: tag train_accuracy, simple_value 0.08719\n",
      "Iteration 182: tag train_loss, simple_value 0.55431\n",
      "Timestamp 1612510409207, Iteration 182\n",
      "batches :182 0.5519660390749738\n",
      "Iteration 183: tag train_accuracy, simple_value 0.08723\n",
      "Iteration 183: tag train_loss, simple_value 0.55197\n",
      "Timestamp 1612510409514, Iteration 183\n",
      "batches :183 0.5494290945308457\n",
      "Iteration 184: tag train_accuracy, simple_value 0.08718\n",
      "Iteration 184: tag train_loss, simple_value 0.54943\n",
      "Timestamp 1612510409821, Iteration 184\n",
      "batches :184 0.5469387949484846\n",
      "Iteration 185: tag train_accuracy, simple_value 0.08704\n",
      "Iteration 185: tag train_loss, simple_value 0.54694\n",
      "Timestamp 1612510410128, Iteration 185\n",
      "batches :185 0.5444116717254793\n",
      "Iteration 186: tag train_accuracy, simple_value 0.08758\n",
      "Iteration 186: tag train_loss, simple_value 0.54441\n",
      "Timestamp 1612510410435, Iteration 186\n",
      "batches :186 0.542014677718442\n",
      "Iteration 187: tag train_accuracy, simple_value 0.0877\n",
      "Iteration 187: tag train_loss, simple_value 0.54201\n",
      "Timestamp 1612510410742, Iteration 187\n",
      "batches :187 0.5398281920641501\n",
      "Iteration 188: tag train_accuracy, simple_value 0.08773\n",
      "Iteration 188: tag train_loss, simple_value 0.53983\n",
      "Timestamp 1612510411048, Iteration 188\n",
      "batches :188 0.5373678731315947\n",
      "Iteration 189: tag train_accuracy, simple_value 0.08793\n",
      "Iteration 189: tag train_loss, simple_value 0.53737\n",
      "Timestamp 1612510411356, Iteration 189\n",
      "batches :189 0.5350765784187291\n",
      "Iteration 190: tag train_accuracy, simple_value 0.08788\n",
      "Iteration 190: tag train_loss, simple_value 0.53508\n",
      "Timestamp 1612510411662, Iteration 190\n",
      "batches :190 0.5326074784915699\n",
      "Iteration 191: tag train_accuracy, simple_value 0.08783\n",
      "Iteration 191: tag train_loss, simple_value 0.53261\n",
      "Timestamp 1612510411969, Iteration 191\n",
      "batches :191 0.5304634553521715\n",
      "Iteration 192: tag train_accuracy, simple_value 0.08778\n",
      "Iteration 192: tag train_loss, simple_value 0.53046\n",
      "Timestamp 1612510412290, Iteration 192\n",
      "batches :192 0.5286635717299456\n",
      "Iteration 193: tag train_accuracy, simple_value 0.0883\n",
      "Iteration 193: tag train_loss, simple_value 0.52866\n",
      "Timestamp 1612510412598, Iteration 193\n",
      "batches :193 0.5269890803978851\n",
      "Iteration 194: tag train_accuracy, simple_value 0.08824\n",
      "Iteration 194: tag train_loss, simple_value 0.52699\n",
      "Timestamp 1612510412904, Iteration 194\n",
      "batches :194 0.5246905207864403\n",
      "Iteration 195: tag train_accuracy, simple_value 0.0886\n",
      "Iteration 195: tag train_loss, simple_value 0.52469\n",
      "Timestamp 1612510413210, Iteration 195\n",
      "batches :195 0.5225875336772356\n",
      "Iteration 196: tag train_accuracy, simple_value 0.08862\n",
      "Iteration 196: tag train_loss, simple_value 0.52259\n",
      "Timestamp 1612510413515, Iteration 196\n",
      "batches :196 0.5205630030178902\n",
      "Iteration 197: tag train_accuracy, simple_value 0.08913\n",
      "Iteration 197: tag train_loss, simple_value 0.52056\n",
      "Timestamp 1612510413824, Iteration 197\n",
      "batches :197 0.5185172164500668\n",
      "Iteration 198: tag train_accuracy, simple_value 0.08915\n",
      "Iteration 198: tag train_loss, simple_value 0.51852\n",
      "Timestamp 1612510414131, Iteration 198\n",
      "batches :198 0.5163827493335261\n",
      "Iteration 199: tag train_accuracy, simple_value 0.08917\n",
      "Iteration 199: tag train_loss, simple_value 0.51638\n",
      "Timestamp 1612510414439, Iteration 199\n",
      "batches :199 0.5141137412669671\n",
      "Iteration 200: tag train_accuracy, simple_value 0.08959\n",
      "Iteration 200: tag train_loss, simple_value 0.51411\n",
      "Timestamp 1612510414747, Iteration 200\n",
      "batches :200 0.5122538170590997\n",
      "Iteration 201: tag train_accuracy, simple_value 0.08937\n",
      "Iteration 201: tag train_loss, simple_value 0.51225\n",
      "Timestamp 1612510415055, Iteration 201\n",
      "batches :201 0.5100982682547759\n",
      "Iteration 202: tag train_accuracy, simple_value 0.0894\n",
      "Iteration 202: tag train_loss, simple_value 0.5101\n",
      "Timestamp 1612510415361, Iteration 202\n",
      "batches :202 0.5080460376742453\n",
      "Iteration 203: tag train_accuracy, simple_value 0.08942\n",
      "Iteration 203: tag train_loss, simple_value 0.50805\n",
      "Timestamp 1612510415667, Iteration 203\n",
      "batches :203 0.5060785989749608\n",
      "Iteration 204: tag train_accuracy, simple_value 0.08959\n",
      "Iteration 204: tag train_loss, simple_value 0.50608\n",
      "Timestamp 1612510415973, Iteration 204\n",
      "batches :204 0.5039961070333626\n",
      "Iteration 205: tag train_accuracy, simple_value 0.08984\n",
      "Iteration 205: tag train_loss, simple_value 0.504\n",
      "Timestamp 1612510416280, Iteration 205\n",
      "batches :205 0.5020467094531873\n",
      "Iteration 206: tag train_accuracy, simple_value 0.09002\n",
      "Iteration 206: tag train_loss, simple_value 0.50205\n",
      "Timestamp 1612510416586, Iteration 206\n",
      "batches :206 0.49997056514314075\n",
      "Iteration 207: tag train_accuracy, simple_value 0.09049\n",
      "Iteration 207: tag train_loss, simple_value 0.49997\n",
      "Timestamp 1612510416893, Iteration 207\n",
      "batches :207 0.4981332970558157\n",
      "Iteration 208: tag train_accuracy, simple_value 0.09058\n",
      "Iteration 208: tag train_loss, simple_value 0.49813\n",
      "Timestamp 1612510417216, Iteration 208\n",
      "batches :208 0.49635474376667005\n",
      "Iteration 209: tag train_accuracy, simple_value 0.09112\n",
      "Iteration 209: tag train_loss, simple_value 0.49635\n",
      "Timestamp 1612510417522, Iteration 209\n",
      "batches :209 0.49459543539006173\n",
      "Iteration 210: tag train_accuracy, simple_value 0.09143\n",
      "Iteration 210: tag train_loss, simple_value 0.4946\n",
      "Timestamp 1612510417829, Iteration 210\n",
      "batches :210 0.4928055882808708\n",
      "Iteration 211: tag train_accuracy, simple_value 0.09152\n",
      "Iteration 211: tag train_loss, simple_value 0.49281\n",
      "Timestamp 1612510418135, Iteration 211\n",
      "batches :211 0.4908041622472035\n",
      "Iteration 212: tag train_accuracy, simple_value 0.09205\n",
      "Iteration 212: tag train_loss, simple_value 0.4908\n",
      "Timestamp 1612510418444, Iteration 212\n",
      "batches :212 0.488880635256756\n",
      "Iteration 213: tag train_accuracy, simple_value 0.09198\n",
      "Iteration 213: tag train_loss, simple_value 0.48888\n",
      "Timestamp 1612510418750, Iteration 213\n",
      "batches :213 0.48693783165003773\n",
      "Iteration 214: tag train_accuracy, simple_value 0.09192\n",
      "Iteration 214: tag train_loss, simple_value 0.48694\n",
      "Timestamp 1612510419055, Iteration 214\n",
      "batches :214 0.4850916264004239\n",
      "Iteration 215: tag train_accuracy, simple_value 0.09185\n",
      "Iteration 215: tag train_loss, simple_value 0.48509\n",
      "Timestamp 1612510419361, Iteration 215\n",
      "batches :215 0.48317675749922906\n",
      "Iteration 216: tag train_accuracy, simple_value 0.09186\n",
      "Iteration 216: tag train_loss, simple_value 0.48318\n",
      "Timestamp 1612510419667, Iteration 216\n",
      "batches :216 0.4825532530882844\n",
      "Iteration 217: tag train_accuracy, simple_value 0.09194\n",
      "Iteration 217: tag train_loss, simple_value 0.48255\n",
      "Timestamp 1612510419972, Iteration 217\n",
      "batches :217 0.4807733018895448\n",
      "Iteration 218: tag train_accuracy, simple_value 0.09202\n",
      "Iteration 218: tag train_loss, simple_value 0.48077\n",
      "Timestamp 1612510420278, Iteration 218\n",
      "batches :218 0.47889410649691155\n",
      "Iteration 219: tag train_accuracy, simple_value 0.09217\n",
      "Iteration 219: tag train_loss, simple_value 0.47889\n",
      "Timestamp 1612510420584, Iteration 219\n",
      "batches :219 0.4770613893458288\n",
      "Iteration 220: tag train_accuracy, simple_value 0.09218\n",
      "Iteration 220: tag train_loss, simple_value 0.47706\n",
      "Timestamp 1612510420892, Iteration 220\n",
      "batches :220 0.47517941506071526\n",
      "Iteration 221: tag train_accuracy, simple_value 0.09233\n",
      "Iteration 221: tag train_loss, simple_value 0.47518\n",
      "Timestamp 1612510421198, Iteration 221\n",
      "batches :221 0.47348678921142856\n",
      "Iteration 222: tag train_accuracy, simple_value 0.09248\n",
      "Iteration 222: tag train_loss, simple_value 0.47349\n",
      "Timestamp 1612510421505, Iteration 222\n",
      "batches :222 0.471838202361051\n",
      "Iteration 223: tag train_accuracy, simple_value 0.09241\n",
      "Iteration 223: tag train_loss, simple_value 0.47184\n",
      "Timestamp 1612510421812, Iteration 223\n",
      "batches :223 0.4701135922306856\n",
      "Iteration 224: tag train_accuracy, simple_value 0.09284\n",
      "Iteration 224: tag train_loss, simple_value 0.47011\n",
      "Timestamp 1612510422133, Iteration 224\n",
      "batches :224 0.46868183430550353\n",
      "Iteration 225: tag train_accuracy, simple_value 0.09277\n",
      "Iteration 225: tag train_loss, simple_value 0.46868\n",
      "Timestamp 1612510422439, Iteration 225\n",
      "batches :225 0.4669183713859982\n",
      "Iteration 226: tag train_accuracy, simple_value 0.09257\n",
      "Iteration 226: tag train_loss, simple_value 0.46692\n",
      "Timestamp 1612510422746, Iteration 226\n",
      "batches :226 0.46519436888330806\n",
      "Iteration 227: tag train_accuracy, simple_value 0.09244\n",
      "Iteration 227: tag train_loss, simple_value 0.46519\n",
      "Timestamp 1612510423053, Iteration 227\n",
      "batches :227 0.4636192768155741\n",
      "Iteration 228: tag train_accuracy, simple_value 0.09292\n",
      "Iteration 228: tag train_loss, simple_value 0.46362\n",
      "Timestamp 1612510423359, Iteration 228\n",
      "batches :228 0.4622438868932557\n",
      "Iteration 229: tag train_accuracy, simple_value 0.09293\n",
      "Iteration 229: tag train_loss, simple_value 0.46224\n",
      "Timestamp 1612510423664, Iteration 229\n",
      "batches :229 0.460541670597016\n",
      "Iteration 230: tag train_accuracy, simple_value 0.09286\n",
      "Iteration 230: tag train_loss, simple_value 0.46054\n",
      "Timestamp 1612510423970, Iteration 230\n",
      "batches :230 0.4588029142793106\n",
      "Iteration 231: tag train_accuracy, simple_value 0.09348\n",
      "Iteration 231: tag train_loss, simple_value 0.4588\n",
      "Timestamp 1612510424276, Iteration 231\n",
      "batches :231 0.4571355241169403\n",
      "Iteration 232: tag train_accuracy, simple_value 0.09348\n",
      "Iteration 232: tag train_loss, simple_value 0.45714\n",
      "Timestamp 1612510424581, Iteration 232\n",
      "batches :232 0.4557229477494698\n",
      "Iteration 233: tag train_accuracy, simple_value 0.09362\n",
      "Iteration 233: tag train_loss, simple_value 0.45572\n",
      "Timestamp 1612510424887, Iteration 233\n",
      "batches :233 0.454072799853373\n",
      "Iteration 234: tag train_accuracy, simple_value 0.09375\n",
      "Iteration 234: tag train_loss, simple_value 0.45407\n",
      "Timestamp 1612510425193, Iteration 234\n",
      "batches :234 0.4524867924519329\n",
      "Iteration 235: tag train_accuracy, simple_value 0.09395\n",
      "Iteration 235: tag train_loss, simple_value 0.45249\n",
      "Timestamp 1612510425500, Iteration 235\n",
      "batches :235 0.4509115181863308\n",
      "Iteration 236: tag train_accuracy, simple_value 0.09388\n",
      "Iteration 236: tag train_loss, simple_value 0.45091\n",
      "Timestamp 1612510425806, Iteration 236\n",
      "batches :236 0.4493378978064757\n",
      "Iteration 237: tag train_accuracy, simple_value 0.09401\n",
      "Iteration 237: tag train_loss, simple_value 0.44934\n",
      "Timestamp 1612510426112, Iteration 237\n",
      "batches :237 0.44777248358110333\n",
      "Iteration 238: tag train_accuracy, simple_value 0.09415\n",
      "Iteration 238: tag train_loss, simple_value 0.44777\n",
      "Timestamp 1612510426420, Iteration 238\n",
      "batches :238 0.4469576715139531\n",
      "Iteration 239: tag train_accuracy, simple_value 0.09434\n",
      "Iteration 239: tag train_loss, simple_value 0.44696\n",
      "Timestamp 1612510426729, Iteration 239\n",
      "batches :239 0.4453609930390344\n",
      "Iteration 240: tag train_accuracy, simple_value 0.09447\n",
      "Iteration 240: tag train_loss, simple_value 0.44536\n",
      "Timestamp 1612510427055, Iteration 240\n",
      "batches :240 0.4438719020690769\n",
      "Iteration 241: tag train_accuracy, simple_value 0.0944\n",
      "Iteration 241: tag train_loss, simple_value 0.44387\n",
      "Timestamp 1612510427362, Iteration 241\n",
      "batches :241 0.4423229858681127\n",
      "Iteration 242: tag train_accuracy, simple_value 0.0944\n",
      "Iteration 242: tag train_loss, simple_value 0.44232\n",
      "Timestamp 1612510427669, Iteration 242\n",
      "batches :242 0.44077512447930073\n",
      "Iteration 243: tag train_accuracy, simple_value 0.09472\n",
      "Iteration 243: tag train_loss, simple_value 0.44078\n",
      "Timestamp 1612510427976, Iteration 243\n",
      "batches :243 0.43926277319774215\n",
      "Iteration 244: tag train_accuracy, simple_value 0.09471\n",
      "Iteration 244: tag train_loss, simple_value 0.43926\n",
      "Timestamp 1612510428283, Iteration 244\n",
      "batches :244 0.43778562614480493\n",
      "Iteration 245: tag train_accuracy, simple_value 0.0949\n",
      "Iteration 245: tag train_loss, simple_value 0.43779\n",
      "Timestamp 1612510428590, Iteration 245\n",
      "batches :245 0.4362380011805466\n",
      "Iteration 246: tag train_accuracy, simple_value 0.09503\n",
      "Iteration 246: tag train_loss, simple_value 0.43624\n",
      "Timestamp 1612510428898, Iteration 246\n",
      "batches :246 0.43493447154457493\n",
      "Iteration 247: tag train_accuracy, simple_value 0.09508\n",
      "Iteration 247: tag train_loss, simple_value 0.43493\n",
      "Timestamp 1612510429204, Iteration 247\n",
      "batches :247 0.43364742330392364\n",
      "Iteration 248: tag train_accuracy, simple_value 0.09502\n",
      "Iteration 248: tag train_loss, simple_value 0.43365\n",
      "Timestamp 1612510429510, Iteration 248\n",
      "batches :248 0.4321978122686907\n",
      "Iteration 249: tag train_accuracy, simple_value 0.09488\n",
      "Iteration 249: tag train_loss, simple_value 0.4322\n",
      "Timestamp 1612510429817, Iteration 249\n",
      "batches :249 0.4307548471812503\n",
      "Iteration 250: tag train_accuracy, simple_value 0.09519\n",
      "Iteration 250: tag train_loss, simple_value 0.43075\n",
      "Timestamp 1612510430124, Iteration 250\n",
      "batches :250 0.4293144433647394\n",
      "Iteration 251: tag train_accuracy, simple_value 0.09525\n",
      "Iteration 251: tag train_loss, simple_value 0.42931\n",
      "Timestamp 1612510430431, Iteration 251\n",
      "batches :251 0.427891993828385\n",
      "Iteration 252: tag train_accuracy, simple_value 0.09543\n",
      "Iteration 252: tag train_loss, simple_value 0.42789\n",
      "Timestamp 1612510430737, Iteration 252\n",
      "batches :252 0.42640915428776116\n",
      "Iteration 253: tag train_accuracy, simple_value 0.09586\n",
      "Iteration 253: tag train_loss, simple_value 0.42641\n",
      "Timestamp 1612510431044, Iteration 253\n",
      "batches :253 0.42502197639449785\n",
      "Iteration 254: tag train_accuracy, simple_value 0.09585\n",
      "Iteration 254: tag train_loss, simple_value 0.42502\n",
      "Timestamp 1612510431350, Iteration 254\n",
      "batches :254 0.4236251531007487\n",
      "Iteration 255: tag train_accuracy, simple_value 0.09596\n",
      "Iteration 255: tag train_loss, simple_value 0.42363\n",
      "Timestamp 1612510431656, Iteration 255\n",
      "batches :255 0.42226533561068424\n",
      "Iteration 256: tag train_accuracy, simple_value 0.09589\n",
      "Iteration 256: tag train_loss, simple_value 0.42227\n",
      "Timestamp 1612510431976, Iteration 256\n",
      "batches :256 0.42089136225695256\n",
      "Iteration 257: tag train_accuracy, simple_value 0.09583\n",
      "Iteration 257: tag train_loss, simple_value 0.42089\n",
      "Timestamp 1612510432285, Iteration 257\n",
      "batches :257 0.41962518503867696\n",
      "Iteration 258: tag train_accuracy, simple_value 0.09582\n",
      "Iteration 258: tag train_loss, simple_value 0.41963\n",
      "Timestamp 1612510432591, Iteration 258\n",
      "batches :258 0.41822673894407214\n",
      "Iteration 259: tag train_accuracy, simple_value 0.09581\n",
      "Iteration 259: tag train_loss, simple_value 0.41823\n",
      "Timestamp 1612510432898, Iteration 259\n",
      "batches :259 0.41684564077888675\n",
      "Iteration 260: tag train_accuracy, simple_value 0.09592\n",
      "Iteration 260: tag train_loss, simple_value 0.41685\n",
      "Timestamp 1612510433204, Iteration 260\n",
      "batches :260 0.4154586834833026\n",
      "Iteration 261: tag train_accuracy, simple_value 0.09615\n",
      "Iteration 261: tag train_loss, simple_value 0.41546\n",
      "Timestamp 1612510433511, Iteration 261\n",
      "batches :261 0.41410253953431303\n",
      "Iteration 262: tag train_accuracy, simple_value 0.09632\n",
      "Iteration 262: tag train_loss, simple_value 0.4141\n",
      "Timestamp 1612510433818, Iteration 262\n",
      "batches :262 0.41274958947062035\n",
      "Iteration 263: tag train_accuracy, simple_value 0.09614\n",
      "Iteration 263: tag train_loss, simple_value 0.41275\n",
      "Timestamp 1612510434124, Iteration 263\n",
      "batches :263 0.41145717277556315\n",
      "Iteration 264: tag train_accuracy, simple_value 0.09607\n",
      "Iteration 264: tag train_loss, simple_value 0.41146\n",
      "Timestamp 1612510434431, Iteration 264\n",
      "batches :264 0.4102761206555773\n",
      "Iteration 265: tag train_accuracy, simple_value 0.09612\n",
      "Iteration 265: tag train_loss, simple_value 0.41028\n",
      "Timestamp 1612510434738, Iteration 265\n",
      "batches :265 0.4091578943971193\n",
      "Iteration 266: tag train_accuracy, simple_value 0.09605\n",
      "Iteration 266: tag train_loss, simple_value 0.40916\n",
      "Timestamp 1612510435043, Iteration 266\n",
      "batches :266 0.40785334365708487\n",
      "Iteration 267: tag train_accuracy, simple_value 0.09586\n",
      "Iteration 267: tag train_loss, simple_value 0.40785\n",
      "Timestamp 1612510435350, Iteration 267\n",
      "batches :267 0.40659456379181436\n",
      "Iteration 268: tag train_accuracy, simple_value 0.09597\n",
      "Iteration 268: tag train_loss, simple_value 0.40659\n",
      "Timestamp 1612510435656, Iteration 268\n",
      "batches :268 0.40531948317231525\n",
      "Iteration 269: tag train_accuracy, simple_value 0.09585\n",
      "Iteration 269: tag train_loss, simple_value 0.40532\n",
      "Timestamp 1612510435963, Iteration 269\n",
      "batches :269 0.4040838856679356\n",
      "Iteration 270: tag train_accuracy, simple_value 0.09607\n",
      "Iteration 270: tag train_loss, simple_value 0.40408\n",
      "Timestamp 1612510436270, Iteration 270\n",
      "batches :270 0.40279131873890206\n",
      "Iteration 271: tag train_accuracy, simple_value 0.09612\n",
      "Iteration 271: tag train_loss, simple_value 0.40279\n",
      "Timestamp 1612510436577, Iteration 271\n",
      "batches :271 0.40153830649668\n",
      "Iteration 272: tag train_accuracy, simple_value 0.096\n",
      "Iteration 272: tag train_loss, simple_value 0.40154\n",
      "Timestamp 1612510436899, Iteration 272\n",
      "batches :272 0.4002754939446116\n",
      "Iteration 273: tag train_accuracy, simple_value 0.09593\n",
      "Iteration 273: tag train_loss, simple_value 0.40028\n",
      "Timestamp 1612510437209, Iteration 273\n",
      "batches :273 0.3995349942928269\n",
      "Iteration 274: tag train_accuracy, simple_value 0.0961\n",
      "Iteration 274: tag train_loss, simple_value 0.39953\n",
      "Timestamp 1612510437517, Iteration 274\n",
      "batches :274 0.3983106130774874\n",
      "Iteration 275: tag train_accuracy, simple_value 0.09603\n",
      "Iteration 275: tag train_loss, simple_value 0.39831\n",
      "Timestamp 1612510437824, Iteration 275\n",
      "batches :275 0.3971277871186083\n",
      "Iteration 276: tag train_accuracy, simple_value 0.09591\n",
      "Iteration 276: tag train_loss, simple_value 0.39713\n",
      "Timestamp 1612510438131, Iteration 276\n",
      "batches :276 0.3959183701300535\n",
      "Iteration 277: tag train_accuracy, simple_value 0.09607\n",
      "Iteration 277: tag train_loss, simple_value 0.39592\n",
      "Timestamp 1612510438440, Iteration 277\n",
      "batches :277 0.39469615984640827\n",
      "Iteration 278: tag train_accuracy, simple_value 0.09618\n",
      "Iteration 278: tag train_loss, simple_value 0.3947\n",
      "Timestamp 1612510438747, Iteration 278\n",
      "batches :278 0.39350325591433394\n",
      "Iteration 279: tag train_accuracy, simple_value 0.09622\n",
      "Iteration 279: tag train_loss, simple_value 0.3935\n",
      "Timestamp 1612510439054, Iteration 279\n",
      "batches :279 0.39229813330276037\n",
      "Iteration 280: tag train_accuracy, simple_value 0.0961\n",
      "Iteration 280: tag train_loss, simple_value 0.3923\n",
      "Timestamp 1612510439360, Iteration 280\n",
      "batches :280 0.3911839131798063\n",
      "Iteration 281: tag train_accuracy, simple_value 0.09626\n",
      "Iteration 281: tag train_loss, simple_value 0.39118\n",
      "Timestamp 1612510439669, Iteration 281\n",
      "batches :281 0.39004724019362835\n",
      "Iteration 282: tag train_accuracy, simple_value 0.09659\n",
      "Iteration 282: tag train_loss, simple_value 0.39005\n",
      "Timestamp 1612510439975, Iteration 282\n",
      "batches :282 0.38887726975248216\n",
      "Iteration 283: tag train_accuracy, simple_value 0.09669\n",
      "Iteration 283: tag train_loss, simple_value 0.38888\n",
      "Timestamp 1612510440283, Iteration 283\n",
      "batches :283 0.38771253633362246\n",
      "Iteration 284: tag train_accuracy, simple_value 0.09679\n",
      "Iteration 284: tag train_loss, simple_value 0.38771\n",
      "Timestamp 1612510440590, Iteration 284\n",
      "batches :284 0.3867048183980752\n",
      "Iteration 285: tag train_accuracy, simple_value 0.09689\n",
      "Iteration 285: tag train_loss, simple_value 0.3867\n",
      "Timestamp 1612510440900, Iteration 285\n",
      "batches :285 0.38560602815266243\n",
      "Iteration 286: tag train_accuracy, simple_value 0.09688\n",
      "Iteration 286: tag train_loss, simple_value 0.38561\n",
      "Timestamp 1612510441207, Iteration 286\n",
      "batches :286 0.3845285343305839\n",
      "Iteration 287: tag train_accuracy, simple_value 0.09697\n",
      "Iteration 287: tag train_loss, simple_value 0.38453\n",
      "Timestamp 1612510441513, Iteration 287\n",
      "batches :287 0.3834292290907496\n",
      "Iteration 288: tag train_accuracy, simple_value 0.09696\n",
      "Iteration 288: tag train_loss, simple_value 0.38343\n",
      "Timestamp 1612510441833, Iteration 288\n",
      "batches :288 0.3823831778459458\n",
      "Iteration 289: tag train_accuracy, simple_value 0.09684\n",
      "Iteration 289: tag train_loss, simple_value 0.38238\n",
      "Timestamp 1612510442141, Iteration 289\n",
      "batches :289 0.38125169221762967\n",
      "Iteration 290: tag train_accuracy, simple_value 0.09689\n",
      "Iteration 290: tag train_loss, simple_value 0.38125\n",
      "Timestamp 1612510442446, Iteration 290\n",
      "batches :290 0.3801904908413517\n",
      "Iteration 291: tag train_accuracy, simple_value 0.09671\n",
      "Iteration 291: tag train_loss, simple_value 0.38019\n",
      "Timestamp 1612510442752, Iteration 291\n",
      "batches :291 0.3790731603388524\n",
      "Iteration 292: tag train_accuracy, simple_value 0.09686\n",
      "Iteration 292: tag train_loss, simple_value 0.37907\n",
      "Timestamp 1612510443059, Iteration 292\n",
      "batches :292 0.3779675568874976\n",
      "Iteration 293: tag train_accuracy, simple_value 0.09691\n",
      "Iteration 293: tag train_loss, simple_value 0.37797\n",
      "Timestamp 1612510443365, Iteration 293\n",
      "batches :293 0.37683333933302565\n",
      "Iteration 294: tag train_accuracy, simple_value 0.09679\n",
      "Iteration 294: tag train_loss, simple_value 0.37683\n",
      "Timestamp 1612510443671, Iteration 294\n",
      "batches :294 0.37575019939112014\n",
      "Iteration 295: tag train_accuracy, simple_value 0.09683\n",
      "Iteration 295: tag train_loss, simple_value 0.37575\n",
      "Timestamp 1612510443977, Iteration 295\n",
      "batches :295 0.37466344777810373\n",
      "Iteration 296: tag train_accuracy, simple_value 0.09688\n",
      "Iteration 296: tag train_loss, simple_value 0.37466\n",
      "Timestamp 1612510444284, Iteration 296\n",
      "batches :296 0.37362162225149775\n",
      "Iteration 297: tag train_accuracy, simple_value 0.09676\n",
      "Iteration 297: tag train_loss, simple_value 0.37362\n",
      "Timestamp 1612510444590, Iteration 297\n",
      "batches :297 0.3726079913000466\n",
      "Iteration 298: tag train_accuracy, simple_value 0.0967\n",
      "Iteration 298: tag train_loss, simple_value 0.37261\n",
      "Timestamp 1612510444895, Iteration 298\n",
      "batches :298 0.3715230106007333\n",
      "Iteration 299: tag train_accuracy, simple_value 0.0969\n",
      "Iteration 299: tag train_loss, simple_value 0.37152\n",
      "Timestamp 1612510445201, Iteration 299\n",
      "batches :299 0.3704668909660151\n",
      "Iteration 300: tag train_accuracy, simple_value 0.09689\n",
      "Iteration 300: tag train_loss, simple_value 0.37047\n",
      "Timestamp 1612510445508, Iteration 300\n",
      "batches :300 0.3695716086526712\n",
      "Iteration 301: tag train_accuracy, simple_value 0.09661\n",
      "Iteration 301: tag train_loss, simple_value 0.36957\n",
      "Timestamp 1612510445813, Iteration 301\n",
      "batches :301 0.3685020785840643\n",
      "Iteration 302: tag train_accuracy, simple_value 0.09676\n",
      "Iteration 302: tag train_loss, simple_value 0.3685\n",
      "Timestamp 1612510446119, Iteration 302\n",
      "batches :302 0.3674892003074387\n",
      "Iteration 303: tag train_accuracy, simple_value 0.0967\n",
      "Iteration 303: tag train_loss, simple_value 0.36749\n",
      "Timestamp 1612510446427, Iteration 303\n",
      "batches :303 0.36651746136913993\n",
      "Iteration 304: tag train_accuracy, simple_value 0.09674\n",
      "Iteration 304: tag train_loss, simple_value 0.36652\n",
      "Timestamp 1612510446749, Iteration 304\n",
      "batches :304 0.36549117644072365\n",
      "Iteration 305: tag train_accuracy, simple_value 0.09683\n",
      "Iteration 305: tag train_loss, simple_value 0.36549\n",
      "Timestamp 1612510447055, Iteration 305\n",
      "batches :305 0.3645164916505579\n",
      "Iteration 306: tag train_accuracy, simple_value 0.09693\n",
      "Iteration 306: tag train_loss, simple_value 0.36452\n",
      "Timestamp 1612510447362, Iteration 306\n",
      "batches :306 0.36347279126688936\n",
      "Iteration 307: tag train_accuracy, simple_value 0.09697\n",
      "Iteration 307: tag train_loss, simple_value 0.36347\n",
      "Timestamp 1612510447670, Iteration 307\n",
      "batches :307 0.3624565191081564\n",
      "Iteration 308: tag train_accuracy, simple_value 0.09701\n",
      "Iteration 308: tag train_loss, simple_value 0.36246\n",
      "Timestamp 1612510447976, Iteration 308\n",
      "batches :308 0.3614527309289226\n",
      "Iteration 309: tag train_accuracy, simple_value 0.09715\n",
      "Iteration 309: tag train_loss, simple_value 0.36145\n",
      "Timestamp 1612510448283, Iteration 309\n",
      "batches :309 0.3604594755278822\n",
      "Iteration 310: tag train_accuracy, simple_value 0.09709\n",
      "Iteration 310: tag train_loss, simple_value 0.36046\n",
      "Timestamp 1612510448590, Iteration 310\n",
      "batches :310 0.3594452533390253\n",
      "Iteration 311: tag train_accuracy, simple_value 0.09733\n",
      "Iteration 311: tag train_loss, simple_value 0.35945\n",
      "Timestamp 1612510448896, Iteration 311\n",
      "batches :311 0.35842454302876325\n",
      "Iteration 312: tag train_accuracy, simple_value 0.09752\n",
      "Iteration 312: tag train_loss, simple_value 0.35842\n",
      "Timestamp 1612510449202, Iteration 312\n",
      "batches :312 0.3574728709287368\n",
      "Iteration 313: tag train_accuracy, simple_value 0.09771\n",
      "Iteration 313: tag train_loss, simple_value 0.35747\n",
      "Timestamp 1612510449508, Iteration 313\n",
      "batches :313 0.3565079220377218\n",
      "Iteration 314: tag train_accuracy, simple_value 0.09769\n",
      "Iteration 314: tag train_loss, simple_value 0.35651\n",
      "Timestamp 1612510449817, Iteration 314\n",
      "batches :314 0.35551623360603857\n",
      "Iteration 315: tag train_accuracy, simple_value 0.09768\n",
      "Iteration 315: tag train_loss, simple_value 0.35552\n",
      "Timestamp 1612510450123, Iteration 315\n",
      "batches :315 0.35454729629887477\n",
      "Iteration 316: tag train_accuracy, simple_value 0.09752\n",
      "Iteration 316: tag train_loss, simple_value 0.35455\n",
      "Timestamp 1612510450430, Iteration 316\n",
      "batches :316 0.3535599284982191\n",
      "Iteration 317: tag train_accuracy, simple_value 0.09771\n",
      "Iteration 317: tag train_loss, simple_value 0.35356\n",
      "Timestamp 1612510450735, Iteration 317\n",
      "batches :317 0.3526235426826056\n",
      "Iteration 318: tag train_accuracy, simple_value 0.09764\n",
      "Iteration 318: tag train_loss, simple_value 0.35262\n",
      "Timestamp 1612510451042, Iteration 318\n",
      "batches :318 0.3516698527214287\n",
      "Iteration 319: tag train_accuracy, simple_value 0.09758\n",
      "Iteration 319: tag train_loss, simple_value 0.35167\n",
      "Timestamp 1612510451351, Iteration 319\n",
      "batches :319 0.3507626892817805\n",
      "Iteration 320: tag train_accuracy, simple_value 0.09767\n",
      "Iteration 320: tag train_loss, simple_value 0.35076\n",
      "Timestamp 1612510451671, Iteration 320\n",
      "batches :320 0.3498087361571379\n",
      "Iteration 321: tag train_accuracy, simple_value 0.09771\n",
      "Iteration 321: tag train_loss, simple_value 0.34981\n",
      "Timestamp 1612510451977, Iteration 321\n",
      "batches :321 0.3488593405443374\n",
      "Iteration 322: tag train_accuracy, simple_value 0.09769\n",
      "Iteration 322: tag train_loss, simple_value 0.34886\n",
      "Timestamp 1612510452285, Iteration 322\n",
      "batches :322 0.34792024932616616\n",
      "Iteration 323: tag train_accuracy, simple_value 0.09763\n",
      "Iteration 323: tag train_loss, simple_value 0.34792\n",
      "Timestamp 1612510452591, Iteration 323\n",
      "batches :323 0.34697852095721676\n",
      "Iteration 324: tag train_accuracy, simple_value 0.09762\n",
      "Iteration 324: tag train_loss, simple_value 0.34698\n",
      "Timestamp 1612510452897, Iteration 324\n",
      "batches :324 0.34604856212060026\n",
      "Iteration 325: tag train_accuracy, simple_value 0.09766\n",
      "Iteration 325: tag train_loss, simple_value 0.34605\n",
      "Timestamp 1612510453204, Iteration 325\n",
      "batches :325 0.3451258496481639\n",
      "Iteration 326: tag train_accuracy, simple_value 0.09774\n",
      "Iteration 326: tag train_loss, simple_value 0.34513\n",
      "Timestamp 1612510453511, Iteration 326\n",
      "batches :326 0.3442412153960554\n",
      "Iteration 327: tag train_accuracy, simple_value 0.09763\n",
      "Iteration 327: tag train_loss, simple_value 0.34424\n",
      "Timestamp 1612510453817, Iteration 327\n",
      "batches :327 0.34332454791264067\n",
      "Iteration 328: tag train_accuracy, simple_value 0.09772\n",
      "Iteration 328: tag train_loss, simple_value 0.34332\n",
      "Timestamp 1612510454125, Iteration 328\n",
      "batches :328 0.342730503563383\n",
      "Iteration 329: tag train_accuracy, simple_value 0.0977\n",
      "Iteration 329: tag train_loss, simple_value 0.34273\n",
      "Timestamp 1612510454431, Iteration 329\n",
      "batches :329 0.3418247256665788\n",
      "Iteration 330: tag train_accuracy, simple_value 0.09769\n",
      "Iteration 330: tag train_loss, simple_value 0.34182\n",
      "Timestamp 1612510454736, Iteration 330\n",
      "batches :330 0.3409245753604354\n",
      "Iteration 331: tag train_accuracy, simple_value 0.09777\n",
      "Iteration 331: tag train_loss, simple_value 0.34092\n",
      "Timestamp 1612510455042, Iteration 331\n",
      "batches :331 0.3400409171791055\n",
      "Iteration 332: tag train_accuracy, simple_value 0.09772\n",
      "Iteration 332: tag train_loss, simple_value 0.34004\n",
      "Timestamp 1612510455347, Iteration 332\n",
      "batches :332 0.33921717225113907\n",
      "Iteration 333: tag train_accuracy, simple_value 0.0977\n",
      "Iteration 333: tag train_loss, simple_value 0.33922\n",
      "Timestamp 1612510455653, Iteration 333\n",
      "batches :333 0.3383810381788212\n",
      "Iteration 334: tag train_accuracy, simple_value 0.09779\n",
      "Iteration 334: tag train_loss, simple_value 0.33838\n",
      "Timestamp 1612510455959, Iteration 334\n",
      "batches :334 0.3375054577666664\n",
      "Iteration 335: tag train_accuracy, simple_value 0.09777\n",
      "Iteration 335: tag train_loss, simple_value 0.33751\n",
      "Timestamp 1612510456264, Iteration 335\n",
      "batches :335 0.3368988465795766\n",
      "Iteration 336: tag train_accuracy, simple_value 0.09776\n",
      "Iteration 336: tag train_loss, simple_value 0.3369\n",
      "Timestamp 1612510456584, Iteration 336\n",
      "batches :336 0.33603229297191967\n",
      "Iteration 337: tag train_accuracy, simple_value 0.09784\n",
      "Iteration 337: tag train_loss, simple_value 0.33603\n",
      "Timestamp 1612510456891, Iteration 337\n",
      "batches :337 0.3351522982120514\n",
      "Iteration 338: tag train_accuracy, simple_value 0.09778\n",
      "Iteration 338: tag train_loss, simple_value 0.33515\n",
      "Timestamp 1612510457198, Iteration 338\n",
      "batches :338 0.3342843944063553\n",
      "Iteration 339: tag train_accuracy, simple_value 0.09777\n",
      "Iteration 339: tag train_loss, simple_value 0.33428\n",
      "Timestamp 1612510457504, Iteration 339\n",
      "batches :339 0.33343256382507913\n",
      "Iteration 340: tag train_accuracy, simple_value 0.09781\n",
      "Iteration 340: tag train_loss, simple_value 0.33343\n",
      "Timestamp 1612510457810, Iteration 340\n",
      "batches :340 0.3325754282009952\n",
      "Iteration 341: tag train_accuracy, simple_value 0.09775\n",
      "Iteration 341: tag train_loss, simple_value 0.33258\n",
      "Timestamp 1612510458116, Iteration 341\n",
      "batches :341 0.331716845975506\n",
      "Iteration 342: tag train_accuracy, simple_value 0.09769\n",
      "Iteration 342: tag train_loss, simple_value 0.33172\n",
      "Timestamp 1612510458422, Iteration 342\n",
      "batches :342 0.3308603823381035\n",
      "Iteration 343: tag train_accuracy, simple_value 0.09772\n",
      "Iteration 343: tag train_loss, simple_value 0.33086\n",
      "Timestamp 1612510458728, Iteration 343\n",
      "batches :343 0.33001445664999784\n",
      "Iteration 344: tag train_accuracy, simple_value 0.09753\n",
      "Iteration 344: tag train_loss, simple_value 0.33001\n",
      "Timestamp 1612510459034, Iteration 344\n",
      "batches :344 0.3291780445183259\n",
      "Iteration 345: tag train_accuracy, simple_value 0.09766\n",
      "Iteration 345: tag train_loss, simple_value 0.32918\n",
      "Timestamp 1612510459340, Iteration 345\n",
      "batches :345 0.3283448339480421\n",
      "Iteration 346: tag train_accuracy, simple_value 0.09787\n",
      "Iteration 346: tag train_loss, simple_value 0.32834\n",
      "Timestamp 1612510459646, Iteration 346\n",
      "batches :346 0.3275225534281462\n",
      "Iteration 347: tag train_accuracy, simple_value 0.09786\n",
      "Iteration 347: tag train_loss, simple_value 0.32752\n",
      "Timestamp 1612510459952, Iteration 347\n",
      "batches :347 0.32672009804965096\n",
      "Iteration 348: tag train_accuracy, simple_value 0.0978\n",
      "Iteration 348: tag train_loss, simple_value 0.32672\n",
      "Timestamp 1612510460258, Iteration 348\n",
      "batches :348 0.32590037259947635\n",
      "Iteration 349: tag train_accuracy, simple_value 0.09784\n",
      "Iteration 349: tag train_loss, simple_value 0.3259\n",
      "Timestamp 1612510460565, Iteration 349\n",
      "batches :349 0.3250815167937716\n",
      "Iteration 350: tag train_accuracy, simple_value 0.09805\n",
      "Iteration 350: tag train_loss, simple_value 0.32508\n",
      "Timestamp 1612510460872, Iteration 350\n",
      "batches :350 0.3242734730456557\n",
      "Iteration 351: tag train_accuracy, simple_value 0.09817\n",
      "Iteration 351: tag train_loss, simple_value 0.32427\n",
      "Timestamp 1612510461179, Iteration 351\n",
      "batches :351 0.32346448068775\n",
      "Iteration 352: tag train_accuracy, simple_value 0.09829\n",
      "Iteration 352: tag train_loss, simple_value 0.32346\n",
      "Timestamp 1612510461501, Iteration 352\n",
      "batches :352 0.3226633253401484\n",
      "Iteration 353: tag train_accuracy, simple_value 0.09832\n",
      "Iteration 353: tag train_loss, simple_value 0.32266\n",
      "Timestamp 1612510461807, Iteration 353\n",
      "batches :353 0.3219113450064706\n",
      "Iteration 354: tag train_accuracy, simple_value 0.09831\n",
      "Iteration 354: tag train_loss, simple_value 0.32191\n",
      "Timestamp 1612510462114, Iteration 354\n",
      "batches :354 0.32112532935110527\n",
      "Iteration 355: tag train_accuracy, simple_value 0.09847\n",
      "Iteration 355: tag train_loss, simple_value 0.32113\n",
      "Timestamp 1612510462424, Iteration 355\n",
      "batches :355 0.3205453909094065\n",
      "Iteration 356: tag train_accuracy, simple_value 0.0985\n",
      "Iteration 356: tag train_loss, simple_value 0.32055\n",
      "Timestamp 1612510462731, Iteration 356\n",
      "batches :356 0.31975732058328526\n",
      "Iteration 357: tag train_accuracy, simple_value 0.09862\n",
      "Iteration 357: tag train_loss, simple_value 0.31976\n",
      "Timestamp 1612510463038, Iteration 357\n",
      "batches :357 0.3189835918634212\n",
      "Iteration 358: tag train_accuracy, simple_value 0.09861\n",
      "Iteration 358: tag train_loss, simple_value 0.31898\n",
      "Timestamp 1612510463346, Iteration 358\n",
      "batches :358 0.31821685936822236\n",
      "Iteration 359: tag train_accuracy, simple_value 0.09859\n",
      "Iteration 359: tag train_loss, simple_value 0.31822\n",
      "Timestamp 1612510463655, Iteration 359\n",
      "batches :359 0.3174354897858705\n",
      "Iteration 360: tag train_accuracy, simple_value 0.09871\n",
      "Iteration 360: tag train_loss, simple_value 0.31744\n",
      "Timestamp 1612510463961, Iteration 360\n",
      "batches :360 0.31667406060215497\n",
      "Iteration 361: tag train_accuracy, simple_value 0.09865\n",
      "Iteration 361: tag train_loss, simple_value 0.31667\n",
      "Timestamp 1612510464268, Iteration 361\n",
      "batches :361 0.3159120750930831\n",
      "Iteration 362: tag train_accuracy, simple_value 0.09868\n",
      "Iteration 362: tag train_loss, simple_value 0.31591\n",
      "Timestamp 1612510464574, Iteration 362\n",
      "batches :362 0.3151693312832005\n",
      "Iteration 363: tag train_accuracy, simple_value 0.09871\n",
      "Iteration 363: tag train_loss, simple_value 0.31517\n",
      "Timestamp 1612510464880, Iteration 363\n",
      "batches :363 0.3144764466308694\n",
      "Iteration 364: tag train_accuracy, simple_value 0.09892\n",
      "Iteration 364: tag train_loss, simple_value 0.31448\n",
      "Timestamp 1612510465186, Iteration 364\n",
      "batches :364 0.3137322572152038\n",
      "Iteration 365: tag train_accuracy, simple_value 0.09882\n",
      "Iteration 365: tag train_loss, simple_value 0.31373\n",
      "Timestamp 1612510465493, Iteration 365\n",
      "batches :365 0.312992857039383\n",
      "Iteration 366: tag train_accuracy, simple_value 0.09884\n",
      "Iteration 366: tag train_loss, simple_value 0.31299\n",
      "Timestamp 1612510465800, Iteration 366\n",
      "batches :366 0.31224610628000377\n",
      "Iteration 367: tag train_accuracy, simple_value 0.09896\n",
      "Iteration 367: tag train_loss, simple_value 0.31225\n",
      "Timestamp 1612510466107, Iteration 367\n",
      "batches :367 0.3115019931658412\n",
      "Iteration 368: tag train_accuracy, simple_value 0.09903\n",
      "Iteration 368: tag train_loss, simple_value 0.3115\n",
      "Timestamp 1612510466429, Iteration 368\n",
      "batches :368 0.31076147110687324\n",
      "Iteration 369: tag train_accuracy, simple_value 0.09914\n",
      "Iteration 369: tag train_loss, simple_value 0.31076\n",
      "Timestamp 1612510466736, Iteration 369\n",
      "batches :369 0.3100478015133359\n",
      "Iteration 370: tag train_accuracy, simple_value 0.09904\n",
      "Iteration 370: tag train_loss, simple_value 0.31005\n",
      "Timestamp 1612510467043, Iteration 370\n",
      "batches :370 0.3093234014269468\n",
      "Iteration 371: tag train_accuracy, simple_value 0.0992\n",
      "Iteration 371: tag train_loss, simple_value 0.30932\n",
      "Timestamp 1612510467350, Iteration 371\n",
      "batches :371 0.3086025118225347\n",
      "Iteration 372: tag train_accuracy, simple_value 0.09944\n",
      "Iteration 372: tag train_loss, simple_value 0.3086\n",
      "Timestamp 1612510467658, Iteration 372\n",
      "batches :372 0.308033160644994\n",
      "Iteration 373: tag train_accuracy, simple_value 0.09942\n",
      "Iteration 373: tag train_loss, simple_value 0.30803\n",
      "Timestamp 1612510467967, Iteration 373\n",
      "batches :373 0.3073278442224932\n",
      "Iteration 374: tag train_accuracy, simple_value 0.09945\n",
      "Iteration 374: tag train_loss, simple_value 0.30733\n",
      "Timestamp 1612510468274, Iteration 374\n",
      "batches :374 0.3066189765312614\n",
      "Iteration 375: tag train_accuracy, simple_value 0.09981\n",
      "Iteration 375: tag train_loss, simple_value 0.30662\n",
      "Timestamp 1612510468582, Iteration 375\n",
      "batches :375 0.30592676332592966\n",
      "Iteration 376: tag train_accuracy, simple_value 0.09988\n",
      "Iteration 376: tag train_loss, simple_value 0.30593\n",
      "Timestamp 1612510468890, Iteration 376\n",
      "batches :376 0.30523296342568196\n",
      "Iteration 377: tag train_accuracy, simple_value 0.0999\n",
      "Iteration 377: tag train_loss, simple_value 0.30523\n",
      "Timestamp 1612510469199, Iteration 377\n",
      "batches :377 0.3045356032149545\n",
      "Iteration 378: tag train_accuracy, simple_value 0.10009\n",
      "Iteration 378: tag train_loss, simple_value 0.30454\n",
      "Timestamp 1612510469505, Iteration 378\n",
      "batches :378 0.30383529022296585\n",
      "Iteration 379: tag train_accuracy, simple_value 0.10041\n",
      "Iteration 379: tag train_loss, simple_value 0.30384\n",
      "Timestamp 1612510469811, Iteration 379\n",
      "batches :379 0.30315074682392984\n",
      "Iteration 380: tag train_accuracy, simple_value 0.10035\n",
      "Iteration 380: tag train_loss, simple_value 0.30315\n",
      "Timestamp 1612510470118, Iteration 380\n",
      "batches :380 0.30246861478019704\n",
      "Iteration 381: tag train_accuracy, simple_value 0.10033\n",
      "Iteration 381: tag train_loss, simple_value 0.30247\n",
      "Timestamp 1612510470423, Iteration 381\n",
      "batches :381 0.3017954364751424\n",
      "Iteration 382: tag train_accuracy, simple_value 0.10019\n",
      "Iteration 382: tag train_loss, simple_value 0.3018\n",
      "Timestamp 1612510470730, Iteration 382\n",
      "batches :382 0.3011220285074093\n",
      "Iteration 383: tag train_accuracy, simple_value 0.10021\n",
      "Iteration 383: tag train_loss, simple_value 0.30112\n",
      "Timestamp 1612510471037, Iteration 383\n",
      "batches :383 0.30045706710678477\n",
      "Iteration 384: tag train_accuracy, simple_value 0.1002\n",
      "Iteration 384: tag train_loss, simple_value 0.30046\n",
      "Timestamp 1612510471360, Iteration 384\n",
      "batches :384 0.299787449495246\n",
      "Iteration 385: tag train_accuracy, simple_value 0.10026\n",
      "Iteration 385: tag train_loss, simple_value 0.29979\n",
      "Timestamp 1612510471665, Iteration 385\n",
      "batches :385 0.2991156919823064\n",
      "Iteration 386: tag train_accuracy, simple_value 0.10041\n",
      "Iteration 386: tag train_loss, simple_value 0.29912\n",
      "Timestamp 1612510471972, Iteration 386\n",
      "batches :386 0.2984410912242424\n",
      "Iteration 387: tag train_accuracy, simple_value 0.10067\n",
      "Iteration 387: tag train_loss, simple_value 0.29844\n",
      "Timestamp 1612510472278, Iteration 387\n",
      "batches :387 0.2977661743509831\n",
      "Iteration 388: tag train_accuracy, simple_value 0.1009\n",
      "Iteration 388: tag train_loss, simple_value 0.29777\n",
      "Timestamp 1612510472586, Iteration 388\n",
      "batches :388 0.29710265240372763\n",
      "Iteration 389: tag train_accuracy, simple_value 0.10104\n",
      "Iteration 389: tag train_loss, simple_value 0.2971\n",
      "Timestamp 1612510472893, Iteration 389\n",
      "batches :389 0.2964516453023588\n",
      "Iteration 390: tag train_accuracy, simple_value 0.10098\n",
      "Iteration 390: tag train_loss, simple_value 0.29645\n",
      "Timestamp 1612510473199, Iteration 390\n",
      "batches :390 0.29579256183634967\n",
      "Iteration 391: tag train_accuracy, simple_value 0.10096\n",
      "Iteration 391: tag train_loss, simple_value 0.29579\n",
      "Timestamp 1612510473510, Iteration 391\n",
      "batches :391 0.2951410029302625\n",
      "Iteration 392: tag train_accuracy, simple_value 0.10082\n",
      "Iteration 392: tag train_loss, simple_value 0.29514\n",
      "Timestamp 1612510473816, Iteration 392\n",
      "batches :392 0.29448262603991493\n",
      "Iteration 393: tag train_accuracy, simple_value 0.10092\n",
      "Iteration 393: tag train_loss, simple_value 0.29448\n",
      "Timestamp 1612510474123, Iteration 393\n",
      "batches :393 0.29383437702332743\n",
      "Iteration 394: tag train_accuracy, simple_value 0.10099\n",
      "Iteration 394: tag train_loss, simple_value 0.29383\n",
      "Timestamp 1612510474429, Iteration 394\n",
      "batches :394 0.293184963693171\n",
      "Iteration 395: tag train_accuracy, simple_value 0.10109\n",
      "Iteration 395: tag train_loss, simple_value 0.29318\n",
      "Timestamp 1612510474737, Iteration 395\n",
      "batches :395 0.2925588859410226\n",
      "Iteration 396: tag train_accuracy, simple_value 0.10095\n",
      "Iteration 396: tag train_loss, simple_value 0.29256\n",
      "Timestamp 1612510475043, Iteration 396\n",
      "batches :396 0.2919290685706367\n",
      "Iteration 397: tag train_accuracy, simple_value 0.10081\n",
      "Iteration 397: tag train_loss, simple_value 0.29193\n",
      "Timestamp 1612510475349, Iteration 397\n",
      "batches :397 0.29129601094487034\n",
      "Iteration 398: tag train_accuracy, simple_value 0.10091\n",
      "Iteration 398: tag train_loss, simple_value 0.2913\n",
      "Timestamp 1612510475655, Iteration 398\n",
      "batches :398 0.290661597577621\n",
      "Iteration 399: tag train_accuracy, simple_value 0.10101\n",
      "Iteration 399: tag train_loss, simple_value 0.29066\n",
      "Timestamp 1612510475963, Iteration 399\n",
      "batches :399 0.2900493420044282\n",
      "Iteration 400: tag train_accuracy, simple_value 0.10099\n",
      "Iteration 400: tag train_loss, simple_value 0.29005\n",
      "Timestamp 1612510476283, Iteration 400\n",
      "batches :400 0.2894234332814813\n",
      "Iteration 401: tag train_accuracy, simple_value 0.10117\n",
      "Iteration 401: tag train_loss, simple_value 0.28942\n",
      "Timestamp 1612510476589, Iteration 401\n",
      "batches :401 0.2888001795588735\n",
      "Iteration 402: tag train_accuracy, simple_value 0.10115\n",
      "Iteration 402: tag train_loss, simple_value 0.2888\n",
      "Timestamp 1612510476895, Iteration 402\n",
      "batches :402 0.288173086959776\n",
      "Iteration 403: tag train_accuracy, simple_value 0.10117\n",
      "Iteration 403: tag train_loss, simple_value 0.28817\n",
      "Timestamp 1612510477201, Iteration 403\n",
      "batches :403 0.2875552290944248\n",
      "Iteration 404: tag train_accuracy, simple_value 0.10108\n",
      "Iteration 404: tag train_loss, simple_value 0.28756\n",
      "Timestamp 1612510477507, Iteration 404\n",
      "batches :404 0.2870049005614059\n",
      "Iteration 405: tag train_accuracy, simple_value 0.10098\n",
      "Iteration 405: tag train_loss, simple_value 0.287\n",
      "Timestamp 1612510477815, Iteration 405\n",
      "batches :405 0.2863935619316719\n",
      "Iteration 406: tag train_accuracy, simple_value 0.10108\n",
      "Iteration 406: tag train_loss, simple_value 0.28639\n",
      "Timestamp 1612510478121, Iteration 406\n",
      "batches :406 0.2857862655532184\n",
      "Iteration 407: tag train_accuracy, simple_value 0.10099\n",
      "Iteration 407: tag train_loss, simple_value 0.28579\n",
      "Timestamp 1612510478427, Iteration 407\n",
      "batches :407 0.28518306603083154\n",
      "Iteration 408: tag train_accuracy, simple_value 0.10085\n",
      "Iteration 408: tag train_loss, simple_value 0.28518\n",
      "Timestamp 1612510478734, Iteration 408\n",
      "batches :408 0.28458096285113227\n",
      "Iteration 409: tag train_accuracy, simple_value 0.10083\n",
      "Iteration 409: tag train_loss, simple_value 0.28458\n",
      "Timestamp 1612510479040, Iteration 409\n",
      "batches :409 0.28399056859296223\n",
      "Iteration 410: tag train_accuracy, simple_value 0.10082\n",
      "Iteration 410: tag train_loss, simple_value 0.28399\n",
      "Timestamp 1612510479347, Iteration 410\n",
      "batches :410 0.2833958538748869\n",
      "Iteration 411: tag train_accuracy, simple_value 0.10088\n",
      "Iteration 411: tag train_loss, simple_value 0.2834\n",
      "Timestamp 1612510479653, Iteration 411\n",
      "batches :411 0.28279602422476396\n",
      "Iteration 412: tag train_accuracy, simple_value 0.10094\n",
      "Iteration 412: tag train_loss, simple_value 0.2828\n",
      "Timestamp 1612510479961, Iteration 412\n",
      "batches :412 0.2822008500671358\n",
      "Iteration 413: tag train_accuracy, simple_value 0.10111\n",
      "Iteration 413: tag train_loss, simple_value 0.2822\n",
      "Timestamp 1612510480268, Iteration 413\n",
      "batches :413 0.28160816859499016\n",
      "Iteration 414: tag train_accuracy, simple_value 0.10105\n",
      "Iteration 414: tag train_loss, simple_value 0.28161\n",
      "Timestamp 1612510480573, Iteration 414\n",
      "batches :414 0.2810202429997892\n",
      "Iteration 415: tag train_accuracy, simple_value 0.10103\n",
      "Iteration 415: tag train_loss, simple_value 0.28102\n",
      "Timestamp 1612510480879, Iteration 415\n",
      "batches :415 0.28043086379766463\n",
      "Iteration 416: tag train_accuracy, simple_value 0.1009\n",
      "Iteration 416: tag train_loss, simple_value 0.28043\n",
      "Timestamp 1612510481200, Iteration 416\n",
      "batches :416 0.27984973250064427\n",
      "Iteration 417: tag train_accuracy, simple_value 0.10096\n",
      "Iteration 417: tag train_loss, simple_value 0.27985\n",
      "Timestamp 1612510481506, Iteration 417\n",
      "batches :417 0.27927224966083214\n",
      "Iteration 418: tag train_accuracy, simple_value 0.10087\n",
      "Iteration 418: tag train_loss, simple_value 0.27927\n",
      "Timestamp 1612510481812, Iteration 418\n",
      "batches :418 0.2786987190308611\n",
      "Iteration 419: tag train_accuracy, simple_value 0.10078\n",
      "Iteration 419: tag train_loss, simple_value 0.2787\n",
      "Timestamp 1612510482118, Iteration 419\n",
      "batches :419 0.2781295254115273\n",
      "Iteration 420: tag train_accuracy, simple_value 0.10065\n",
      "Iteration 420: tag train_loss, simple_value 0.27813\n",
      "Timestamp 1612510482425, Iteration 420\n",
      "batches :420 0.27756868155584447\n",
      "Iteration 421: tag train_accuracy, simple_value 0.10086\n",
      "Iteration 421: tag train_loss, simple_value 0.27757\n",
      "Timestamp 1612510482732, Iteration 421\n",
      "batches :421 0.27699846354027824\n",
      "Iteration 422: tag train_accuracy, simple_value 0.10091\n",
      "Iteration 422: tag train_loss, simple_value 0.277\n",
      "Timestamp 1612510483037, Iteration 422\n",
      "batches :422 0.27643266080997847\n",
      "Iteration 423: tag train_accuracy, simple_value 0.10093\n",
      "Iteration 423: tag train_loss, simple_value 0.27643\n",
      "Timestamp 1612510483344, Iteration 423\n",
      "batches :423 0.2758720075143567\n",
      "Iteration 424: tag train_accuracy, simple_value 0.10095\n",
      "Iteration 424: tag train_loss, simple_value 0.27587\n",
      "Timestamp 1612510483651, Iteration 424\n",
      "batches :424 0.2753101717417111\n",
      "Iteration 425: tag train_accuracy, simple_value 0.10112\n",
      "Iteration 425: tag train_loss, simple_value 0.27531\n",
      "Timestamp 1612510483959, Iteration 425\n",
      "batches :425 0.2747601101240691\n",
      "Iteration 426: tag train_accuracy, simple_value 0.10121\n",
      "Iteration 426: tag train_loss, simple_value 0.27476\n",
      "Timestamp 1612510484265, Iteration 426\n",
      "batches :426 0.2742128649752745\n",
      "Iteration 427: tag train_accuracy, simple_value 0.10116\n",
      "Iteration 427: tag train_loss, simple_value 0.27421\n",
      "Timestamp 1612510484572, Iteration 427\n",
      "batches :427 0.2736615285591443\n",
      "Iteration 428: tag train_accuracy, simple_value 0.10114\n",
      "Iteration 428: tag train_loss, simple_value 0.27366\n",
      "Timestamp 1612510484879, Iteration 428\n",
      "batches :428 0.27310944497411216\n",
      "Iteration 429: tag train_accuracy, simple_value 0.10112\n",
      "Iteration 429: tag train_loss, simple_value 0.27311\n",
      "Timestamp 1612510485184, Iteration 429\n",
      "batches :429 0.2725652792514899\n",
      "Iteration 430: tag train_accuracy, simple_value 0.10107\n",
      "Iteration 430: tag train_loss, simple_value 0.27257\n",
      "Timestamp 1612510485490, Iteration 430\n",
      "batches :430 0.2720218782248192\n",
      "Iteration 431: tag train_accuracy, simple_value 0.10094\n",
      "Iteration 431: tag train_loss, simple_value 0.27202\n",
      "Timestamp 1612510485799, Iteration 431\n",
      "batches :431 0.271479378443583\n",
      "Iteration 432: tag train_accuracy, simple_value 0.101\n",
      "Iteration 432: tag train_loss, simple_value 0.27148\n",
      "Timestamp 1612510486119, Iteration 432\n",
      "batches :432 0.2709578721766808\n",
      "Iteration 433: tag train_accuracy, simple_value 0.10098\n",
      "Iteration 433: tag train_loss, simple_value 0.27096\n",
      "Timestamp 1612510486425, Iteration 433\n",
      "batches :433 0.27042276340840044\n",
      "Iteration 434: tag train_accuracy, simple_value 0.101\n",
      "Iteration 434: tag train_loss, simple_value 0.27042\n",
      "Timestamp 1612510486731, Iteration 434\n",
      "batches :434 0.2698960059026282\n",
      "Iteration 435: tag train_accuracy, simple_value 0.10091\n",
      "Iteration 435: tag train_loss, simple_value 0.2699\n",
      "Timestamp 1612510487038, Iteration 435\n",
      "batches :435 0.26937696514972326\n",
      "Iteration 436: tag train_accuracy, simple_value 0.10079\n",
      "Iteration 436: tag train_loss, simple_value 0.26938\n",
      "Timestamp 1612510487344, Iteration 436\n",
      "batches :436 0.26884845247782696\n",
      "Iteration 437: tag train_accuracy, simple_value 0.10092\n",
      "Iteration 437: tag train_loss, simple_value 0.26885\n",
      "Timestamp 1612510487650, Iteration 437\n",
      "batches :437 0.2683258887457084\n",
      "Iteration 438: tag train_accuracy, simple_value 0.10076\n",
      "Iteration 438: tag train_loss, simple_value 0.26833\n",
      "Timestamp 1612510487957, Iteration 438\n",
      "batches :438 0.26781487100880985\n",
      "Iteration 439: tag train_accuracy, simple_value 0.10085\n",
      "Iteration 439: tag train_loss, simple_value 0.26781\n",
      "Timestamp 1612510488264, Iteration 439\n",
      "batches :439 0.26732016966629407\n",
      "Iteration 440: tag train_accuracy, simple_value 0.10094\n",
      "Iteration 440: tag train_loss, simple_value 0.26732\n",
      "Timestamp 1612510488570, Iteration 440\n",
      "batches :440 0.266827432108535\n",
      "Iteration 441: tag train_accuracy, simple_value 0.10103\n",
      "Iteration 441: tag train_loss, simple_value 0.26683\n",
      "Timestamp 1612510488877, Iteration 441\n",
      "batches :441 0.2663055675910984\n",
      "Iteration 442: tag train_accuracy, simple_value 0.10126\n",
      "Iteration 442: tag train_loss, simple_value 0.26631\n",
      "Timestamp 1612510489183, Iteration 442\n",
      "batches :442 0.2657932066337555\n",
      "Iteration 443: tag train_accuracy, simple_value 0.10142\n",
      "Iteration 443: tag train_loss, simple_value 0.26579\n",
      "Timestamp 1612510489490, Iteration 443\n",
      "batches :443 0.2652841004843636\n",
      "Iteration 444: tag train_accuracy, simple_value 0.10154\n",
      "Iteration 444: tag train_loss, simple_value 0.26528\n",
      "Timestamp 1612510489796, Iteration 444\n",
      "batches :444 0.264784544345562\n",
      "Iteration 445: tag train_accuracy, simple_value 0.10146\n",
      "Iteration 445: tag train_loss, simple_value 0.26478\n",
      "Timestamp 1612510490102, Iteration 445\n",
      "batches :445 0.2642757076895639\n",
      "Iteration 446: tag train_accuracy, simple_value 0.10144\n",
      "Iteration 446: tag train_loss, simple_value 0.26428\n",
      "Timestamp 1612510490409, Iteration 446\n",
      "batches :446 0.26376942933574654\n",
      "Iteration 447: tag train_accuracy, simple_value 0.10139\n",
      "Iteration 447: tag train_loss, simple_value 0.26377\n",
      "Timestamp 1612510490715, Iteration 447\n",
      "batches :447 0.2632762770558097\n",
      "Iteration 448: tag train_accuracy, simple_value 0.10134\n",
      "Iteration 448: tag train_loss, simple_value 0.26328\n",
      "Timestamp 1612510491036, Iteration 448\n",
      "batches :448 0.2627796552842483\n",
      "Iteration 449: tag train_accuracy, simple_value 0.10125\n",
      "Iteration 449: tag train_loss, simple_value 0.26278\n",
      "Timestamp 1612510491343, Iteration 449\n",
      "batches :449 0.26228689837097324\n",
      "Iteration 450: tag train_accuracy, simple_value 0.10113\n",
      "Iteration 450: tag train_loss, simple_value 0.26229\n",
      "Timestamp 1612510491650, Iteration 450\n",
      "batches :450 0.26179667156603603\n",
      "Iteration 451: tag train_accuracy, simple_value 0.10101\n",
      "Iteration 451: tag train_loss, simple_value 0.2618\n",
      "Timestamp 1612510491958, Iteration 451\n",
      "batches :451 0.2613100509041561\n",
      "Iteration 452: tag train_accuracy, simple_value 0.10103\n",
      "Iteration 452: tag train_loss, simple_value 0.26131\n",
      "Timestamp 1612510492266, Iteration 452\n",
      "batches :452 0.26081865108613156\n",
      "Iteration 453: tag train_accuracy, simple_value 0.10108\n",
      "Iteration 453: tag train_loss, simple_value 0.26082\n",
      "Timestamp 1612510492572, Iteration 453\n",
      "batches :453 0.260328532737283\n",
      "Iteration 454: tag train_accuracy, simple_value 0.10117\n",
      "Iteration 454: tag train_loss, simple_value 0.26033\n",
      "Timestamp 1612510492880, Iteration 454\n",
      "batches :454 0.25983885108669685\n",
      "Iteration 455: tag train_accuracy, simple_value 0.10118\n",
      "Iteration 455: tag train_loss, simple_value 0.25984\n",
      "Timestamp 1612510493186, Iteration 455\n",
      "batches :455 0.25935485545586756\n",
      "Iteration 456: tag train_accuracy, simple_value 0.10127\n",
      "Iteration 456: tag train_loss, simple_value 0.25935\n",
      "Timestamp 1612510493492, Iteration 456\n",
      "batches :456 0.2588794732695086\n",
      "Iteration 457: tag train_accuracy, simple_value 0.10122\n",
      "Iteration 457: tag train_loss, simple_value 0.25888\n",
      "Timestamp 1612510493798, Iteration 457\n",
      "batches :457 0.25840326382307827\n",
      "Iteration 458: tag train_accuracy, simple_value 0.1012\n",
      "Iteration 458: tag train_loss, simple_value 0.2584\n",
      "Timestamp 1612510494105, Iteration 458\n",
      "batches :458 0.257923713051531\n",
      "Iteration 459: tag train_accuracy, simple_value 0.10122\n",
      "Iteration 459: tag train_loss, simple_value 0.25792\n",
      "Timestamp 1612510494410, Iteration 459\n",
      "batches :459 0.25744569034585507\n",
      "Iteration 460: tag train_accuracy, simple_value 0.10124\n",
      "Iteration 460: tag train_loss, simple_value 0.25745\n",
      "Timestamp 1612510494717, Iteration 460\n",
      "batches :460 0.2569748879207865\n",
      "Iteration 461: tag train_accuracy, simple_value 0.10115\n",
      "Iteration 461: tag train_loss, simple_value 0.25697\n",
      "Timestamp 1612510495023, Iteration 461\n",
      "batches :461 0.2565027962392297\n",
      "Iteration 462: tag train_accuracy, simple_value 0.10131\n",
      "Iteration 462: tag train_loss, simple_value 0.2565\n",
      "Timestamp 1612510495329, Iteration 462\n",
      "batches :462 0.25603110810894986\n",
      "Iteration 463: tag train_accuracy, simple_value 0.10126\n",
      "Iteration 463: tag train_loss, simple_value 0.25603\n",
      "Timestamp 1612510495635, Iteration 463\n",
      "batches :463 0.2555596014689936\n",
      "Iteration 464: tag train_accuracy, simple_value 0.10131\n",
      "Iteration 464: tag train_loss, simple_value 0.25556\n",
      "Timestamp 1612510495954, Iteration 464\n",
      "batches :464 0.255099329398945\n",
      "Iteration 465: tag train_accuracy, simple_value 0.10126\n",
      "Iteration 465: tag train_loss, simple_value 0.2551\n",
      "Timestamp 1612510496259, Iteration 465\n",
      "batches :465 0.25463095002917835\n",
      "Iteration 466: tag train_accuracy, simple_value 0.10134\n",
      "Iteration 466: tag train_loss, simple_value 0.25463\n",
      "Timestamp 1612510496565, Iteration 466\n",
      "batches :466 0.2541628520248274\n",
      "Iteration 467: tag train_accuracy, simple_value 0.10139\n",
      "Iteration 467: tag train_loss, simple_value 0.25416\n",
      "Timestamp 1612510496872, Iteration 467\n",
      "batches :467 0.25370665060503833\n",
      "Iteration 468: tag train_accuracy, simple_value 0.10155\n",
      "Iteration 468: tag train_loss, simple_value 0.25371\n",
      "Timestamp 1612510497180, Iteration 468\n",
      "batches :468 0.2532480653598268\n",
      "Iteration 469: tag train_accuracy, simple_value 0.10156\n",
      "Iteration 469: tag train_loss, simple_value 0.25325\n",
      "Timestamp 1612510497488, Iteration 469\n",
      "batches :469 0.2527878183974767\n",
      "Iteration 470: tag train_accuracy, simple_value 0.10148\n",
      "Iteration 470: tag train_loss, simple_value 0.25279\n",
      "Timestamp 1612510497795, Iteration 470\n",
      "batches :470 0.25233055775628443\n",
      "Iteration 471: tag train_accuracy, simple_value 0.10163\n",
      "Iteration 471: tag train_loss, simple_value 0.25233\n",
      "Timestamp 1612510498102, Iteration 471\n",
      "batches :471 0.25187469110082666\n",
      "Iteration 472: tag train_accuracy, simple_value 0.10161\n",
      "Iteration 472: tag train_loss, simple_value 0.25187\n",
      "Timestamp 1612510498408, Iteration 472\n",
      "batches :472 0.251423959438768\n",
      "Iteration 473: tag train_accuracy, simple_value 0.10169\n",
      "Iteration 473: tag train_loss, simple_value 0.25142\n",
      "Timestamp 1612510498714, Iteration 473\n",
      "batches :473 0.2509732195205719\n",
      "Iteration 474: tag train_accuracy, simple_value 0.10178\n",
      "Iteration 474: tag train_loss, simple_value 0.25097\n",
      "Timestamp 1612510499020, Iteration 474\n",
      "batches :474 0.2505260808747026\n",
      "Iteration 475: tag train_accuracy, simple_value 0.10189\n",
      "Iteration 475: tag train_loss, simple_value 0.25053\n",
      "Timestamp 1612510499327, Iteration 475\n",
      "batches :475 0.2500835921105586\n",
      "Iteration 476: tag train_accuracy, simple_value 0.10187\n",
      "Iteration 476: tag train_loss, simple_value 0.25008\n",
      "Timestamp 1612510499634, Iteration 476\n",
      "batches :476 0.24964138808339334\n",
      "Iteration 477: tag train_accuracy, simple_value 0.10186\n",
      "Iteration 477: tag train_loss, simple_value 0.24964\n",
      "Timestamp 1612510499940, Iteration 477\n",
      "batches :477 0.2492032543730686\n",
      "Iteration 478: tag train_accuracy, simple_value 0.10187\n",
      "Iteration 478: tag train_loss, simple_value 0.2492\n",
      "Timestamp 1612510500249, Iteration 478\n",
      "batches :478 0.24876677469354294\n",
      "Iteration 479: tag train_accuracy, simple_value 0.10179\n",
      "Iteration 479: tag train_loss, simple_value 0.24877\n",
      "Timestamp 1612510500557, Iteration 479\n",
      "batches :479 0.2483329934326617\n",
      "Iteration 480: tag train_accuracy, simple_value 0.10168\n",
      "Iteration 480: tag train_loss, simple_value 0.24833\n",
      "Timestamp 1612510500879, Iteration 480\n",
      "batches :480 0.24789468540499607\n",
      "Iteration 481: tag train_accuracy, simple_value 0.10176\n",
      "Iteration 481: tag train_loss, simple_value 0.24789\n",
      "Timestamp 1612510501185, Iteration 481\n",
      "batches :481 0.24745464697480202\n",
      "Iteration 482: tag train_accuracy, simple_value 0.10197\n",
      "Iteration 482: tag train_loss, simple_value 0.24745\n",
      "Timestamp 1612510501491, Iteration 482\n",
      "batches :482 0.24702571836373619\n",
      "Iteration 483: tag train_accuracy, simple_value 0.10202\n",
      "Iteration 483: tag train_loss, simple_value 0.24703\n",
      "Timestamp 1612510501797, Iteration 483\n",
      "batches :483 0.2466034699331653\n",
      "Iteration 484: tag train_accuracy, simple_value 0.1019\n",
      "Iteration 484: tag train_loss, simple_value 0.2466\n",
      "Timestamp 1612510502103, Iteration 484\n",
      "batches :484 0.24617496273703565\n",
      "Iteration 485: tag train_accuracy, simple_value 0.10189\n",
      "Iteration 485: tag train_loss, simple_value 0.24617\n",
      "Timestamp 1612510502411, Iteration 485\n",
      "batches :485 0.24574580430677256\n",
      "Iteration 486: tag train_accuracy, simple_value 0.10197\n",
      "Iteration 486: tag train_loss, simple_value 0.24575\n",
      "Timestamp 1612510502717, Iteration 486\n",
      "batches :486 0.2453193341079072\n",
      "Iteration 487: tag train_accuracy, simple_value 0.10192\n",
      "Iteration 487: tag train_loss, simple_value 0.24532\n",
      "Timestamp 1612510503024, Iteration 487\n",
      "batches :487 0.24489428612424607\n",
      "Iteration 488: tag train_accuracy, simple_value 0.10193\n",
      "Iteration 488: tag train_loss, simple_value 0.24489\n",
      "Timestamp 1612510503335, Iteration 488\n",
      "batches :488 0.24446827998445902\n",
      "Iteration 489: tag train_accuracy, simple_value 0.10201\n",
      "Iteration 489: tag train_loss, simple_value 0.24447\n",
      "Timestamp 1612510503643, Iteration 489\n",
      "batches :489 0.24405025012775922\n",
      "Iteration 490: tag train_accuracy, simple_value 0.10196\n",
      "Iteration 490: tag train_loss, simple_value 0.24405\n",
      "Timestamp 1612510503951, Iteration 490\n",
      "batches :490 0.24363027405069798\n",
      "Iteration 491: tag train_accuracy, simple_value 0.10188\n",
      "Iteration 491: tag train_loss, simple_value 0.24363\n",
      "Timestamp 1612510504258, Iteration 491\n",
      "batches :491 0.2432121555121756\n",
      "Iteration 492: tag train_accuracy, simple_value 0.10186\n",
      "Iteration 492: tag train_loss, simple_value 0.24321\n",
      "Timestamp 1612510504566, Iteration 492\n",
      "batches :492 0.24279777811673597\n",
      "Iteration 493: tag train_accuracy, simple_value 0.10191\n",
      "Iteration 493: tag train_loss, simple_value 0.2428\n",
      "Timestamp 1612510504874, Iteration 493\n",
      "batches :493 0.24238312205877796\n",
      "Iteration 494: tag train_accuracy, simple_value 0.10186\n",
      "Iteration 494: tag train_loss, simple_value 0.24238\n",
      "Timestamp 1612510505179, Iteration 494\n",
      "batches :494 0.24196941181564863\n",
      "Iteration 495: tag train_accuracy, simple_value 0.10178\n",
      "Iteration 495: tag train_loss, simple_value 0.24197\n",
      "Timestamp 1612510505485, Iteration 495\n",
      "batches :495 0.2415588865526999\n",
      "Iteration 496: tag train_accuracy, simple_value 0.10186\n",
      "Iteration 496: tag train_loss, simple_value 0.24156\n",
      "Timestamp 1612510505806, Iteration 496\n",
      "batches :496 0.24114903616118094\n",
      "Iteration 497: tag train_accuracy, simple_value 0.10188\n",
      "Iteration 497: tag train_loss, simple_value 0.24115\n",
      "Timestamp 1612510506112, Iteration 497\n",
      "batches :497 0.24074055886334578\n",
      "Iteration 498: tag train_accuracy, simple_value 0.1018\n",
      "Iteration 498: tag train_loss, simple_value 0.24074\n",
      "Timestamp 1612510506418, Iteration 498\n",
      "batches :498 0.2403338837874941\n",
      "Iteration 499: tag train_accuracy, simple_value 0.10191\n",
      "Iteration 499: tag train_loss, simple_value 0.24033\n",
      "Timestamp 1612510506723, Iteration 499\n",
      "batches :499 0.23993014626369208\n",
      "Iteration 500: tag train_accuracy, simple_value 0.10186\n",
      "Iteration 500: tag train_loss, simple_value 0.23993\n",
      "Timestamp 1612510507029, Iteration 500\n",
      "batches :500 0.23952666792273522\n",
      "Iteration 501: tag train_accuracy, simple_value 0.10184\n",
      "Iteration 501: tag train_loss, simple_value 0.23953\n",
      "Timestamp 1612510507336, Iteration 501\n",
      "batches :501 0.2391309745743603\n",
      "Iteration 502: tag train_accuracy, simple_value 0.10189\n",
      "Iteration 502: tag train_loss, simple_value 0.23913\n",
      "Timestamp 1612510507642, Iteration 502\n",
      "batches :502 0.2387323273917237\n",
      "Iteration 503: tag train_accuracy, simple_value 0.1019\n",
      "Iteration 503: tag train_loss, simple_value 0.23873\n",
      "Timestamp 1612510507947, Iteration 503\n",
      "batches :503 0.23833333032861145\n",
      "Iteration 504: tag train_accuracy, simple_value 0.10189\n",
      "Iteration 504: tag train_loss, simple_value 0.23833\n",
      "Timestamp 1612510508252, Iteration 504\n",
      "batches :504 0.23793462273620425\n",
      "Iteration 505: tag train_accuracy, simple_value 0.10187\n",
      "Iteration 505: tag train_loss, simple_value 0.23793\n",
      "Timestamp 1612510508559, Iteration 505\n",
      "batches :505 0.23753840833312215\n",
      "Iteration 506: tag train_accuracy, simple_value 0.10195\n",
      "Iteration 506: tag train_loss, simple_value 0.23754\n",
      "Timestamp 1612510508866, Iteration 506\n",
      "batches :506 0.23714637723217605\n",
      "Iteration 507: tag train_accuracy, simple_value 0.1019\n",
      "Iteration 507: tag train_loss, simple_value 0.23715\n",
      "Timestamp 1612510509172, Iteration 507\n",
      "batches :507 0.2367546121264703\n",
      "Iteration 508: tag train_accuracy, simple_value 0.10186\n",
      "Iteration 508: tag train_loss, simple_value 0.23675\n",
      "Timestamp 1612510509479, Iteration 508\n",
      "batches :508 0.2363651640067889\n",
      "Iteration 509: tag train_accuracy, simple_value 0.10184\n",
      "Iteration 509: tag train_loss, simple_value 0.23637\n",
      "Timestamp 1612510509786, Iteration 509\n",
      "batches :509 0.23597635599566819\n",
      "Iteration 510: tag train_accuracy, simple_value 0.10173\n",
      "Iteration 510: tag train_loss, simple_value 0.23598\n",
      "Timestamp 1612510510093, Iteration 510\n",
      "batches :510 0.23559050523475106\n",
      "Iteration 511: tag train_accuracy, simple_value 0.10172\n",
      "Iteration 511: tag train_loss, simple_value 0.23559\n",
      "Timestamp 1612510510400, Iteration 511\n",
      "batches :511 0.23520579551384874\n",
      "Iteration 512: tag train_accuracy, simple_value 0.10167\n",
      "Iteration 512: tag train_loss, simple_value 0.23521\n",
      "Timestamp 1612510510721, Iteration 512\n",
      "batches :512 0.2348215533493203\n",
      "Iteration 513: tag train_accuracy, simple_value 0.10172\n",
      "Iteration 513: tag train_loss, simple_value 0.23482\n",
      "Timestamp 1612510511027, Iteration 513\n",
      "batches :513 0.2344392020233542\n",
      "Iteration 514: tag train_accuracy, simple_value 0.1017\n",
      "Iteration 514: tag train_loss, simple_value 0.23444\n",
      "Timestamp 1612510511333, Iteration 514\n",
      "batches :514 0.2340561627747368\n",
      "Iteration 515: tag train_accuracy, simple_value 0.10168\n",
      "Iteration 515: tag train_loss, simple_value 0.23406\n",
      "Timestamp 1612510511639, Iteration 515\n",
      "batches :515 0.23367757839919295\n",
      "Iteration 516: tag train_accuracy, simple_value 0.1017\n",
      "Iteration 516: tag train_loss, simple_value 0.23368\n",
      "Timestamp 1612510511945, Iteration 516\n",
      "batches :516 0.23329725400941778\n",
      "Iteration 517: tag train_accuracy, simple_value 0.10187\n",
      "Iteration 517: tag train_loss, simple_value 0.2333\n",
      "Timestamp 1612510512251, Iteration 517\n",
      "batches :517 0.23291968558581702\n",
      "Iteration 518: tag train_accuracy, simple_value 0.10188\n",
      "Iteration 518: tag train_loss, simple_value 0.23292\n",
      "Timestamp 1612510512559, Iteration 518\n",
      "batches :518 0.23254356706608448\n",
      "Iteration 519: tag train_accuracy, simple_value 0.10186\n",
      "Iteration 519: tag train_loss, simple_value 0.23254\n",
      "Timestamp 1612510512866, Iteration 519\n",
      "batches :519 0.23217058879447122\n",
      "Iteration 520: tag train_accuracy, simple_value 0.10179\n",
      "Iteration 520: tag train_loss, simple_value 0.23217\n",
      "Timestamp 1612510513172, Iteration 520\n",
      "batches :520 0.23179658071782727\n",
      "Iteration 521: tag train_accuracy, simple_value 0.10177\n",
      "Iteration 521: tag train_loss, simple_value 0.2318\n",
      "Timestamp 1612510513478, Iteration 521\n",
      "batches :521 0.23142481239909402\n",
      "Iteration 522: tag train_accuracy, simple_value 0.1017\n",
      "Iteration 522: tag train_loss, simple_value 0.23142\n",
      "Timestamp 1612510513785, Iteration 522\n",
      "batches :522 0.23105463481212027\n",
      "Iteration 523: tag train_accuracy, simple_value 0.10165\n",
      "Iteration 523: tag train_loss, simple_value 0.23105\n",
      "Timestamp 1612510514094, Iteration 523\n",
      "batches :523 0.23068570196058735\n",
      "Iteration 524: tag train_accuracy, simple_value 0.10158\n",
      "Iteration 524: tag train_loss, simple_value 0.23069\n",
      "Timestamp 1612510514400, Iteration 524\n",
      "batches :524 0.23031808816977128\n",
      "Iteration 525: tag train_accuracy, simple_value 0.10156\n",
      "Iteration 525: tag train_loss, simple_value 0.23032\n",
      "Timestamp 1612510514708, Iteration 525\n",
      "batches :525 0.2299508782440708\n",
      "Iteration 526: tag train_accuracy, simple_value 0.10164\n",
      "Iteration 526: tag train_loss, simple_value 0.22995\n",
      "Timestamp 1612510515014, Iteration 526\n",
      "batches :526 0.2295853657853354\n",
      "Iteration 527: tag train_accuracy, simple_value 0.10162\n",
      "Iteration 527: tag train_loss, simple_value 0.22959\n",
      "Timestamp 1612510515319, Iteration 527\n",
      "batches :527 0.22922215110974023\n",
      "Iteration 528: tag train_accuracy, simple_value 0.10167\n",
      "Iteration 528: tag train_loss, simple_value 0.22922\n",
      "Timestamp 1612510515640, Iteration 528\n",
      "batches :528 0.22885997785311757\n",
      "Iteration 529: tag train_accuracy, simple_value 0.10174\n",
      "Iteration 529: tag train_loss, simple_value 0.22886\n",
      "Timestamp 1612510515946, Iteration 529\n",
      "batches :529 0.22849931860860218\n",
      "Iteration 530: tag train_accuracy, simple_value 0.10175\n",
      "Iteration 530: tag train_loss, simple_value 0.2285\n",
      "Timestamp 1612510516251, Iteration 530\n",
      "batches :530 0.2281379874865964\n",
      "Iteration 531: tag train_accuracy, simple_value 0.10186\n",
      "Iteration 531: tag train_loss, simple_value 0.22814\n",
      "Timestamp 1612510516558, Iteration 531\n",
      "batches :531 0.22777938784543852\n",
      "Iteration 532: tag train_accuracy, simple_value 0.10187\n",
      "Iteration 532: tag train_loss, simple_value 0.22778\n",
      "Timestamp 1612510516865, Iteration 532\n",
      "batches :532 0.22742160872549266\n",
      "Iteration 533: tag train_accuracy, simple_value 0.10183\n",
      "Iteration 533: tag train_loss, simple_value 0.22742\n",
      "Timestamp 1612510517172, Iteration 533\n",
      "batches :533 0.22706817963473344\n",
      "Iteration 534: tag train_accuracy, simple_value 0.10184\n",
      "Iteration 534: tag train_loss, simple_value 0.22707\n",
      "Timestamp 1612510517480, Iteration 534\n",
      "batches :534 0.22671360189046305\n",
      "Iteration 535: tag train_accuracy, simple_value 0.10183\n",
      "Iteration 535: tag train_loss, simple_value 0.22671\n",
      "Timestamp 1612510517786, Iteration 535\n",
      "batches :535 0.22635824658046258\n",
      "Iteration 536: tag train_accuracy, simple_value 0.10196\n",
      "Iteration 536: tag train_loss, simple_value 0.22636\n",
      "Timestamp 1612510518092, Iteration 536\n",
      "batches :536 0.22600767315963088\n",
      "Iteration 537: tag train_accuracy, simple_value 0.10197\n",
      "Iteration 537: tag train_loss, simple_value 0.22601\n",
      "Timestamp 1612510518399, Iteration 537\n",
      "batches :537 0.22565747097523076\n",
      "Iteration 538: tag train_accuracy, simple_value 0.10193\n",
      "Iteration 538: tag train_loss, simple_value 0.22566\n",
      "Timestamp 1612510518705, Iteration 538\n",
      "batches :538 0.22530753350363122\n",
      "Iteration 539: tag train_accuracy, simple_value 0.10191\n",
      "Iteration 539: tag train_loss, simple_value 0.22531\n",
      "Timestamp 1612510519012, Iteration 539\n",
      "batches :539 0.22495950372105183\n",
      "Iteration 540: tag train_accuracy, simple_value 0.10181\n",
      "Iteration 540: tag train_loss, simple_value 0.22496\n",
      "Timestamp 1612510519319, Iteration 540\n",
      "batches :540 0.22461598778350486\n",
      "Iteration 541: tag train_accuracy, simple_value 0.10171\n",
      "Iteration 541: tag train_loss, simple_value 0.22462\n",
      "Timestamp 1612510519627, Iteration 541\n",
      "batches :541 0.22427393657167385\n",
      "Iteration 542: tag train_accuracy, simple_value 0.10161\n",
      "Iteration 542: tag train_loss, simple_value 0.22427\n",
      "Timestamp 1612510519933, Iteration 542\n",
      "batches :542 0.2239307359795729\n",
      "Iteration 543: tag train_accuracy, simple_value 0.10153\n",
      "Iteration 543: tag train_loss, simple_value 0.22393\n",
      "Timestamp 1612510520239, Iteration 543\n",
      "batches :543 0.22358871224588453\n",
      "Iteration 544: tag train_accuracy, simple_value 0.10155\n",
      "Iteration 544: tag train_loss, simple_value 0.22359\n",
      "Timestamp 1612510520562, Iteration 544\n",
      "batches :544 0.2232477103209342\n",
      "Iteration 545: tag train_accuracy, simple_value 0.10145\n",
      "Iteration 545: tag train_loss, simple_value 0.22325\n",
      "Timestamp 1612510520868, Iteration 545\n",
      "batches :545 0.2229056172756427\n",
      "Iteration 546: tag train_accuracy, simple_value 0.10143\n",
      "Iteration 546: tag train_loss, simple_value 0.22291\n",
      "Timestamp 1612510521174, Iteration 546\n",
      "batches :546 0.22256407568797523\n",
      "Iteration 547: tag train_accuracy, simple_value 0.10145\n",
      "Iteration 547: tag train_loss, simple_value 0.22256\n",
      "Timestamp 1612510521480, Iteration 547\n",
      "batches :547 0.22222468755471858\n",
      "Iteration 548: tag train_accuracy, simple_value 0.10152\n",
      "Iteration 548: tag train_loss, simple_value 0.22222\n",
      "Timestamp 1612510521786, Iteration 548\n",
      "batches :548 0.22188599169743758\n",
      "Iteration 549: tag train_accuracy, simple_value 0.10156\n",
      "Iteration 549: tag train_loss, simple_value 0.22189\n",
      "Timestamp 1612510522092, Iteration 549\n",
      "batches :549 0.2215502626773221\n",
      "Iteration 550: tag train_accuracy, simple_value 0.10161\n",
      "Iteration 550: tag train_loss, simple_value 0.22155\n",
      "Timestamp 1612510522398, Iteration 550\n",
      "batches :550 0.2212145128304308\n",
      "Iteration 551: tag train_accuracy, simple_value 0.10162\n",
      "Iteration 551: tag train_loss, simple_value 0.22121\n",
      "Timestamp 1612510522704, Iteration 551\n",
      "batches :551 0.22088386572748694\n",
      "Iteration 552: tag train_accuracy, simple_value 0.10158\n",
      "Iteration 552: tag train_loss, simple_value 0.22088\n",
      "Timestamp 1612510523010, Iteration 552\n",
      "batches :552 0.22054932142972297\n",
      "Iteration 553: tag train_accuracy, simple_value 0.10168\n",
      "Iteration 553: tag train_loss, simple_value 0.22055\n",
      "Timestamp 1612510523316, Iteration 553\n",
      "batches :553 0.22021858132095706\n",
      "Iteration 554: tag train_accuracy, simple_value 0.10169\n",
      "Iteration 554: tag train_loss, simple_value 0.22022\n",
      "Timestamp 1612510523624, Iteration 554\n",
      "batches :554 0.21988822190958454\n",
      "Iteration 555: tag train_accuracy, simple_value 0.10165\n",
      "Iteration 555: tag train_loss, simple_value 0.21989\n",
      "Timestamp 1612510523931, Iteration 555\n",
      "batches :555 0.21955741573427173\n",
      "Iteration 556: tag train_accuracy, simple_value 0.10169\n",
      "Iteration 556: tag train_loss, simple_value 0.21956\n",
      "Timestamp 1612510524239, Iteration 556\n",
      "batches :556 0.21922837784044819\n",
      "Iteration 557: tag train_accuracy, simple_value 0.1017\n",
      "Iteration 557: tag train_loss, simple_value 0.21923\n",
      "Timestamp 1612510524545, Iteration 557\n",
      "batches :557 0.21889969705866527\n",
      "Iteration 558: tag train_accuracy, simple_value 0.10166\n",
      "Iteration 558: tag train_loss, simple_value 0.2189\n",
      "Timestamp 1612510524851, Iteration 558\n",
      "batches :558 0.21857570398825898\n",
      "Iteration 559: tag train_accuracy, simple_value 0.1017\n",
      "Iteration 559: tag train_loss, simple_value 0.21858\n",
      "Timestamp 1612510525157, Iteration 559\n",
      "batches :559 0.21824866726541348\n",
      "Iteration 560: tag train_accuracy, simple_value 0.10183\n",
      "Iteration 560: tag train_loss, simple_value 0.21825\n",
      "Timestamp 1612510525477, Iteration 560\n",
      "batches :560 0.21792330848319189\n",
      "Iteration 561: tag train_accuracy, simple_value 0.10193\n",
      "Iteration 561: tag train_loss, simple_value 0.21792\n",
      "Timestamp 1612510525783, Iteration 561\n",
      "batches :561 0.21759930764343224\n",
      "Iteration 562: tag train_accuracy, simple_value 0.10208\n",
      "Iteration 562: tag train_loss, simple_value 0.2176\n",
      "Timestamp 1612510526089, Iteration 562\n",
      "batches :562 0.2172782664117652\n",
      "Iteration 563: tag train_accuracy, simple_value 0.10209\n",
      "Iteration 563: tag train_loss, simple_value 0.21728\n",
      "Timestamp 1612510526396, Iteration 563\n",
      "batches :563 0.216957177417634\n",
      "Iteration 564: tag train_accuracy, simple_value 0.10227\n",
      "Iteration 564: tag train_loss, simple_value 0.21696\n",
      "Timestamp 1612510526702, Iteration 564\n",
      "batches :564 0.21663887568927825\n",
      "Iteration 565: tag train_accuracy, simple_value 0.10237\n",
      "Iteration 565: tag train_loss, simple_value 0.21664\n",
      "Timestamp 1612510527008, Iteration 565\n",
      "batches :565 0.21632149992263422\n",
      "Iteration 566: tag train_accuracy, simple_value 0.10241\n",
      "Iteration 566: tag train_loss, simple_value 0.21632\n",
      "Timestamp 1612510527315, Iteration 566\n",
      "batches :566 0.21600648202660228\n",
      "Iteration 567: tag train_accuracy, simple_value 0.10242\n",
      "Iteration 567: tag train_loss, simple_value 0.21601\n",
      "Timestamp 1612510527621, Iteration 567\n",
      "batches :567 0.2156901257326153\n",
      "Iteration 568: tag train_accuracy, simple_value 0.10238\n",
      "Iteration 568: tag train_loss, simple_value 0.21569\n",
      "Timestamp 1612510527927, Iteration 568\n",
      "batches :568 0.21537442711299995\n",
      "Iteration 569: tag train_accuracy, simple_value 0.10242\n",
      "Iteration 569: tag train_loss, simple_value 0.21537\n",
      "Timestamp 1612510528234, Iteration 569\n",
      "batches :569 0.21505859088944845\n",
      "Iteration 570: tag train_accuracy, simple_value 0.10248\n",
      "Iteration 570: tag train_loss, simple_value 0.21506\n",
      "Timestamp 1612510528542, Iteration 570\n",
      "batches :570 0.21474520064106112\n",
      "Iteration 571: tag train_accuracy, simple_value 0.10255\n",
      "Iteration 571: tag train_loss, simple_value 0.21475\n",
      "Timestamp 1612510528850, Iteration 571\n",
      "batches :571 0.21443245599728122\n",
      "Iteration 572: tag train_accuracy, simple_value 0.10259\n",
      "Iteration 572: tag train_loss, simple_value 0.21443\n",
      "Timestamp 1612510529158, Iteration 572\n",
      "batches :572 0.2141237184275072\n",
      "Iteration 573: tag train_accuracy, simple_value 0.10255\n",
      "Iteration 573: tag train_loss, simple_value 0.21412\n",
      "Timestamp 1612510529465, Iteration 573\n",
      "batches :573 0.21381536276968777\n",
      "Iteration 574: tag train_accuracy, simple_value 0.10259\n",
      "Iteration 574: tag train_loss, simple_value 0.21382\n",
      "Timestamp 1612510529770, Iteration 574\n",
      "batches :574 0.21350673440103954\n",
      "Iteration 575: tag train_accuracy, simple_value 0.1026\n",
      "Iteration 575: tag train_loss, simple_value 0.21351\n",
      "Timestamp 1612510530079, Iteration 575\n",
      "batches :575 0.21319950629187667\n",
      "Iteration 576: tag train_accuracy, simple_value 0.10264\n",
      "Iteration 576: tag train_loss, simple_value 0.2132\n",
      "Timestamp 1612510530400, Iteration 576\n",
      "batches :576 0.21289452228746894\n",
      "Iteration 577: tag train_accuracy, simple_value 0.10267\n",
      "Iteration 577: tag train_loss, simple_value 0.21289\n",
      "Timestamp 1612510530708, Iteration 577\n",
      "batches :577 0.212591608369681\n",
      "Iteration 578: tag train_accuracy, simple_value 0.10261\n",
      "Iteration 578: tag train_loss, simple_value 0.21259\n",
      "Timestamp 1612510531015, Iteration 578\n",
      "batches :578 0.2122912746064597\n",
      "Iteration 579: tag train_accuracy, simple_value 0.10254\n",
      "Iteration 579: tag train_loss, simple_value 0.21229\n",
      "Timestamp 1612510531321, Iteration 579\n",
      "batches :579 0.21199355456521268\n",
      "Iteration 580: tag train_accuracy, simple_value 0.10257\n",
      "Iteration 580: tag train_loss, simple_value 0.21199\n",
      "Timestamp 1612510531628, Iteration 580\n",
      "batches :580 0.21169281827350114\n",
      "Iteration 581: tag train_accuracy, simple_value 0.10256\n",
      "Iteration 581: tag train_loss, simple_value 0.21169\n",
      "Timestamp 1612510531935, Iteration 581\n",
      "batches :581 0.21139388830239103\n",
      "Iteration 582: tag train_accuracy, simple_value 0.10257\n",
      "Iteration 582: tag train_loss, simple_value 0.21139\n",
      "Timestamp 1612510532242, Iteration 582\n",
      "batches :582 0.21109292964063764\n",
      "Iteration 583: tag train_accuracy, simple_value 0.10266\n",
      "Iteration 583: tag train_loss, simple_value 0.21109\n",
      "Timestamp 1612510532548, Iteration 583\n",
      "batches :583 0.21079384690185762\n",
      "Iteration 584: tag train_accuracy, simple_value 0.1027\n",
      "Iteration 584: tag train_loss, simple_value 0.21079\n",
      "Timestamp 1612510532856, Iteration 584\n",
      "batches :584 0.21049568319233924\n",
      "Iteration 585: tag train_accuracy, simple_value 0.10271\n",
      "Iteration 585: tag train_loss, simple_value 0.2105\n",
      "Timestamp 1612510533162, Iteration 585\n",
      "batches :585 0.2101972114581328\n",
      "Iteration 586: tag train_accuracy, simple_value 0.10275\n",
      "Iteration 586: tag train_loss, simple_value 0.2102\n",
      "Timestamp 1612510533467, Iteration 586\n",
      "batches :586 0.20990031653118826\n",
      "Iteration 587: tag train_accuracy, simple_value 0.10279\n",
      "Iteration 587: tag train_loss, simple_value 0.2099\n",
      "Timestamp 1612510533776, Iteration 587\n",
      "batches :587 0.20960443508152865\n",
      "Iteration 588: tag train_accuracy, simple_value 0.10291\n",
      "Iteration 588: tag train_loss, simple_value 0.2096\n",
      "Timestamp 1612510534081, Iteration 588\n",
      "batches :588 0.2093108609052641\n",
      "Iteration 589: tag train_accuracy, simple_value 0.103\n",
      "Iteration 589: tag train_loss, simple_value 0.20931\n",
      "Timestamp 1612510534388, Iteration 589\n",
      "batches :589 0.20901701870692202\n",
      "Iteration 590: tag train_accuracy, simple_value 0.10309\n",
      "Iteration 590: tag train_loss, simple_value 0.20902\n",
      "Timestamp 1612510534694, Iteration 590\n",
      "batches :590 0.20872361846268178\n",
      "Iteration 591: tag train_accuracy, simple_value 0.10312\n",
      "Iteration 591: tag train_loss, simple_value 0.20872\n",
      "Timestamp 1612510535001, Iteration 591\n",
      "batches :591 0.20843296213778345\n",
      "Iteration 592: tag train_accuracy, simple_value 0.10321\n",
      "Iteration 592: tag train_loss, simple_value 0.20843\n",
      "Timestamp 1612510535322, Iteration 592\n",
      "batches :592 0.20814403615313004\n",
      "Iteration 593: tag train_accuracy, simple_value 0.1032\n",
      "Iteration 593: tag train_loss, simple_value 0.20814\n",
      "Timestamp 1612510535629, Iteration 593\n",
      "batches :593 0.20785802131609812\n",
      "Iteration 594: tag train_accuracy, simple_value 0.10316\n",
      "Iteration 594: tag train_loss, simple_value 0.20786\n",
      "Timestamp 1612510535936, Iteration 594\n",
      "batches :594 0.20757219182195688\n",
      "Iteration 595: tag train_accuracy, simple_value 0.10322\n",
      "Iteration 595: tag train_loss, simple_value 0.20757\n",
      "Timestamp 1612510536244, Iteration 595\n",
      "batches :595 0.20728494640158004\n",
      "Iteration 596: tag train_accuracy, simple_value 0.10323\n",
      "Iteration 596: tag train_loss, simple_value 0.20728\n",
      "Timestamp 1612510536550, Iteration 596\n",
      "batches :596 0.20699914887972526\n",
      "Iteration 597: tag train_accuracy, simple_value 0.10324\n",
      "Iteration 597: tag train_loss, simple_value 0.207\n",
      "Timestamp 1612510536859, Iteration 597\n",
      "batches :597 0.20671445046437645\n",
      "Iteration 598: tag train_accuracy, simple_value 0.10328\n",
      "Iteration 598: tag train_loss, simple_value 0.20671\n",
      "Timestamp 1612510537165, Iteration 598\n",
      "batches :598 0.2064281039400942\n",
      "Iteration 599: tag train_accuracy, simple_value 0.10347\n",
      "Iteration 599: tag train_loss, simple_value 0.20643\n",
      "Timestamp 1612510537471, Iteration 599\n",
      "batches :599 0.2061461529294915\n",
      "Iteration 600: tag train_accuracy, simple_value 0.10348\n",
      "Iteration 600: tag train_loss, simple_value 0.20615\n",
      "Timestamp 1612510537778, Iteration 600\n",
      "batches :600 0.20586669327070314\n",
      "Iteration 601: tag train_accuracy, simple_value 0.10336\n",
      "Iteration 601: tag train_loss, simple_value 0.20587\n",
      "Timestamp 1612510538086, Iteration 601\n",
      "batches :601 0.2055837798386366\n",
      "Iteration 602: tag train_accuracy, simple_value 0.10347\n",
      "Iteration 602: tag train_loss, simple_value 0.20558\n",
      "Timestamp 1612510538391, Iteration 602\n",
      "batches :602 0.20530367505263253\n",
      "Iteration 603: tag train_accuracy, simple_value 0.10343\n",
      "Iteration 603: tag train_loss, simple_value 0.2053\n",
      "Timestamp 1612510538697, Iteration 603\n",
      "batches :603 0.20502578594395968\n",
      "Iteration 604: tag train_accuracy, simple_value 0.10334\n",
      "Iteration 604: tag train_loss, simple_value 0.20503\n",
      "Timestamp 1612510539005, Iteration 604\n",
      "batches :604 0.2047468303079834\n",
      "Iteration 605: tag train_accuracy, simple_value 0.10337\n",
      "Iteration 605: tag train_loss, simple_value 0.20475\n",
      "Timestamp 1612510539313, Iteration 605\n",
      "batches :605 0.20446966050331258\n",
      "Iteration 606: tag train_accuracy, simple_value 0.10333\n",
      "Iteration 606: tag train_loss, simple_value 0.20447\n",
      "Timestamp 1612510539622, Iteration 606\n",
      "batches :606 0.20419403155743093\n",
      "Iteration 607: tag train_accuracy, simple_value 0.10332\n",
      "Iteration 607: tag train_loss, simple_value 0.20419\n",
      "Timestamp 1612510539930, Iteration 607\n",
      "batches :607 0.20391608121662202\n",
      "Iteration 608: tag train_accuracy, simple_value 0.10351\n",
      "Iteration 608: tag train_loss, simple_value 0.20392\n",
      "Timestamp 1612510540250, Iteration 608\n",
      "batches :608 0.20364279216032868\n",
      "Iteration 609: tag train_accuracy, simple_value 0.10341\n",
      "Iteration 609: tag train_loss, simple_value 0.20364\n",
      "Timestamp 1612510540558, Iteration 609\n",
      "batches :609 0.20337119107616358\n",
      "Iteration 610: tag train_accuracy, simple_value 0.10345\n",
      "Iteration 610: tag train_loss, simple_value 0.20337\n",
      "Timestamp 1612510540867, Iteration 610\n",
      "batches :610 0.2030982556951339\n",
      "Iteration 611: tag train_accuracy, simple_value 0.10343\n",
      "Iteration 611: tag train_loss, simple_value 0.2031\n",
      "Timestamp 1612510541174, Iteration 611\n",
      "batches :611 0.20282409167597024\n",
      "Iteration 612: tag train_accuracy, simple_value 0.1036\n",
      "Iteration 612: tag train_loss, simple_value 0.20282\n",
      "Timestamp 1612510541481, Iteration 612\n",
      "batches :612 0.20255321897322837\n",
      "Iteration 613: tag train_accuracy, simple_value 0.10355\n",
      "Iteration 613: tag train_loss, simple_value 0.20255\n",
      "Timestamp 1612510541788, Iteration 613\n",
      "batches :613 0.20228027114204053\n",
      "Iteration 614: tag train_accuracy, simple_value 0.10364\n",
      "Iteration 614: tag train_loss, simple_value 0.20228\n",
      "Timestamp 1612510542094, Iteration 614\n",
      "batches :614 0.20200974113802375\n",
      "Iteration 615: tag train_accuracy, simple_value 0.10383\n",
      "Iteration 615: tag train_loss, simple_value 0.20201\n",
      "Timestamp 1612510542401, Iteration 615\n",
      "batches :615 0.20174531610152585\n",
      "Iteration 616: tag train_accuracy, simple_value 0.10386\n",
      "Iteration 616: tag train_loss, simple_value 0.20175\n",
      "Timestamp 1612510542707, Iteration 616\n",
      "batches :616 0.2014753133929395\n",
      "Iteration 617: tag train_accuracy, simple_value 0.10392\n",
      "Iteration 617: tag train_loss, simple_value 0.20148\n",
      "Timestamp 1612510543014, Iteration 617\n",
      "batches :617 0.20120649572274674\n",
      "Iteration 618: tag train_accuracy, simple_value 0.10396\n",
      "Iteration 618: tag train_loss, simple_value 0.20121\n",
      "Timestamp 1612510543321, Iteration 618\n",
      "batches :618 0.20093895344841248\n",
      "Iteration 619: tag train_accuracy, simple_value 0.10399\n",
      "Iteration 619: tag train_loss, simple_value 0.20094\n",
      "Timestamp 1612510543628, Iteration 619\n",
      "batches :619 0.2006722036658851\n",
      "Iteration 620: tag train_accuracy, simple_value 0.10405\n",
      "Iteration 620: tag train_loss, simple_value 0.20067\n",
      "Timestamp 1612510543935, Iteration 620\n",
      "batches :620 0.20040664221610754\n",
      "Iteration 621: tag train_accuracy, simple_value 0.10401\n",
      "Iteration 621: tag train_loss, simple_value 0.20041\n",
      "Timestamp 1612510544243, Iteration 621\n",
      "batches :621 0.20014270589403485\n",
      "Iteration 622: tag train_accuracy, simple_value 0.10404\n",
      "Iteration 622: tag train_loss, simple_value 0.20014\n",
      "Timestamp 1612510544550, Iteration 622\n",
      "batches :622 0.19987955440278987\n",
      "Iteration 623: tag train_accuracy, simple_value 0.10395\n",
      "Iteration 623: tag train_loss, simple_value 0.19988\n",
      "Timestamp 1612510544855, Iteration 623\n",
      "batches :623 0.19961609993949173\n",
      "Iteration 624: tag train_accuracy, simple_value 0.10403\n",
      "Iteration 624: tag train_loss, simple_value 0.19962\n",
      "Timestamp 1612510545175, Iteration 624\n",
      "batches :624 0.19935396807984665\n",
      "Iteration 625: tag train_accuracy, simple_value 0.10412\n",
      "Iteration 625: tag train_loss, simple_value 0.19935\n",
      "Timestamp 1612510545481, Iteration 625\n",
      "batches :625 0.199092558413744\n",
      "Iteration 626: tag train_accuracy, simple_value 0.10408\n",
      "Iteration 626: tag train_loss, simple_value 0.19909\n",
      "Timestamp 1612510545788, Iteration 626\n",
      "batches :626 0.19883166511837666\n",
      "Iteration 627: tag train_accuracy, simple_value 0.10416\n",
      "Iteration 627: tag train_loss, simple_value 0.19883\n",
      "Timestamp 1612510546094, Iteration 627\n",
      "batches :627 0.1985713608705541\n",
      "Iteration 628: tag train_accuracy, simple_value 0.10429\n",
      "Iteration 628: tag train_loss, simple_value 0.19857\n",
      "Timestamp 1612510546400, Iteration 628\n",
      "batches :628 0.19831187976227635\n",
      "Iteration 629: tag train_accuracy, simple_value 0.10432\n",
      "Iteration 629: tag train_loss, simple_value 0.19831\n",
      "Timestamp 1612510546706, Iteration 629\n",
      "batches :629 0.19805344147385587\n",
      "Iteration 630: tag train_accuracy, simple_value 0.10438\n",
      "Iteration 630: tag train_loss, simple_value 0.19805\n",
      "Timestamp 1612510547013, Iteration 630\n",
      "batches :630 0.19779670357349374\n",
      "Iteration 631: tag train_accuracy, simple_value 0.10434\n",
      "Iteration 631: tag train_loss, simple_value 0.1978\n",
      "Timestamp 1612510547320, Iteration 631\n",
      "batches :631 0.1975415002690442\n",
      "Iteration 632: tag train_accuracy, simple_value 0.10425\n",
      "Iteration 632: tag train_loss, simple_value 0.19754\n",
      "Timestamp 1612510547626, Iteration 632\n",
      "batches :632 0.19728429475209758\n",
      "Iteration 633: tag train_accuracy, simple_value 0.10426\n",
      "Iteration 633: tag train_loss, simple_value 0.19728\n",
      "Timestamp 1612510547933, Iteration 633\n",
      "batches :633 0.19703175683062976\n",
      "Iteration 634: tag train_accuracy, simple_value 0.10429\n",
      "Iteration 634: tag train_loss, simple_value 0.19703\n",
      "Timestamp 1612510548240, Iteration 634\n",
      "batches :634 0.1967782414862034\n",
      "Iteration 635: tag train_accuracy, simple_value 0.10427\n",
      "Iteration 635: tag train_loss, simple_value 0.19678\n",
      "Timestamp 1612510548547, Iteration 635\n",
      "batches :635 0.19652616902012524\n",
      "Iteration 636: tag train_accuracy, simple_value 0.10428\n",
      "Iteration 636: tag train_loss, simple_value 0.19653\n",
      "Timestamp 1612510548852, Iteration 636\n",
      "batches :636 0.19627251214224775\n",
      "Iteration 637: tag train_accuracy, simple_value 0.10424\n",
      "Iteration 637: tag train_loss, simple_value 0.19627\n",
      "Timestamp 1612510549159, Iteration 637\n",
      "batches :637 0.1960207758415812\n",
      "Iteration 638: tag train_accuracy, simple_value 0.1042\n",
      "Iteration 638: tag train_loss, simple_value 0.19602\n",
      "Timestamp 1612510549465, Iteration 638\n",
      "batches :638 0.19577007566149518\n",
      "Iteration 639: tag train_accuracy, simple_value 0.10423\n",
      "Iteration 639: tag train_loss, simple_value 0.19577\n",
      "Timestamp 1612510549772, Iteration 639\n",
      "batches :639 0.19552018489333012\n",
      "Iteration 640: tag train_accuracy, simple_value 0.10422\n",
      "Iteration 640: tag train_loss, simple_value 0.19552\n",
      "Timestamp 1612510550094, Iteration 640\n",
      "batches :640 0.19527032657060772\n",
      "Iteration 641: tag train_accuracy, simple_value 0.10427\n",
      "Iteration 641: tag train_loss, simple_value 0.19527\n",
      "Timestamp 1612510550400, Iteration 641\n",
      "batches :641 0.1950218643115575\n",
      "Iteration 642: tag train_accuracy, simple_value 0.10433\n",
      "Iteration 642: tag train_loss, simple_value 0.19502\n",
      "Timestamp 1612510550706, Iteration 642\n",
      "batches :642 0.19477321184312815\n",
      "Iteration 643: tag train_accuracy, simple_value 0.10434\n",
      "Iteration 643: tag train_loss, simple_value 0.19477\n",
      "Timestamp 1612510551012, Iteration 643\n",
      "batches :643 0.19452737786258176\n",
      "Iteration 644: tag train_accuracy, simple_value 0.10425\n",
      "Iteration 644: tag train_loss, simple_value 0.19453\n",
      "Timestamp 1612510551319, Iteration 644\n",
      "batches :644 0.19428243490387193\n",
      "Iteration 645: tag train_accuracy, simple_value 0.10426\n",
      "Iteration 645: tag train_loss, simple_value 0.19428\n",
      "Timestamp 1612510551625, Iteration 645\n",
      "batches :645 0.19403761886926585\n",
      "Iteration 646: tag train_accuracy, simple_value 0.10431\n",
      "Iteration 646: tag train_loss, simple_value 0.19404\n",
      "Timestamp 1612510551932, Iteration 646\n",
      "batches :646 0.19379336923799095\n",
      "Iteration 647: tag train_accuracy, simple_value 0.10425\n",
      "Iteration 647: tag train_loss, simple_value 0.19379\n",
      "Timestamp 1612510552238, Iteration 647\n",
      "batches :647 0.19354970311570388\n",
      "Iteration 648: tag train_accuracy, simple_value 0.10421\n",
      "Iteration 648: tag train_loss, simple_value 0.19355\n",
      "Timestamp 1612510552545, Iteration 648\n",
      "batches :648 0.19330839369515027\n",
      "Iteration 649: tag train_accuracy, simple_value 0.10429\n",
      "Iteration 649: tag train_loss, simple_value 0.19331\n",
      "Timestamp 1612510552851, Iteration 649\n",
      "batches :649 0.1930670279475684\n",
      "Iteration 650: tag train_accuracy, simple_value 0.1042\n",
      "Iteration 650: tag train_loss, simple_value 0.19307\n",
      "Timestamp 1612510553157, Iteration 650\n",
      "batches :650 0.19282554851701628\n",
      "Iteration 651: tag train_accuracy, simple_value 0.10423\n",
      "Iteration 651: tag train_loss, simple_value 0.19283\n",
      "Timestamp 1612510553463, Iteration 651\n",
      "batches :651 0.19258467410726848\n",
      "Iteration 652: tag train_accuracy, simple_value 0.10426\n",
      "Iteration 652: tag train_loss, simple_value 0.19258\n",
      "Timestamp 1612510553770, Iteration 652\n",
      "batches :652 0.19234378723873316\n",
      "Iteration 653: tag train_accuracy, simple_value 0.10427\n",
      "Iteration 653: tag train_loss, simple_value 0.19234\n",
      "Timestamp 1612510554076, Iteration 653\n",
      "batches :653 0.19210386848445143\n",
      "Iteration 654: tag train_accuracy, simple_value 0.10437\n",
      "Iteration 654: tag train_loss, simple_value 0.1921\n",
      "Timestamp 1612510554383, Iteration 654\n",
      "batches :654 0.19186633075687134\n",
      "Iteration 655: tag train_accuracy, simple_value 0.10438\n",
      "Iteration 655: tag train_loss, simple_value 0.19187\n",
      "Timestamp 1612510554690, Iteration 655\n",
      "batches :655 0.1916296167114309\n",
      "Iteration 656: tag train_accuracy, simple_value 0.10439\n",
      "Iteration 656: tag train_loss, simple_value 0.19163\n",
      "Timestamp 1612510555012, Iteration 656\n",
      "batches :656 0.1913941660100912\n",
      "Iteration 657: tag train_accuracy, simple_value 0.1044\n",
      "Iteration 657: tag train_loss, simple_value 0.19139\n",
      "Timestamp 1612510555319, Iteration 657\n",
      "batches :657 0.1911576930972176\n",
      "Iteration 658: tag train_accuracy, simple_value 0.10438\n",
      "Iteration 658: tag train_loss, simple_value 0.19116\n",
      "Timestamp 1612510555627, Iteration 658\n",
      "batches :658 0.1909234064679287\n",
      "Iteration 659: tag train_accuracy, simple_value 0.10434\n",
      "Iteration 659: tag train_loss, simple_value 0.19092\n",
      "Timestamp 1612510555934, Iteration 659\n",
      "batches :659 0.1906871903116768\n",
      "Iteration 660: tag train_accuracy, simple_value 0.10437\n",
      "Iteration 660: tag train_loss, simple_value 0.19069\n",
      "Timestamp 1612510556240, Iteration 660\n",
      "batches :660 0.19045256366106597\n",
      "Iteration 661: tag train_accuracy, simple_value 0.10445\n",
      "Iteration 661: tag train_loss, simple_value 0.19045\n",
      "Timestamp 1612510556545, Iteration 661\n",
      "batches :661 0.1902204393078087\n",
      "Iteration 662: tag train_accuracy, simple_value 0.10439\n",
      "Iteration 662: tag train_loss, simple_value 0.19022\n",
      "Timestamp 1612510556852, Iteration 662\n",
      "batches :662 0.18999206870119018\n",
      "Iteration 663: tag train_accuracy, simple_value 0.10439\n",
      "Iteration 663: tag train_loss, simple_value 0.18999\n",
      "Timestamp 1612510557161, Iteration 663\n",
      "batches :663 0.18975918657437169\n",
      "Iteration 664: tag train_accuracy, simple_value 0.10436\n",
      "Iteration 664: tag train_loss, simple_value 0.18976\n",
      "Timestamp 1612510557471, Iteration 664\n",
      "batches :664 0.18952690500833363\n",
      "Iteration 665: tag train_accuracy, simple_value 0.10443\n",
      "Iteration 665: tag train_loss, simple_value 0.18953\n",
      "Timestamp 1612510557779, Iteration 665\n",
      "batches :665 0.18929598493907684\n",
      "Iteration 666: tag train_accuracy, simple_value 0.10442\n",
      "Iteration 666: tag train_loss, simple_value 0.1893\n",
      "Timestamp 1612510558086, Iteration 666\n",
      "batches :666 0.1890662258977557\n",
      "Iteration 667: tag train_accuracy, simple_value 0.1045\n",
      "Iteration 667: tag train_loss, simple_value 0.18907\n",
      "Timestamp 1612510558393, Iteration 667\n",
      "batches :667 0.18883498444192592\n",
      "Iteration 668: tag train_accuracy, simple_value 0.10455\n",
      "Iteration 668: tag train_loss, simple_value 0.18883\n",
      "Timestamp 1612510558699, Iteration 668\n",
      "batches :668 0.18860650019062136\n",
      "Iteration 669: tag train_accuracy, simple_value 0.10451\n",
      "Iteration 669: tag train_loss, simple_value 0.18861\n",
      "Timestamp 1612510559008, Iteration 669\n",
      "batches :669 0.18837674288984016\n",
      "Iteration 670: tag train_accuracy, simple_value 0.10449\n",
      "Iteration 670: tag train_loss, simple_value 0.18838\n",
      "Timestamp 1612510559315, Iteration 670\n",
      "batches :670 0.18815045534118788\n",
      "Iteration 671: tag train_accuracy, simple_value 0.10438\n",
      "Iteration 671: tag train_loss, simple_value 0.18815\n",
      "Timestamp 1612510559622, Iteration 671\n",
      "batches :671 0.18792509616265177\n",
      "Iteration 672: tag train_accuracy, simple_value 0.10435\n",
      "Iteration 672: tag train_loss, simple_value 0.18793\n",
      "Timestamp 1612510559943, Iteration 672\n",
      "batches :672 0.18770048004530726\n",
      "Iteration 673: tag train_accuracy, simple_value 0.10433\n",
      "Iteration 673: tag train_loss, simple_value 0.1877\n",
      "Timestamp 1612510560250, Iteration 673\n",
      "batches :673 0.18747427662693375\n",
      "Iteration 674: tag train_accuracy, simple_value 0.10438\n",
      "Iteration 674: tag train_loss, simple_value 0.18747\n",
      "Timestamp 1612510560556, Iteration 674\n",
      "batches :674 0.18724844470973892\n",
      "Iteration 675: tag train_accuracy, simple_value 0.10439\n",
      "Iteration 675: tag train_loss, simple_value 0.18725\n",
      "Timestamp 1612510560862, Iteration 675\n",
      "batches :675 0.18702341475972423\n",
      "Iteration 676: tag train_accuracy, simple_value 0.10437\n",
      "Iteration 676: tag train_loss, simple_value 0.18702\n",
      "Timestamp 1612510561168, Iteration 676\n",
      "batches :676 0.1868016064696058\n",
      "Iteration 677: tag train_accuracy, simple_value 0.10445\n",
      "Iteration 677: tag train_loss, simple_value 0.1868\n",
      "Timestamp 1612510561476, Iteration 677\n",
      "batches :677 0.18658193566202236\n",
      "Iteration 678: tag train_accuracy, simple_value 0.10441\n",
      "Iteration 678: tag train_loss, simple_value 0.18658\n",
      "Timestamp 1612510561782, Iteration 678\n",
      "batches :678 0.18636065870963014\n",
      "Iteration 679: tag train_accuracy, simple_value 0.10435\n",
      "Iteration 679: tag train_loss, simple_value 0.18636\n",
      "Timestamp 1612510562088, Iteration 679\n",
      "batches :679 0.18614059816140666\n",
      "Iteration 680: tag train_accuracy, simple_value 0.10436\n",
      "Iteration 680: tag train_loss, simple_value 0.18614\n",
      "Timestamp 1612510562395, Iteration 680\n",
      "batches :680 0.18592116172041964\n",
      "Iteration 681: tag train_accuracy, simple_value 0.10441\n",
      "Iteration 681: tag train_loss, simple_value 0.18592\n",
      "Timestamp 1612510562701, Iteration 681\n",
      "batches :681 0.18570128758266985\n",
      "Iteration 682: tag train_accuracy, simple_value 0.10449\n",
      "Iteration 682: tag train_loss, simple_value 0.1857\n",
      "Timestamp 1612510563008, Iteration 682\n",
      "batches :682 0.1854822398914445\n",
      "Iteration 683: tag train_accuracy, simple_value 0.1045\n",
      "Iteration 683: tag train_loss, simple_value 0.18548\n",
      "Timestamp 1612510563314, Iteration 683\n",
      "batches :683 0.18526413416596318\n",
      "Iteration 684: tag train_accuracy, simple_value 0.10443\n",
      "Iteration 684: tag train_loss, simple_value 0.18526\n",
      "Timestamp 1612510563620, Iteration 684\n",
      "batches :684 0.18504628034755152\n",
      "Iteration 685: tag train_accuracy, simple_value 0.10451\n",
      "Iteration 685: tag train_loss, simple_value 0.18505\n",
      "Timestamp 1612510563926, Iteration 685\n",
      "batches :685 0.18483130160691966\n",
      "Iteration 686: tag train_accuracy, simple_value 0.10458\n",
      "Iteration 686: tag train_loss, simple_value 0.18483\n",
      "Timestamp 1612510564232, Iteration 686\n",
      "batches :686 0.18461479917820273\n",
      "Iteration 687: tag train_accuracy, simple_value 0.1045\n",
      "Iteration 687: tag train_loss, simple_value 0.18461\n",
      "Timestamp 1612510564540, Iteration 687\n",
      "batches :687 0.18439927286694838\n",
      "Iteration 688: tag train_accuracy, simple_value 0.10442\n",
      "Iteration 688: tag train_loss, simple_value 0.1844\n",
      "Timestamp 1612510564861, Iteration 688\n",
      "batches :688 0.1841846701450819\n",
      "Iteration 689: tag train_accuracy, simple_value 0.10436\n",
      "Iteration 689: tag train_loss, simple_value 0.18418\n",
      "Timestamp 1612510565168, Iteration 689\n",
      "batches :689 0.18396846880847034\n",
      "Iteration 690: tag train_accuracy, simple_value 0.10441\n",
      "Iteration 690: tag train_loss, simple_value 0.18397\n",
      "Timestamp 1612510565474, Iteration 690\n",
      "batches :690 0.18375395724306937\n",
      "Iteration 691: tag train_accuracy, simple_value 0.10442\n",
      "Iteration 691: tag train_loss, simple_value 0.18375\n",
      "Timestamp 1612510565781, Iteration 691\n",
      "batches :691 0.18354064705424475\n",
      "Iteration 692: tag train_accuracy, simple_value 0.1044\n",
      "Iteration 692: tag train_loss, simple_value 0.18354\n",
      "Timestamp 1612510566087, Iteration 692\n",
      "batches :692 0.18332746529122654\n",
      "Iteration 693: tag train_accuracy, simple_value 0.10452\n",
      "Iteration 693: tag train_loss, simple_value 0.18333\n",
      "Timestamp 1612510566393, Iteration 693\n",
      "batches :693 0.1831154010458856\n",
      "Iteration 694: tag train_accuracy, simple_value 0.1045\n",
      "Iteration 694: tag train_loss, simple_value 0.18312\n",
      "Timestamp 1612510566699, Iteration 694\n",
      "batches :694 0.18290711410601304\n",
      "Iteration 695: tag train_accuracy, simple_value 0.10447\n",
      "Iteration 695: tag train_loss, simple_value 0.18291\n",
      "Timestamp 1612510567006, Iteration 695\n",
      "batches :695 0.18269707038998603\n",
      "Iteration 696: tag train_accuracy, simple_value 0.10445\n",
      "Iteration 696: tag train_loss, simple_value 0.1827\n",
      "Timestamp 1612510567313, Iteration 696\n",
      "batches :696 0.18248617188234267\n",
      "Iteration 697: tag train_accuracy, simple_value 0.10448\n",
      "Iteration 697: tag train_loss, simple_value 0.18249\n",
      "Timestamp 1612510567620, Iteration 697\n",
      "batches :697 0.18227576180073932\n",
      "Iteration 698: tag train_accuracy, simple_value 0.10447\n",
      "Iteration 698: tag train_loss, simple_value 0.18228\n",
      "Timestamp 1612510567927, Iteration 698\n",
      "batches :698 0.18206492663070262\n",
      "Iteration 699: tag train_accuracy, simple_value 0.10458\n",
      "Iteration 699: tag train_loss, simple_value 0.18206\n",
      "Timestamp 1612510568232, Iteration 699\n",
      "batches :699 0.18185586520658542\n",
      "Iteration 700: tag train_accuracy, simple_value 0.10461\n",
      "Iteration 700: tag train_loss, simple_value 0.18186\n",
      "Timestamp 1612510568538, Iteration 700\n",
      "batches :700 0.18164774223097732\n",
      "Iteration 701: tag train_accuracy, simple_value 0.10471\n",
      "Iteration 701: tag train_loss, simple_value 0.18165\n",
      "Timestamp 1612510568845, Iteration 701\n",
      "batches :701 0.18144259725122072\n",
      "Iteration 702: tag train_accuracy, simple_value 0.10465\n",
      "Iteration 702: tag train_loss, simple_value 0.18144\n",
      "Timestamp 1612510569152, Iteration 702\n",
      "batches :702 0.1812344009210581\n",
      "Iteration 703: tag train_accuracy, simple_value 0.10472\n",
      "Iteration 703: tag train_loss, simple_value 0.18123\n",
      "Timestamp 1612510569459, Iteration 703\n",
      "batches :703 0.18102701909172417\n",
      "Iteration 704: tag train_accuracy, simple_value 0.10486\n",
      "Iteration 704: tag train_loss, simple_value 0.18103\n",
      "Timestamp 1612510569783, Iteration 704\n",
      "batches :704 0.18082131961339407\n",
      "Iteration 705: tag train_accuracy, simple_value 0.10494\n",
      "Iteration 705: tag train_loss, simple_value 0.18082\n",
      "Timestamp 1612510570090, Iteration 705\n",
      "batches :705 0.18061624595776518\n",
      "Iteration 706: tag train_accuracy, simple_value 0.1049\n",
      "Iteration 706: tag train_loss, simple_value 0.18062\n",
      "Timestamp 1612510570397, Iteration 706\n",
      "batches :706 0.18041145601032813\n",
      "Iteration 707: tag train_accuracy, simple_value 0.10495\n",
      "Iteration 707: tag train_loss, simple_value 0.18041\n",
      "Timestamp 1612510570704, Iteration 707\n",
      "batches :707 0.18020603396894233\n",
      "Iteration 708: tag train_accuracy, simple_value 0.10498\n",
      "Iteration 708: tag train_loss, simple_value 0.18021\n",
      "Timestamp 1612510571011, Iteration 708\n",
      "batches :708 0.1800030350874541\n",
      "Iteration 709: tag train_accuracy, simple_value 0.10496\n",
      "Iteration 709: tag train_loss, simple_value 0.18\n",
      "Timestamp 1612510571318, Iteration 709\n",
      "batches :709 0.17979991558429553\n",
      "Iteration 710: tag train_accuracy, simple_value 0.10499\n",
      "Iteration 710: tag train_loss, simple_value 0.1798\n",
      "Timestamp 1612510571624, Iteration 710\n",
      "batches :710 0.17959766854912462\n",
      "Iteration 711: tag train_accuracy, simple_value 0.10508\n",
      "Iteration 711: tag train_loss, simple_value 0.1796\n",
      "Timestamp 1612510571930, Iteration 711\n",
      "batches :711 0.17939689055441468\n",
      "Iteration 712: tag train_accuracy, simple_value 0.10502\n",
      "Iteration 712: tag train_loss, simple_value 0.1794\n",
      "Timestamp 1612510572236, Iteration 712\n",
      "batches :712 0.1791946122611172\n",
      "Iteration 713: tag train_accuracy, simple_value 0.10516\n",
      "Iteration 713: tag train_loss, simple_value 0.17919\n",
      "Timestamp 1612510572543, Iteration 713\n",
      "batches :713 0.17899440873680744\n",
      "Iteration 714: tag train_accuracy, simple_value 0.10515\n",
      "Iteration 714: tag train_loss, simple_value 0.17899\n",
      "Timestamp 1612510572849, Iteration 714\n",
      "batches :714 0.17879478152872635\n",
      "Iteration 715: tag train_accuracy, simple_value 0.10511\n",
      "Iteration 715: tag train_loss, simple_value 0.17879\n",
      "Timestamp 1612510573156, Iteration 715\n",
      "batches :715 0.17859289396059264\n",
      "Iteration 716: tag train_accuracy, simple_value 0.10518\n",
      "Iteration 716: tag train_loss, simple_value 0.17859\n",
      "Timestamp 1612510573463, Iteration 716\n",
      "batches :716 0.1783948336472974\n",
      "Iteration 717: tag train_accuracy, simple_value 0.1051\n",
      "Iteration 717: tag train_loss, simple_value 0.17839\n",
      "Timestamp 1612510573769, Iteration 717\n",
      "batches :717 0.1781983152655378\n",
      "Iteration 718: tag train_accuracy, simple_value 0.10504\n",
      "Iteration 718: tag train_loss, simple_value 0.1782\n",
      "Timestamp 1612510574075, Iteration 718\n",
      "batches :718 0.17799926145536654\n",
      "Iteration 719: tag train_accuracy, simple_value 0.10515\n",
      "Iteration 719: tag train_loss, simple_value 0.178\n",
      "Timestamp 1612510574380, Iteration 719\n",
      "batches :719 0.1778016352562314\n",
      "Iteration 720: tag train_accuracy, simple_value 0.10514\n",
      "Iteration 720: tag train_loss, simple_value 0.1778\n",
      "Timestamp 1612510574702, Iteration 720\n",
      "batches :720 0.1776044033985171\n",
      "Iteration 721: tag train_accuracy, simple_value 0.10519\n",
      "Iteration 721: tag train_loss, simple_value 0.1776\n",
      "Timestamp 1612510575008, Iteration 721\n",
      "batches :721 0.17740898453804888\n",
      "Iteration 722: tag train_accuracy, simple_value 0.10519\n",
      "Iteration 722: tag train_loss, simple_value 0.17741\n",
      "Timestamp 1612510575314, Iteration 722\n",
      "batches :722 0.1772125202316244\n",
      "Iteration 723: tag train_accuracy, simple_value 0.10535\n",
      "Iteration 723: tag train_loss, simple_value 0.17721\n",
      "Timestamp 1612510575620, Iteration 723\n",
      "batches :723 0.17701751015884581\n",
      "Iteration 724: tag train_accuracy, simple_value 0.1054\n",
      "Iteration 724: tag train_loss, simple_value 0.17702\n",
      "Timestamp 1612510575926, Iteration 724\n",
      "batches :724 0.17682200582770025\n",
      "Iteration 725: tag train_accuracy, simple_value 0.10549\n",
      "Iteration 725: tag train_loss, simple_value 0.17682\n",
      "Timestamp 1612510576233, Iteration 725\n",
      "batches :725 0.17662952775071408\n",
      "Iteration 726: tag train_accuracy, simple_value 0.10552\n",
      "Iteration 726: tag train_loss, simple_value 0.17663\n",
      "Timestamp 1612510576538, Iteration 726\n",
      "batches :726 0.17643420664547396\n",
      "Iteration 727: tag train_accuracy, simple_value 0.10561\n",
      "Iteration 727: tag train_loss, simple_value 0.17643\n",
      "Timestamp 1612510576845, Iteration 727\n",
      "batches :727 0.17623958314424368\n",
      "Iteration 728: tag train_accuracy, simple_value 0.1057\n",
      "Iteration 728: tag train_loss, simple_value 0.17624\n",
      "Timestamp 1612510577151, Iteration 728\n",
      "batches :728 0.17604692143152703\n",
      "Iteration 729: tag train_accuracy, simple_value 0.10575\n",
      "Iteration 729: tag train_loss, simple_value 0.17605\n",
      "Timestamp 1612510577458, Iteration 729\n",
      "batches :729 0.1758543153399466\n",
      "Iteration 730: tag train_accuracy, simple_value 0.10584\n",
      "Iteration 730: tag train_loss, simple_value 0.17585\n",
      "Timestamp 1612510577765, Iteration 730\n",
      "batches :730 0.17566231424881987\n",
      "Iteration 731: tag train_accuracy, simple_value 0.1058\n",
      "Iteration 731: tag train_loss, simple_value 0.17566\n",
      "Timestamp 1612510578073, Iteration 731\n",
      "batches :731 0.17546965464664224\n",
      "Iteration 732: tag train_accuracy, simple_value 0.10593\n",
      "Iteration 732: tag train_loss, simple_value 0.17547\n",
      "Timestamp 1612510578379, Iteration 732\n",
      "batches :732 0.17528000306541638\n",
      "Iteration 733: tag train_accuracy, simple_value 0.1059\n",
      "Iteration 733: tag train_loss, simple_value 0.17528\n",
      "Timestamp 1612510578685, Iteration 733\n",
      "batches :733 0.1750897292103644\n",
      "Iteration 734: tag train_accuracy, simple_value 0.10586\n",
      "Iteration 734: tag train_loss, simple_value 0.17509\n",
      "Timestamp 1612510578992, Iteration 734\n",
      "batches :734 0.17489923091163753\n",
      "Iteration 735: tag train_accuracy, simple_value 0.10588\n",
      "Iteration 735: tag train_loss, simple_value 0.1749\n",
      "Timestamp 1612510579298, Iteration 735\n",
      "batches :735 0.17471077550126582\n",
      "Iteration 736: tag train_accuracy, simple_value 0.10593\n",
      "Iteration 736: tag train_loss, simple_value 0.17471\n",
      "Timestamp 1612510579618, Iteration 736\n",
      "batches :736 0.17452224162305988\n",
      "Iteration 737: tag train_accuracy, simple_value 0.10602\n",
      "Iteration 737: tag train_loss, simple_value 0.17452\n",
      "Timestamp 1612510579923, Iteration 737\n",
      "batches :737 0.1743329604089341\n",
      "Iteration 738: tag train_accuracy, simple_value 0.10617\n",
      "Iteration 738: tag train_loss, simple_value 0.17433\n",
      "Timestamp 1612510580228, Iteration 738\n",
      "batches :738 0.17414542463756996\n",
      "Iteration 739: tag train_accuracy, simple_value 0.10626\n",
      "Iteration 739: tag train_loss, simple_value 0.17415\n",
      "Timestamp 1612510580533, Iteration 739\n",
      "batches :739 0.1739583318758156\n",
      "Iteration 740: tag train_accuracy, simple_value 0.10629\n",
      "Iteration 740: tag train_loss, simple_value 0.17396\n",
      "Timestamp 1612510580838, Iteration 740\n",
      "batches :740 0.1737719501494556\n",
      "Iteration 741: tag train_accuracy, simple_value 0.10629\n",
      "Iteration 741: tag train_loss, simple_value 0.17377\n",
      "Timestamp 1612510581144, Iteration 741\n",
      "batches :741 0.1735863435747009\n",
      "Iteration 742: tag train_accuracy, simple_value 0.1063\n",
      "Iteration 742: tag train_loss, simple_value 0.17359\n",
      "Timestamp 1612510581451, Iteration 742\n",
      "batches :742 0.1734007019449196\n",
      "Iteration 743: tag train_accuracy, simple_value 0.1063\n",
      "Iteration 743: tag train_loss, simple_value 0.1734\n",
      "Timestamp 1612510581757, Iteration 743\n",
      "batches :743 0.17321626076908483\n",
      "Iteration 744: tag train_accuracy, simple_value 0.1063\n",
      "Iteration 744: tag train_loss, simple_value 0.17322\n",
      "Timestamp 1612510582064, Iteration 744\n",
      "batches :744 0.17303168452194623\n",
      "Iteration 745: tag train_accuracy, simple_value 0.10629\n",
      "Iteration 745: tag train_loss, simple_value 0.17303\n",
      "Timestamp 1612510582370, Iteration 745\n",
      "batches :745 0.17284594235704248\n",
      "Iteration 746: tag train_accuracy, simple_value 0.10631\n",
      "Iteration 746: tag train_loss, simple_value 0.17285\n",
      "Timestamp 1612510582677, Iteration 746\n",
      "batches :746 0.1726637150612897\n",
      "Iteration 747: tag train_accuracy, simple_value 0.1063\n",
      "Iteration 747: tag train_loss, simple_value 0.17266\n",
      "Timestamp 1612510582991, Iteration 747\n",
      "batches :747 0.17248053721574894\n",
      "Iteration 748: tag train_accuracy, simple_value 0.10632\n",
      "Iteration 748: tag train_loss, simple_value 0.17248\n",
      "Timestamp 1612510583299, Iteration 748\n",
      "batches :748 0.1722969008450282\n",
      "Iteration 749: tag train_accuracy, simple_value 0.10645\n",
      "Iteration 749: tag train_loss, simple_value 0.1723\n",
      "Timestamp 1612510583606, Iteration 749\n",
      "batches :749 0.17211379038694705\n",
      "Iteration 750: tag train_accuracy, simple_value 0.10654\n",
      "Iteration 750: tag train_loss, simple_value 0.17211\n",
      "Timestamp 1612510583913, Iteration 750\n",
      "batches :750 0.17193198938667775\n",
      "Iteration 751: tag train_accuracy, simple_value 0.10654\n",
      "Iteration 751: tag train_loss, simple_value 0.17193\n",
      "Timestamp 1612510584219, Iteration 751\n",
      "batches :751 0.17175062046883585\n",
      "Iteration 752: tag train_accuracy, simple_value 0.10657\n",
      "Iteration 752: tag train_loss, simple_value 0.17175\n",
      "Timestamp 1612510584539, Iteration 752\n",
      "batches :752 0.1715695355582903\n",
      "Iteration 753: tag train_accuracy, simple_value 0.10661\n",
      "Iteration 753: tag train_loss, simple_value 0.17157\n",
      "Timestamp 1612510584845, Iteration 753\n",
      "batches :753 0.17138833625323743\n",
      "Iteration 754: tag train_accuracy, simple_value 0.10668\n",
      "Iteration 754: tag train_loss, simple_value 0.17139\n",
      "Timestamp 1612510585151, Iteration 754\n",
      "batches :754 0.17120662479308144\n",
      "Iteration 755: tag train_accuracy, simple_value 0.10681\n",
      "Iteration 755: tag train_loss, simple_value 0.17121\n",
      "Timestamp 1612510585457, Iteration 755\n",
      "batches :755 0.1710272810317033\n",
      "Iteration 756: tag train_accuracy, simple_value 0.10683\n",
      "Iteration 756: tag train_loss, simple_value 0.17103\n",
      "Timestamp 1612510585764, Iteration 756\n",
      "batches :756 0.17084868758838012\n",
      "Iteration 757: tag train_accuracy, simple_value 0.10685\n",
      "Iteration 757: tag train_loss, simple_value 0.17085\n",
      "Timestamp 1612510586071, Iteration 757\n",
      "batches :757 0.17066900709661184\n",
      "Iteration 758: tag train_accuracy, simple_value 0.10694\n",
      "Iteration 758: tag train_loss, simple_value 0.17067\n",
      "Timestamp 1612510586378, Iteration 758\n",
      "batches :758 0.17048954626540394\n",
      "Iteration 759: tag train_accuracy, simple_value 0.10707\n",
      "Iteration 759: tag train_loss, simple_value 0.17049\n",
      "Timestamp 1612510586685, Iteration 759\n",
      "batches :759 0.17031090519095285\n",
      "Iteration 760: tag train_accuracy, simple_value 0.10717\n",
      "Iteration 760: tag train_loss, simple_value 0.17031\n",
      "Timestamp 1612510586992, Iteration 760\n",
      "batches :760 0.17013367262031687\n",
      "Iteration 761: tag train_accuracy, simple_value 0.10722\n",
      "Iteration 761: tag train_loss, simple_value 0.17013\n",
      "Timestamp 1612510587300, Iteration 761\n",
      "batches :761 0.16995580861058562\n",
      "Iteration 762: tag train_accuracy, simple_value 0.10742\n",
      "Iteration 762: tag train_loss, simple_value 0.16996\n",
      "Timestamp 1612510587608, Iteration 762\n",
      "batches :762 0.1697799835666975\n",
      "Iteration 763: tag train_accuracy, simple_value 0.10753\n",
      "Iteration 763: tag train_loss, simple_value 0.16978\n",
      "Timestamp 1612510587914, Iteration 763\n",
      "batches :763 0.16960428994476093\n",
      "Iteration 764: tag train_accuracy, simple_value 0.10765\n",
      "Iteration 764: tag train_loss, simple_value 0.1696\n",
      "Timestamp 1612510588221, Iteration 764\n",
      "batches :764 0.16943063499181682\n",
      "Iteration 765: tag train_accuracy, simple_value 0.10776\n",
      "Iteration 765: tag train_loss, simple_value 0.16943\n",
      "Timestamp 1612510588528, Iteration 765\n",
      "batches :765 0.1692550201382902\n",
      "Iteration 766: tag train_accuracy, simple_value 0.1078\n",
      "Iteration 766: tag train_loss, simple_value 0.16926\n",
      "Timestamp 1612510588834, Iteration 766\n",
      "batches :766 0.16908094327770387\n",
      "Iteration 767: tag train_accuracy, simple_value 0.10776\n",
      "Iteration 767: tag train_loss, simple_value 0.16908\n",
      "Timestamp 1612510589142, Iteration 767\n",
      "batches :767 0.16890790158059482\n",
      "Iteration 768: tag train_accuracy, simple_value 0.10785\n",
      "Iteration 768: tag train_loss, simple_value 0.16891\n",
      "Timestamp 1612510589464, Iteration 768\n",
      "batches :768 0.1687339987111045\n",
      "Iteration 769: tag train_accuracy, simple_value 0.10783\n",
      "Iteration 769: tag train_loss, simple_value 0.16873\n",
      "Timestamp 1612510589770, Iteration 769\n",
      "batches :769 0.16855932083295752\n",
      "Iteration 770: tag train_accuracy, simple_value 0.10799\n",
      "Iteration 770: tag train_loss, simple_value 0.16856\n",
      "Timestamp 1612510590078, Iteration 770\n",
      "batches :770 0.1683867146345702\n",
      "Iteration 771: tag train_accuracy, simple_value 0.10814\n",
      "Iteration 771: tag train_loss, simple_value 0.16839\n",
      "Timestamp 1612510590383, Iteration 771\n",
      "batches :771 0.16821260667670562\n",
      "Iteration 772: tag train_accuracy, simple_value 0.10824\n",
      "Iteration 772: tag train_loss, simple_value 0.16821\n",
      "Timestamp 1612510590689, Iteration 772\n",
      "batches :772 0.1680410984189399\n",
      "Iteration 773: tag train_accuracy, simple_value 0.10834\n",
      "Iteration 773: tag train_loss, simple_value 0.16804\n",
      "Timestamp 1612510590996, Iteration 773\n",
      "batches :773 0.16787094117791657\n",
      "Iteration 774: tag train_accuracy, simple_value 0.10836\n",
      "Iteration 774: tag train_loss, simple_value 0.16787\n",
      "Timestamp 1612510591302, Iteration 774\n",
      "batches :774 0.1677000270987974\n",
      "Iteration 775: tag train_accuracy, simple_value 0.10843\n",
      "Iteration 775: tag train_loss, simple_value 0.1677\n",
      "Timestamp 1612510591608, Iteration 775\n",
      "batches :775 0.16752980898945563\n",
      "Iteration 776: tag train_accuracy, simple_value 0.10857\n",
      "Iteration 776: tag train_loss, simple_value 0.16753\n",
      "Timestamp 1612510591913, Iteration 776\n",
      "batches :776 0.16735837482799268\n",
      "Iteration 777: tag train_accuracy, simple_value 0.10875\n",
      "Iteration 777: tag train_loss, simple_value 0.16736\n",
      "Timestamp 1612510592217, Iteration 777\n",
      "batches :777 0.16718862514506588\n",
      "Iteration 778: tag train_accuracy, simple_value 0.10881\n",
      "Iteration 778: tag train_loss, simple_value 0.16719\n",
      "Timestamp 1612510592521, Iteration 778\n",
      "batches :778 0.16701794766694078\n",
      "Iteration 779: tag train_accuracy, simple_value 0.10905\n",
      "Iteration 779: tag train_loss, simple_value 0.16702\n",
      "Timestamp 1612510592826, Iteration 779\n",
      "batches :779 0.16685069683790973\n",
      "Iteration 780: tag train_accuracy, simple_value 0.10907\n",
      "Iteration 780: tag train_loss, simple_value 0.16685\n",
      "Timestamp 1612510593130, Iteration 780\n",
      "batches :780 0.1666815043021089\n",
      "Iteration 781: tag train_accuracy, simple_value 0.10915\n",
      "Iteration 781: tag train_loss, simple_value 0.16668\n",
      "Timestamp 1612510593481, Iteration 781\n",
      "batches :781 0.036350611597299576\n",
      "Iteration 0: tag train_accuracy, simple_value 0.1875\n",
      "Iteration 0: tag train_loss, simple_value 0.03635\n",
      "Timestamp 1612510593913, Iteration 782\n",
      "batches :782 0.036115339025855064\n",
      "Iteration 2: tag train_accuracy, simple_value 0.14062\n",
      "Iteration 2: tag train_loss, simple_value 0.03612\n",
      "Timestamp 1612510594219, Iteration 783\n",
      "batches :783 0.036083973944187164\n",
      "Iteration 3: tag train_accuracy, simple_value 0.15625\n",
      "Iteration 3: tag train_loss, simple_value 0.03608\n",
      "Timestamp 1612510594526, Iteration 784\n",
      "batches :784 0.035820966586470604\n",
      "Iteration 4: tag train_accuracy, simple_value 0.14844\n",
      "Iteration 4: tag train_loss, simple_value 0.03582\n",
      "Timestamp 1612510594833, Iteration 785\n",
      "batches :785 0.03575182631611824\n",
      "Iteration 5: tag train_accuracy, simple_value 0.15\n",
      "Iteration 5: tag train_loss, simple_value 0.03575\n",
      "Timestamp 1612510595141, Iteration 786\n",
      "batches :786 0.03543610560397307\n",
      "Iteration 6: tag train_accuracy, simple_value 0.15104\n",
      "Iteration 6: tag train_loss, simple_value 0.03544\n",
      "Timestamp 1612510595447, Iteration 787\n",
      "batches :787 0.035457509968961985\n",
      "Iteration 7: tag train_accuracy, simple_value 0.14955\n",
      "Iteration 7: tag train_loss, simple_value 0.03546\n",
      "Timestamp 1612510595753, Iteration 788\n",
      "batches :788 0.035320340655744076\n",
      "Iteration 8: tag train_accuracy, simple_value 0.15039\n",
      "Iteration 8: tag train_loss, simple_value 0.03532\n",
      "Timestamp 1612510596058, Iteration 789\n",
      "batches :789 0.03533731111221843\n",
      "Iteration 9: tag train_accuracy, simple_value 0.15625\n",
      "Iteration 9: tag train_loss, simple_value 0.03534\n",
      "Timestamp 1612510596364, Iteration 790\n",
      "batches :790 0.035202810540795326\n",
      "Iteration 10: tag train_accuracy, simple_value 0.16094\n",
      "Iteration 10: tag train_loss, simple_value 0.0352\n",
      "Timestamp 1612510596672, Iteration 791\n",
      "batches :791 0.035200029611587524\n",
      "Iteration 11: tag train_accuracy, simple_value 0.16335\n",
      "Iteration 11: tag train_loss, simple_value 0.0352\n",
      "Timestamp 1612510596978, Iteration 792\n",
      "batches :792 0.03515427280217409\n",
      "Iteration 12: tag train_accuracy, simple_value 0.16276\n",
      "Iteration 12: tag train_loss, simple_value 0.03515\n",
      "Timestamp 1612510597285, Iteration 793\n",
      "batches :793 0.03518031795437519\n",
      "Iteration 13: tag train_accuracy, simple_value 0.16106\n",
      "Iteration 13: tag train_loss, simple_value 0.03518\n",
      "Timestamp 1612510597593, Iteration 794\n",
      "batches :794 0.03526770802480834\n",
      "Iteration 14: tag train_accuracy, simple_value 0.15848\n",
      "Iteration 14: tag train_loss, simple_value 0.03527\n",
      "Timestamp 1612510597899, Iteration 795\n",
      "batches :795 0.03520983134706815\n",
      "Iteration 15: tag train_accuracy, simple_value 0.16146\n",
      "Iteration 15: tag train_loss, simple_value 0.03521\n",
      "Timestamp 1612510598205, Iteration 796\n",
      "batches :796 0.03514279774390161\n",
      "Iteration 16: tag train_accuracy, simple_value 0.16406\n",
      "Iteration 16: tag train_loss, simple_value 0.03514\n",
      "Timestamp 1612510598527, Iteration 797\n",
      "batches :797 0.03512176323462935\n",
      "Iteration 17: tag train_accuracy, simple_value 0.16176\n",
      "Iteration 17: tag train_loss, simple_value 0.03512\n",
      "Timestamp 1612510598834, Iteration 798\n",
      "batches :798 0.035112295713689595\n",
      "Iteration 18: tag train_accuracy, simple_value 0.16059\n",
      "Iteration 18: tag train_loss, simple_value 0.03511\n",
      "Timestamp 1612510599139, Iteration 799\n",
      "batches :799 0.03509945206736263\n",
      "Iteration 19: tag train_accuracy, simple_value 0.1653\n",
      "Iteration 19: tag train_loss, simple_value 0.0351\n",
      "Timestamp 1612510599445, Iteration 800\n",
      "batches :800 0.03514482956379652\n",
      "Iteration 20: tag train_accuracy, simple_value 0.16484\n",
      "Iteration 20: tag train_loss, simple_value 0.03514\n",
      "Timestamp 1612510599752, Iteration 801\n",
      "batches :801 0.035117845450128825\n",
      "Iteration 21: tag train_accuracy, simple_value 0.17262\n",
      "Iteration 21: tag train_loss, simple_value 0.03512\n",
      "Timestamp 1612510600060, Iteration 802\n",
      "batches :802 0.03506850654428655\n",
      "Iteration 22: tag train_accuracy, simple_value 0.17685\n",
      "Iteration 22: tag train_loss, simple_value 0.03507\n",
      "Timestamp 1612510600366, Iteration 803\n",
      "batches :803 0.03516305659128272\n",
      "Iteration 23: tag train_accuracy, simple_value 0.17731\n",
      "Iteration 23: tag train_loss, simple_value 0.03516\n",
      "Timestamp 1612510600673, Iteration 804\n",
      "batches :804 0.0351479595216612\n",
      "Iteration 24: tag train_accuracy, simple_value 0.17513\n",
      "Iteration 24: tag train_loss, simple_value 0.03515\n",
      "Timestamp 1612510600979, Iteration 805\n",
      "batches :805 0.035163086503744126\n",
      "Iteration 25: tag train_accuracy, simple_value 0.17375\n",
      "Iteration 25: tag train_loss, simple_value 0.03516\n",
      "Timestamp 1612510601286, Iteration 806\n",
      "batches :806 0.035148296505212784\n",
      "Iteration 26: tag train_accuracy, simple_value 0.17188\n",
      "Iteration 26: tag train_loss, simple_value 0.03515\n",
      "Timestamp 1612510601593, Iteration 807\n",
      "batches :807 0.035161195115910635\n",
      "Iteration 27: tag train_accuracy, simple_value 0.1713\n",
      "Iteration 27: tag train_loss, simple_value 0.03516\n",
      "Timestamp 1612510601901, Iteration 808\n",
      "batches :808 0.035146797741098065\n",
      "Iteration 28: tag train_accuracy, simple_value 0.1702\n",
      "Iteration 28: tag train_loss, simple_value 0.03515\n",
      "Timestamp 1612510602207, Iteration 809\n",
      "batches :809 0.03516837170925634\n",
      "Iteration 29: tag train_accuracy, simple_value 0.16703\n",
      "Iteration 29: tag train_loss, simple_value 0.03517\n",
      "Timestamp 1612510602513, Iteration 810\n",
      "batches :810 0.03513910621404648\n",
      "Iteration 30: tag train_accuracy, simple_value 0.16823\n",
      "Iteration 30: tag train_loss, simple_value 0.03514\n",
      "Timestamp 1612510602820, Iteration 811\n",
      "batches :811 0.03516892235605947\n",
      "Iteration 31: tag train_accuracy, simple_value 0.16784\n",
      "Iteration 31: tag train_loss, simple_value 0.03517\n",
      "Timestamp 1612510603127, Iteration 812\n",
      "batches :812 0.03518605686258525\n",
      "Iteration 32: tag train_accuracy, simple_value 0.1665\n",
      "Iteration 32: tag train_loss, simple_value 0.03519\n",
      "Timestamp 1612510603447, Iteration 813\n",
      "batches :813 0.035178820179267364\n",
      "Iteration 33: tag train_accuracy, simple_value 0.16572\n",
      "Iteration 33: tag train_loss, simple_value 0.03518\n",
      "Timestamp 1612510603753, Iteration 814\n",
      "batches :814 0.03516751922228757\n",
      "Iteration 34: tag train_accuracy, simple_value 0.16728\n",
      "Iteration 34: tag train_loss, simple_value 0.03517\n",
      "Timestamp 1612510604060, Iteration 815\n",
      "batches :815 0.03512219595057624\n",
      "Iteration 35: tag train_accuracy, simple_value 0.17143\n",
      "Iteration 35: tag train_loss, simple_value 0.03512\n",
      "Timestamp 1612510604366, Iteration 816\n",
      "batches :816 0.03511854995869928\n",
      "Iteration 36: tag train_accuracy, simple_value 0.17231\n",
      "Iteration 36: tag train_loss, simple_value 0.03512\n",
      "Timestamp 1612510604673, Iteration 817\n",
      "batches :817 0.035085477841061516\n",
      "Iteration 37: tag train_accuracy, simple_value 0.17399\n",
      "Iteration 37: tag train_loss, simple_value 0.03509\n",
      "Timestamp 1612510604981, Iteration 818\n",
      "batches :818 0.03506202446786981\n",
      "Iteration 38: tag train_accuracy, simple_value 0.17434\n",
      "Iteration 38: tag train_loss, simple_value 0.03506\n",
      "Timestamp 1612510605288, Iteration 819\n",
      "batches :819 0.03502693657691662\n",
      "Iteration 39: tag train_accuracy, simple_value 0.17508\n",
      "Iteration 39: tag train_loss, simple_value 0.03503\n",
      "Timestamp 1612510605593, Iteration 820\n",
      "batches :820 0.035060412064194676\n",
      "Iteration 40: tag train_accuracy, simple_value 0.17461\n",
      "Iteration 40: tag train_loss, simple_value 0.03506\n",
      "Timestamp 1612510605899, Iteration 821\n",
      "batches :821 0.03503739679368531\n",
      "Iteration 41: tag train_accuracy, simple_value 0.17569\n",
      "Iteration 41: tag train_loss, simple_value 0.03504\n",
      "Timestamp 1612510606204, Iteration 822\n",
      "batches :822 0.03505202400542441\n",
      "Iteration 42: tag train_accuracy, simple_value 0.17411\n",
      "Iteration 42: tag train_loss, simple_value 0.03505\n",
      "Timestamp 1612510606511, Iteration 823\n",
      "batches :823 0.035075093597866765\n",
      "Iteration 43: tag train_accuracy, simple_value 0.17188\n",
      "Iteration 43: tag train_loss, simple_value 0.03508\n",
      "Timestamp 1612510606817, Iteration 824\n",
      "batches :824 0.035084186121821404\n",
      "Iteration 44: tag train_accuracy, simple_value 0.17116\n",
      "Iteration 44: tag train_loss, simple_value 0.03508\n",
      "Timestamp 1612510607122, Iteration 825\n",
      "batches :825 0.03505789066354434\n",
      "Iteration 45: tag train_accuracy, simple_value 0.17257\n",
      "Iteration 45: tag train_loss, simple_value 0.03506\n",
      "Timestamp 1612510607428, Iteration 826\n",
      "batches :826 0.03504449833670388\n",
      "Iteration 46: tag train_accuracy, simple_value 0.17391\n",
      "Iteration 46: tag train_loss, simple_value 0.03504\n",
      "Timestamp 1612510607735, Iteration 827\n",
      "batches :827 0.03504522430135849\n",
      "Iteration 47: tag train_accuracy, simple_value 0.1732\n",
      "Iteration 47: tag train_loss, simple_value 0.03505\n",
      "Timestamp 1612510608041, Iteration 828\n",
      "batches :828 0.03506575617939234\n",
      "Iteration 48: tag train_accuracy, simple_value 0.17188\n",
      "Iteration 48: tag train_loss, simple_value 0.03507\n",
      "Timestamp 1612510608361, Iteration 829\n",
      "batches :829 0.035083758511713574\n",
      "Iteration 49: tag train_accuracy, simple_value 0.17092\n",
      "Iteration 49: tag train_loss, simple_value 0.03508\n",
      "Timestamp 1612510608667, Iteration 830\n",
      "batches :830 0.0350510224699974\n",
      "Iteration 50: tag train_accuracy, simple_value 0.17344\n",
      "Iteration 50: tag train_loss, simple_value 0.03505\n",
      "Timestamp 1612510608972, Iteration 831\n",
      "batches :831 0.035057350627931895\n",
      "Iteration 51: tag train_accuracy, simple_value 0.1731\n",
      "Iteration 51: tag train_loss, simple_value 0.03506\n",
      "Timestamp 1612510609278, Iteration 832\n",
      "batches :832 0.035051955483280696\n",
      "Iteration 52: tag train_accuracy, simple_value 0.17127\n",
      "Iteration 52: tag train_loss, simple_value 0.03505\n",
      "Timestamp 1612510609584, Iteration 833\n",
      "batches :833 0.03502125377362629\n",
      "Iteration 53: tag train_accuracy, simple_value 0.17158\n",
      "Iteration 53: tag train_loss, simple_value 0.03502\n",
      "Timestamp 1612510609891, Iteration 834\n",
      "batches :834 0.03500273382222211\n",
      "Iteration 54: tag train_accuracy, simple_value 0.17159\n",
      "Iteration 54: tag train_loss, simple_value 0.035\n",
      "Timestamp 1612510610197, Iteration 835\n",
      "batches :835 0.034998139806769114\n",
      "Iteration 55: tag train_accuracy, simple_value 0.17188\n",
      "Iteration 55: tag train_loss, simple_value 0.035\n",
      "Timestamp 1612510610504, Iteration 836\n",
      "batches :836 0.03499590578888144\n",
      "Iteration 56: tag train_accuracy, simple_value 0.17132\n",
      "Iteration 56: tag train_loss, simple_value 0.035\n",
      "Timestamp 1612510610811, Iteration 837\n",
      "batches :837 0.0349955800593945\n",
      "Iteration 57: tag train_accuracy, simple_value 0.17105\n",
      "Iteration 57: tag train_loss, simple_value 0.035\n",
      "Timestamp 1612510611118, Iteration 838\n",
      "batches :838 0.03497551606389983\n",
      "Iteration 58: tag train_accuracy, simple_value 0.17107\n",
      "Iteration 58: tag train_loss, simple_value 0.03498\n",
      "Timestamp 1612510611425, Iteration 839\n",
      "batches :839 0.03497335049560515\n",
      "Iteration 59: tag train_accuracy, simple_value 0.17135\n",
      "Iteration 59: tag train_loss, simple_value 0.03497\n",
      "Timestamp 1612510611731, Iteration 840\n",
      "batches :840 0.03497300148010254\n",
      "Iteration 60: tag train_accuracy, simple_value 0.17005\n",
      "Iteration 60: tag train_loss, simple_value 0.03497\n",
      "Timestamp 1612510612037, Iteration 841\n",
      "batches :841 0.03497438407579406\n",
      "Iteration 61: tag train_accuracy, simple_value 0.16983\n",
      "Iteration 61: tag train_loss, simple_value 0.03497\n",
      "Timestamp 1612510612343, Iteration 842\n",
      "batches :842 0.03497918004230145\n",
      "Iteration 62: tag train_accuracy, simple_value 0.16809\n",
      "Iteration 62: tag train_loss, simple_value 0.03498\n",
      "Timestamp 1612510612650, Iteration 843\n",
      "batches :843 0.034977272330295475\n",
      "Iteration 63: tag train_accuracy, simple_value 0.16741\n",
      "Iteration 63: tag train_loss, simple_value 0.03498\n",
      "Timestamp 1612510612957, Iteration 844\n",
      "batches :844 0.034953617898281664\n",
      "Iteration 64: tag train_accuracy, simple_value 0.16919\n",
      "Iteration 64: tag train_loss, simple_value 0.03495\n",
      "Timestamp 1612510613278, Iteration 845\n",
      "batches :845 0.03496218919754028\n",
      "Iteration 65: tag train_accuracy, simple_value 0.16851\n",
      "Iteration 65: tag train_loss, simple_value 0.03496\n",
      "Timestamp 1612510613584, Iteration 846\n",
      "batches :846 0.03496029515835372\n",
      "Iteration 66: tag train_accuracy, simple_value 0.16832\n",
      "Iteration 66: tag train_loss, simple_value 0.03496\n",
      "Timestamp 1612510613891, Iteration 847\n",
      "batches :847 0.034942808936336146\n",
      "Iteration 67: tag train_accuracy, simple_value 0.16861\n",
      "Iteration 67: tag train_loss, simple_value 0.03494\n",
      "Timestamp 1612510614196, Iteration 848\n",
      "batches :848 0.034941056798047876\n",
      "Iteration 68: tag train_accuracy, simple_value 0.16843\n",
      "Iteration 68: tag train_loss, simple_value 0.03494\n",
      "Timestamp 1612510614502, Iteration 849\n",
      "batches :849 0.03491482694727787\n",
      "Iteration 69: tag train_accuracy, simple_value 0.16938\n",
      "Iteration 69: tag train_loss, simple_value 0.03491\n",
      "Timestamp 1612510614809, Iteration 850\n",
      "batches :850 0.034921458469969885\n",
      "Iteration 70: tag train_accuracy, simple_value 0.16897\n",
      "Iteration 70: tag train_loss, simple_value 0.03492\n",
      "Timestamp 1612510615117, Iteration 851\n",
      "batches :851 0.03491724272009353\n",
      "Iteration 71: tag train_accuracy, simple_value 0.16857\n",
      "Iteration 71: tag train_loss, simple_value 0.03492\n",
      "Timestamp 1612510615425, Iteration 852\n",
      "batches :852 0.034911850156883396\n",
      "Iteration 72: tag train_accuracy, simple_value 0.16862\n",
      "Iteration 72: tag train_loss, simple_value 0.03491\n",
      "Timestamp 1612510615732, Iteration 853\n",
      "batches :853 0.034909470899872584\n",
      "Iteration 73: tag train_accuracy, simple_value 0.16866\n",
      "Iteration 73: tag train_loss, simple_value 0.03491\n",
      "Timestamp 1612510616038, Iteration 854\n",
      "batches :854 0.03489500435220229\n",
      "Iteration 74: tag train_accuracy, simple_value 0.16871\n",
      "Iteration 74: tag train_loss, simple_value 0.0349\n",
      "Timestamp 1612510616344, Iteration 855\n",
      "batches :855 0.034885053038597104\n",
      "Iteration 75: tag train_accuracy, simple_value 0.16854\n",
      "Iteration 75: tag train_loss, simple_value 0.03489\n",
      "Timestamp 1612510616651, Iteration 856\n",
      "batches :856 0.034865300073043295\n",
      "Iteration 76: tag train_accuracy, simple_value 0.16859\n",
      "Iteration 76: tag train_loss, simple_value 0.03487\n",
      "Timestamp 1612510616957, Iteration 857\n",
      "batches :857 0.03487582209628898\n",
      "Iteration 77: tag train_accuracy, simple_value 0.16883\n",
      "Iteration 77: tag train_loss, simple_value 0.03488\n",
      "Timestamp 1612510617264, Iteration 858\n",
      "batches :858 0.034874216868327215\n",
      "Iteration 78: tag train_accuracy, simple_value 0.16867\n",
      "Iteration 78: tag train_loss, simple_value 0.03487\n",
      "Timestamp 1612510617571, Iteration 859\n",
      "batches :859 0.034874066521849816\n",
      "Iteration 79: tag train_accuracy, simple_value 0.16891\n",
      "Iteration 79: tag train_loss, simple_value 0.03487\n",
      "Timestamp 1612510617877, Iteration 860\n",
      "batches :860 0.03486596392467618\n",
      "Iteration 80: tag train_accuracy, simple_value 0.16875\n",
      "Iteration 80: tag train_loss, simple_value 0.03487\n",
      "Timestamp 1612510618199, Iteration 861\n",
      "batches :861 0.03485107306896904\n",
      "Iteration 81: tag train_accuracy, simple_value 0.16956\n",
      "Iteration 81: tag train_loss, simple_value 0.03485\n",
      "Timestamp 1612510618505, Iteration 862\n",
      "batches :862 0.03485103215022785\n",
      "Iteration 82: tag train_accuracy, simple_value 0.16997\n",
      "Iteration 82: tag train_loss, simple_value 0.03485\n",
      "Timestamp 1612510618812, Iteration 863\n",
      "batches :863 0.034842534462012444\n",
      "Iteration 83: tag train_accuracy, simple_value 0.16943\n",
      "Iteration 83: tag train_loss, simple_value 0.03484\n",
      "Timestamp 1612510619118, Iteration 864\n",
      "batches :864 0.03482543504131692\n",
      "Iteration 84: tag train_accuracy, simple_value 0.16871\n",
      "Iteration 84: tag train_loss, simple_value 0.03483\n",
      "Timestamp 1612510619425, Iteration 865\n",
      "batches :865 0.03480991988497622\n",
      "Iteration 85: tag train_accuracy, simple_value 0.16949\n",
      "Iteration 85: tag train_loss, simple_value 0.03481\n",
      "Timestamp 1612510619732, Iteration 866\n",
      "batches :866 0.034797671922417575\n",
      "Iteration 86: tag train_accuracy, simple_value 0.16933\n",
      "Iteration 86: tag train_loss, simple_value 0.0348\n",
      "Timestamp 1612510620039, Iteration 867\n",
      "batches :867 0.03477967517643139\n",
      "Iteration 87: tag train_accuracy, simple_value 0.169\n",
      "Iteration 87: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510620345, Iteration 868\n",
      "batches :868 0.03477416737851771\n",
      "Iteration 88: tag train_accuracy, simple_value 0.16921\n",
      "Iteration 88: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510620652, Iteration 869\n",
      "batches :869 0.034774870391976964\n",
      "Iteration 89: tag train_accuracy, simple_value 0.16836\n",
      "Iteration 89: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510620958, Iteration 870\n",
      "batches :870 0.034759398301442465\n",
      "Iteration 90: tag train_accuracy, simple_value 0.16788\n",
      "Iteration 90: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510621265, Iteration 871\n",
      "batches :871 0.034781187158692016\n",
      "Iteration 91: tag train_accuracy, simple_value 0.16724\n",
      "Iteration 91: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510621571, Iteration 872\n",
      "batches :872 0.0347764160403091\n",
      "Iteration 92: tag train_accuracy, simple_value 0.1678\n",
      "Iteration 92: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510621879, Iteration 873\n",
      "batches :873 0.03478809566267075\n",
      "Iteration 93: tag train_accuracy, simple_value 0.16751\n",
      "Iteration 93: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510622186, Iteration 874\n",
      "batches :874 0.03478639228071304\n",
      "Iteration 94: tag train_accuracy, simple_value 0.16705\n",
      "Iteration 94: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510622492, Iteration 875\n",
      "batches :875 0.03479059675806447\n",
      "Iteration 95: tag train_accuracy, simple_value 0.16645\n",
      "Iteration 95: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510622799, Iteration 876\n",
      "batches :876 0.034781664687519274\n",
      "Iteration 96: tag train_accuracy, simple_value 0.16748\n",
      "Iteration 96: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510623120, Iteration 877\n",
      "batches :877 0.034785680165610365\n",
      "Iteration 97: tag train_accuracy, simple_value 0.16704\n",
      "Iteration 97: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510623426, Iteration 878\n",
      "batches :878 0.0347743138138737\n",
      "Iteration 98: tag train_accuracy, simple_value 0.16757\n",
      "Iteration 98: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510623734, Iteration 879\n",
      "batches :879 0.03476619633911836\n",
      "Iteration 99: tag train_accuracy, simple_value 0.16809\n",
      "Iteration 99: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510624041, Iteration 880\n",
      "batches :880 0.034771584495902064\n",
      "Iteration 100: tag train_accuracy, simple_value 0.16844\n",
      "Iteration 100: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510624348, Iteration 881\n",
      "batches :881 0.03477526876595941\n",
      "Iteration 101: tag train_accuracy, simple_value 0.16878\n",
      "Iteration 101: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510624656, Iteration 882\n",
      "batches :882 0.03479153421871802\n",
      "Iteration 102: tag train_accuracy, simple_value 0.1682\n",
      "Iteration 102: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510624962, Iteration 883\n",
      "batches :883 0.0347918944017401\n",
      "Iteration 103: tag train_accuracy, simple_value 0.16717\n",
      "Iteration 103: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510625269, Iteration 884\n",
      "batches :884 0.03478223414948353\n",
      "Iteration 104: tag train_accuracy, simple_value 0.16767\n",
      "Iteration 104: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510625576, Iteration 885\n",
      "batches :885 0.03478351182171276\n",
      "Iteration 105: tag train_accuracy, simple_value 0.16667\n",
      "Iteration 105: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510625884, Iteration 886\n",
      "batches :886 0.03478493197065479\n",
      "Iteration 106: tag train_accuracy, simple_value 0.16686\n",
      "Iteration 106: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510626190, Iteration 887\n",
      "batches :887 0.034785978426443084\n",
      "Iteration 107: tag train_accuracy, simple_value 0.16691\n",
      "Iteration 107: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510626499, Iteration 888\n",
      "batches :888 0.03479155332401947\n",
      "Iteration 108: tag train_accuracy, simple_value 0.16681\n",
      "Iteration 108: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510626807, Iteration 889\n",
      "batches :889 0.03479532794941456\n",
      "Iteration 109: tag train_accuracy, simple_value 0.16757\n",
      "Iteration 109: tag train_loss, simple_value 0.0348\n",
      "Timestamp 1612510627113, Iteration 890\n",
      "batches :890 0.03477561582218517\n",
      "Iteration 110: tag train_accuracy, simple_value 0.16875\n",
      "Iteration 110: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510627420, Iteration 891\n",
      "batches :891 0.03478451704119777\n",
      "Iteration 111: tag train_accuracy, simple_value 0.16864\n",
      "Iteration 111: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510627728, Iteration 892\n",
      "batches :892 0.0347846250182816\n",
      "Iteration 112: tag train_accuracy, simple_value 0.16797\n",
      "Iteration 112: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510628047, Iteration 893\n",
      "batches :893 0.034790858725267174\n",
      "Iteration 113: tag train_accuracy, simple_value 0.16773\n",
      "Iteration 113: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510628354, Iteration 894\n",
      "batches :894 0.034790084386865296\n",
      "Iteration 114: tag train_accuracy, simple_value 0.16735\n",
      "Iteration 114: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510628660, Iteration 895\n",
      "batches :895 0.034795362340367356\n",
      "Iteration 115: tag train_accuracy, simple_value 0.16671\n",
      "Iteration 115: tag train_loss, simple_value 0.0348\n",
      "Timestamp 1612510628970, Iteration 896\n",
      "batches :896 0.03480077496376531\n",
      "Iteration 116: tag train_accuracy, simple_value 0.16635\n",
      "Iteration 116: tag train_loss, simple_value 0.0348\n",
      "Timestamp 1612510629275, Iteration 897\n",
      "batches :897 0.03479241309130294\n",
      "Iteration 117: tag train_accuracy, simple_value 0.16627\n",
      "Iteration 117: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510629583, Iteration 898\n",
      "batches :898 0.03478755262064732\n",
      "Iteration 118: tag train_accuracy, simple_value 0.16645\n",
      "Iteration 118: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510629891, Iteration 899\n",
      "batches :899 0.0347910000308722\n",
      "Iteration 119: tag train_accuracy, simple_value 0.16649\n",
      "Iteration 119: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510630199, Iteration 900\n",
      "batches :900 0.034777962633719045\n",
      "Iteration 120: tag train_accuracy, simple_value 0.16693\n",
      "Iteration 120: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510630506, Iteration 901\n",
      "batches :901 0.034774058921770615\n",
      "Iteration 121: tag train_accuracy, simple_value 0.16671\n",
      "Iteration 121: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510630813, Iteration 902\n",
      "batches :902 0.0348084527144178\n",
      "Iteration 122: tag train_accuracy, simple_value 0.16598\n",
      "Iteration 122: tag train_loss, simple_value 0.03481\n",
      "Timestamp 1612510631121, Iteration 903\n",
      "batches :903 0.03479239732269349\n",
      "Iteration 123: tag train_accuracy, simple_value 0.16654\n",
      "Iteration 123: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510631430, Iteration 904\n",
      "batches :904 0.034780381217358575\n",
      "Iteration 124: tag train_accuracy, simple_value 0.16658\n",
      "Iteration 124: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510631737, Iteration 905\n",
      "batches :905 0.03477857306599617\n",
      "Iteration 125: tag train_accuracy, simple_value 0.16612\n",
      "Iteration 125: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510632044, Iteration 906\n",
      "batches :906 0.03479416771895356\n",
      "Iteration 126: tag train_accuracy, simple_value 0.1653\n",
      "Iteration 126: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510632350, Iteration 907\n",
      "batches :907 0.03479330954471911\n",
      "Iteration 127: tag train_accuracy, simple_value 0.16499\n",
      "Iteration 127: tag train_loss, simple_value 0.03479\n",
      "Timestamp 1612510632657, Iteration 908\n",
      "batches :908 0.034776301705278456\n",
      "Iteration 128: tag train_accuracy, simple_value 0.16553\n",
      "Iteration 128: tag train_loss, simple_value 0.03478\n",
      "Timestamp 1612510632978, Iteration 909\n",
      "batches :909 0.03477195627236551\n",
      "Iteration 129: tag train_accuracy, simple_value 0.16533\n",
      "Iteration 129: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510633284, Iteration 910\n",
      "batches :910 0.034757177932904317\n",
      "Iteration 130: tag train_accuracy, simple_value 0.16502\n",
      "Iteration 130: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510633591, Iteration 911\n",
      "batches :911 0.03476417980348791\n",
      "Iteration 131: tag train_accuracy, simple_value 0.16472\n",
      "Iteration 131: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510633898, Iteration 912\n",
      "batches :912 0.03476248063485731\n",
      "Iteration 132: tag train_accuracy, simple_value 0.16477\n",
      "Iteration 132: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510634206, Iteration 913\n",
      "batches :913 0.034761897575362286\n",
      "Iteration 133: tag train_accuracy, simple_value 0.16447\n",
      "Iteration 133: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510634512, Iteration 914\n",
      "batches :914 0.034757905125395575\n",
      "Iteration 134: tag train_accuracy, simple_value 0.16453\n",
      "Iteration 134: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510634819, Iteration 915\n",
      "batches :915 0.03474462432441888\n",
      "Iteration 135: tag train_accuracy, simple_value 0.16447\n",
      "Iteration 135: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510635126, Iteration 916\n",
      "batches :916 0.034752539961653596\n",
      "Iteration 136: tag train_accuracy, simple_value 0.16406\n",
      "Iteration 136: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510635433, Iteration 917\n",
      "batches :917 0.03474341341070015\n",
      "Iteration 137: tag train_accuracy, simple_value 0.16412\n",
      "Iteration 137: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510635745, Iteration 918\n",
      "batches :918 0.0347441957210717\n",
      "Iteration 138: tag train_accuracy, simple_value 0.16406\n",
      "Iteration 138: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510636053, Iteration 919\n",
      "batches :919 0.0347401320987897\n",
      "Iteration 139: tag train_accuracy, simple_value 0.16378\n",
      "Iteration 139: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510636361, Iteration 920\n",
      "batches :920 0.0347464594990015\n",
      "Iteration 140: tag train_accuracy, simple_value 0.16384\n",
      "Iteration 140: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510636667, Iteration 921\n",
      "batches :921 0.03473992606109761\n",
      "Iteration 141: tag train_accuracy, simple_value 0.16401\n",
      "Iteration 141: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510636974, Iteration 922\n",
      "batches :922 0.03475748863018734\n",
      "Iteration 142: tag train_accuracy, simple_value 0.16318\n",
      "Iteration 142: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510637280, Iteration 923\n",
      "batches :923 0.034752233097186454\n",
      "Iteration 143: tag train_accuracy, simple_value 0.16292\n",
      "Iteration 143: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510637586, Iteration 924\n",
      "batches :924 0.03474723840028875\n",
      "Iteration 144: tag train_accuracy, simple_value 0.16309\n",
      "Iteration 144: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510637906, Iteration 925\n",
      "batches :925 0.03475625139885935\n",
      "Iteration 145: tag train_accuracy, simple_value 0.16293\n",
      "Iteration 145: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510638212, Iteration 926\n",
      "batches :926 0.03474656025534623\n",
      "Iteration 146: tag train_accuracy, simple_value 0.16267\n",
      "Iteration 146: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510638520, Iteration 927\n",
      "batches :927 0.034753934942743404\n",
      "Iteration 147: tag train_accuracy, simple_value 0.16273\n",
      "Iteration 147: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510638826, Iteration 928\n",
      "batches :928 0.034753592760377636\n",
      "Iteration 148: tag train_accuracy, simple_value 0.16227\n",
      "Iteration 148: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510639133, Iteration 929\n",
      "batches :929 0.03475018415675067\n",
      "Iteration 149: tag train_accuracy, simple_value 0.16233\n",
      "Iteration 149: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510639440, Iteration 930\n",
      "batches :930 0.03475014820694924\n",
      "Iteration 150: tag train_accuracy, simple_value 0.16229\n",
      "Iteration 150: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510639745, Iteration 931\n",
      "batches :931 0.03475416579980724\n",
      "Iteration 151: tag train_accuracy, simple_value 0.16225\n",
      "Iteration 151: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510640051, Iteration 932\n",
      "batches :932 0.03475576965138316\n",
      "Iteration 152: tag train_accuracy, simple_value 0.1618\n",
      "Iteration 152: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510640358, Iteration 933\n",
      "batches :933 0.03475651700122684\n",
      "Iteration 153: tag train_accuracy, simple_value 0.16136\n",
      "Iteration 153: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510640674, Iteration 934\n",
      "batches :934 0.03474822294499193\n",
      "Iteration 154: tag train_accuracy, simple_value 0.16153\n",
      "Iteration 154: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510640982, Iteration 935\n",
      "batches :935 0.034756375296461965\n",
      "Iteration 155: tag train_accuracy, simple_value 0.16119\n",
      "Iteration 155: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510641290, Iteration 936\n",
      "batches :936 0.03475720877162157\n",
      "Iteration 156: tag train_accuracy, simple_value 0.16096\n",
      "Iteration 156: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510641596, Iteration 937\n",
      "batches :937 0.03477098648050788\n",
      "Iteration 157: tag train_accuracy, simple_value 0.16063\n",
      "Iteration 157: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510641905, Iteration 938\n",
      "batches :938 0.03476609669248514\n",
      "Iteration 158: tag train_accuracy, simple_value 0.161\n",
      "Iteration 158: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510642210, Iteration 939\n",
      "batches :939 0.03476509709590636\n",
      "Iteration 159: tag train_accuracy, simple_value 0.16136\n",
      "Iteration 159: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510642515, Iteration 940\n",
      "batches :940 0.03476897168438882\n",
      "Iteration 160: tag train_accuracy, simple_value 0.16162\n",
      "Iteration 160: tag train_loss, simple_value 0.03477\n",
      "Timestamp 1612510642835, Iteration 941\n",
      "batches :941 0.03475706508348447\n",
      "Iteration 161: tag train_accuracy, simple_value 0.16188\n",
      "Iteration 161: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510643140, Iteration 942\n",
      "batches :942 0.03475844436957513\n",
      "Iteration 162: tag train_accuracy, simple_value 0.16213\n",
      "Iteration 162: tag train_loss, simple_value 0.03476\n",
      "Timestamp 1612510643445, Iteration 943\n",
      "batches :943 0.03474934785369715\n",
      "Iteration 163: tag train_accuracy, simple_value 0.16248\n",
      "Iteration 163: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510643753, Iteration 944\n",
      "batches :944 0.0347517453270351\n",
      "Iteration 164: tag train_accuracy, simple_value 0.16197\n",
      "Iteration 164: tag train_loss, simple_value 0.03475\n",
      "Timestamp 1612510644061, Iteration 945\n",
      "batches :945 0.03474359552968632\n",
      "Iteration 165: tag train_accuracy, simple_value 0.16212\n",
      "Iteration 165: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510644369, Iteration 946\n",
      "batches :946 0.03473623947476048\n",
      "Iteration 166: tag train_accuracy, simple_value 0.1619\n",
      "Iteration 166: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510644675, Iteration 947\n",
      "batches :947 0.03474316596538721\n",
      "Iteration 167: tag train_accuracy, simple_value 0.16158\n",
      "Iteration 167: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510644983, Iteration 948\n",
      "batches :948 0.03473510076513603\n",
      "Iteration 168: tag train_accuracy, simple_value 0.16137\n",
      "Iteration 168: tag train_loss, simple_value 0.03474\n",
      "Timestamp 1612510645290, Iteration 949\n",
      "batches :949 0.034726431626363616\n",
      "Iteration 169: tag train_accuracy, simple_value 0.1617\n",
      "Iteration 169: tag train_loss, simple_value 0.03473\n",
      "Timestamp 1612510645597, Iteration 950\n",
      "batches :950 0.03472510151126806\n",
      "Iteration 170: tag train_accuracy, simple_value 0.16149\n",
      "Iteration 170: tag train_loss, simple_value 0.03473\n",
      "Timestamp 1612510645906, Iteration 951\n",
      "batches :951 0.034724826526920695\n",
      "Iteration 171: tag train_accuracy, simple_value 0.16128\n",
      "Iteration 171: tag train_loss, simple_value 0.03472\n",
      "Timestamp 1612510646213, Iteration 952\n",
      "batches :952 0.0347277462958943\n",
      "Iteration 172: tag train_accuracy, simple_value 0.16179\n",
      "Iteration 172: tag train_loss, simple_value 0.03473\n",
      "Timestamp 1612510646520, Iteration 953\n",
      "batches :953 0.0347225124264039\n",
      "Iteration 173: tag train_accuracy, simple_value 0.16203\n",
      "Iteration 173: tag train_loss, simple_value 0.03472\n",
      "Timestamp 1612510646828, Iteration 954\n",
      "batches :954 0.03471216124792894\n",
      "Iteration 174: tag train_accuracy, simple_value 0.16236\n",
      "Iteration 174: tag train_loss, simple_value 0.03471\n",
      "Timestamp 1612510647135, Iteration 955\n",
      "batches :955 0.03470845554556166\n",
      "Iteration 175: tag train_accuracy, simple_value 0.16205\n",
      "Iteration 175: tag train_loss, simple_value 0.03471\n",
      "Timestamp 1612510647442, Iteration 956\n",
      "batches :956 0.034703107889403\n",
      "Iteration 176: tag train_accuracy, simple_value 0.1622\n",
      "Iteration 176: tag train_loss, simple_value 0.0347\n",
      "Timestamp 1612510647765, Iteration 957\n",
      "batches :957 0.03470050859249244\n",
      "Iteration 177: tag train_accuracy, simple_value 0.16243\n",
      "Iteration 177: tag train_loss, simple_value 0.0347\n",
      "Timestamp 1612510648073, Iteration 958\n",
      "batches :958 0.034691326110885386\n",
      "Iteration 178: tag train_accuracy, simple_value 0.16257\n",
      "Iteration 178: tag train_loss, simple_value 0.03469\n",
      "Timestamp 1612510648379, Iteration 959\n",
      "batches :959 0.034691647839779294\n",
      "Iteration 179: tag train_accuracy, simple_value 0.1628\n",
      "Iteration 179: tag train_loss, simple_value 0.03469\n",
      "Timestamp 1612510648686, Iteration 960\n",
      "batches :960 0.034690887874199286\n",
      "Iteration 180: tag train_accuracy, simple_value 0.16276\n",
      "Iteration 180: tag train_loss, simple_value 0.03469\n",
      "Timestamp 1612510648994, Iteration 961\n",
      "batches :961 0.03467897416313709\n",
      "Iteration 181: tag train_accuracy, simple_value 0.16324\n",
      "Iteration 181: tag train_loss, simple_value 0.03468\n",
      "Timestamp 1612510649301, Iteration 962\n",
      "batches :962 0.03466980020096014\n",
      "Iteration 182: tag train_accuracy, simple_value 0.16363\n",
      "Iteration 182: tag train_loss, simple_value 0.03467\n",
      "Timestamp 1612510649609, Iteration 963\n",
      "batches :963 0.03466848387825684\n",
      "Iteration 183: tag train_accuracy, simple_value 0.16385\n",
      "Iteration 183: tag train_loss, simple_value 0.03467\n",
      "Timestamp 1612510649915, Iteration 964\n",
      "batches :964 0.03466586999433196\n",
      "Iteration 184: tag train_accuracy, simple_value 0.16355\n",
      "Iteration 184: tag train_loss, simple_value 0.03467\n",
      "Timestamp 1612510650221, Iteration 965\n",
      "batches :965 0.03466431224668348\n",
      "Iteration 185: tag train_accuracy, simple_value 0.16343\n",
      "Iteration 185: tag train_loss, simple_value 0.03466\n",
      "Timestamp 1612510650527, Iteration 966\n",
      "batches :966 0.03465574767480614\n",
      "Iteration 186: tag train_accuracy, simple_value 0.16347\n",
      "Iteration 186: tag train_loss, simple_value 0.03466\n",
      "Timestamp 1612510650834, Iteration 967\n",
      "batches :967 0.034655293320270786\n",
      "Iteration 187: tag train_accuracy, simple_value 0.16344\n",
      "Iteration 187: tag train_loss, simple_value 0.03466\n",
      "Timestamp 1612510651141, Iteration 968\n",
      "batches :968 0.03465344312977284\n",
      "Iteration 188: tag train_accuracy, simple_value 0.16348\n",
      "Iteration 188: tag train_loss, simple_value 0.03465\n",
      "Timestamp 1612510651449, Iteration 969\n",
      "batches :969 0.034652214162248784\n",
      "Iteration 189: tag train_accuracy, simple_value 0.16344\n",
      "Iteration 189: tag train_loss, simple_value 0.03465\n",
      "Timestamp 1612510651757, Iteration 970\n",
      "batches :970 0.03464839715314539\n",
      "Iteration 190: tag train_accuracy, simple_value 0.16357\n",
      "Iteration 190: tag train_loss, simple_value 0.03465\n",
      "Timestamp 1612510652064, Iteration 971\n",
      "batches :971 0.034647700401188815\n",
      "Iteration 191: tag train_accuracy, simple_value 0.16378\n",
      "Iteration 191: tag train_loss, simple_value 0.03465\n",
      "Timestamp 1612510652373, Iteration 972\n",
      "batches :972 0.034638433038101844\n",
      "Iteration 192: tag train_accuracy, simple_value 0.16406\n",
      "Iteration 192: tag train_loss, simple_value 0.03464\n",
      "Timestamp 1612510652693, Iteration 973\n",
      "batches :973 0.03463455739339399\n",
      "Iteration 193: tag train_accuracy, simple_value 0.16435\n",
      "Iteration 193: tag train_loss, simple_value 0.03463\n",
      "Timestamp 1612510653000, Iteration 974\n",
      "batches :974 0.03462927114487309\n",
      "Iteration 194: tag train_accuracy, simple_value 0.16471\n",
      "Iteration 194: tag train_loss, simple_value 0.03463\n",
      "Timestamp 1612510653307, Iteration 975\n",
      "batches :975 0.03462608410761907\n",
      "Iteration 195: tag train_accuracy, simple_value 0.16498\n",
      "Iteration 195: tag train_loss, simple_value 0.03463\n",
      "Timestamp 1612510653616, Iteration 976\n",
      "batches :976 0.034620661687638075\n",
      "Iteration 196: tag train_accuracy, simple_value 0.1651\n",
      "Iteration 196: tag train_loss, simple_value 0.03462\n",
      "Timestamp 1612510653922, Iteration 977\n",
      "batches :977 0.03461251303856143\n",
      "Iteration 197: tag train_accuracy, simple_value 0.16521\n",
      "Iteration 197: tag train_loss, simple_value 0.03461\n",
      "Timestamp 1612510654230, Iteration 978\n",
      "batches :978 0.034613283144103155\n",
      "Iteration 198: tag train_accuracy, simple_value 0.16517\n",
      "Iteration 198: tag train_loss, simple_value 0.03461\n",
      "Timestamp 1612510654538, Iteration 979\n",
      "batches :979 0.03460783468344104\n",
      "Iteration 199: tag train_accuracy, simple_value 0.16504\n",
      "Iteration 199: tag train_loss, simple_value 0.03461\n",
      "Timestamp 1612510654845, Iteration 980\n",
      "batches :980 0.03460204653441906\n",
      "Iteration 200: tag train_accuracy, simple_value 0.16492\n",
      "Iteration 200: tag train_loss, simple_value 0.0346\n",
      "Timestamp 1612510655151, Iteration 981\n",
      "batches :981 0.03460035360052218\n",
      "Iteration 201: tag train_accuracy, simple_value 0.1648\n",
      "Iteration 201: tag train_loss, simple_value 0.0346\n",
      "Timestamp 1612510655457, Iteration 982\n",
      "batches :982 0.03459592628301961\n",
      "Iteration 202: tag train_accuracy, simple_value 0.16499\n",
      "Iteration 202: tag train_loss, simple_value 0.0346\n",
      "Timestamp 1612510655764, Iteration 983\n",
      "batches :983 0.03458908244306818\n",
      "Iteration 203: tag train_accuracy, simple_value 0.16502\n",
      "Iteration 203: tag train_loss, simple_value 0.03459\n",
      "Timestamp 1612510656073, Iteration 984\n",
      "batches :984 0.03458726071916959\n",
      "Iteration 204: tag train_accuracy, simple_value 0.16506\n",
      "Iteration 204: tag train_loss, simple_value 0.03459\n",
      "Timestamp 1612510656380, Iteration 985\n",
      "batches :985 0.03458648443948932\n",
      "Iteration 205: tag train_accuracy, simple_value 0.16471\n",
      "Iteration 205: tag train_loss, simple_value 0.03459\n",
      "Timestamp 1612510656688, Iteration 986\n",
      "batches :986 0.03458334780433803\n",
      "Iteration 206: tag train_accuracy, simple_value 0.16467\n",
      "Iteration 206: tag train_loss, simple_value 0.03458\n",
      "Timestamp 1612510656994, Iteration 987\n",
      "batches :987 0.0345786857072282\n",
      "Iteration 207: tag train_accuracy, simple_value 0.1647\n",
      "Iteration 207: tag train_loss, simple_value 0.03458\n",
      "Timestamp 1612510657299, Iteration 988\n",
      "batches :988 0.034577797095362954\n",
      "Iteration 208: tag train_accuracy, simple_value 0.16504\n",
      "Iteration 208: tag train_loss, simple_value 0.03458\n",
      "Timestamp 1612510657617, Iteration 989\n",
      "batches :989 0.034575563031092785\n",
      "Iteration 209: tag train_accuracy, simple_value 0.16485\n",
      "Iteration 209: tag train_loss, simple_value 0.03458\n",
      "Timestamp 1612510657922, Iteration 990\n",
      "batches :990 0.03455880330432029\n",
      "Iteration 210: tag train_accuracy, simple_value 0.16533\n",
      "Iteration 210: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510658227, Iteration 991\n",
      "batches :991 0.03455639002023715\n",
      "Iteration 211: tag train_accuracy, simple_value 0.16536\n",
      "Iteration 211: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510658534, Iteration 992\n",
      "batches :992 0.03455644636854248\n",
      "Iteration 212: tag train_accuracy, simple_value 0.16502\n",
      "Iteration 212: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510658839, Iteration 993\n",
      "batches :993 0.034555637045925215\n",
      "Iteration 213: tag train_accuracy, simple_value 0.16505\n",
      "Iteration 213: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510659145, Iteration 994\n",
      "batches :994 0.03455645827768005\n",
      "Iteration 214: tag train_accuracy, simple_value 0.16487\n",
      "Iteration 214: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510659451, Iteration 995\n",
      "batches :995 0.03456408315619757\n",
      "Iteration 215: tag train_accuracy, simple_value 0.16446\n",
      "Iteration 215: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510659756, Iteration 996\n",
      "batches :996 0.034566129109373796\n",
      "Iteration 216: tag train_accuracy, simple_value 0.1645\n",
      "Iteration 216: tag train_loss, simple_value 0.03457\n",
      "Timestamp 1612510660062, Iteration 997\n",
      "batches :997 0.034564594742477216\n",
      "Iteration 217: tag train_accuracy, simple_value 0.16496\n",
      "Iteration 217: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510660368, Iteration 998\n",
      "batches :998 0.0345572203124335\n",
      "Iteration 218: tag train_accuracy, simple_value 0.16499\n",
      "Iteration 218: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510660675, Iteration 999\n",
      "batches :999 0.03455675685922849\n",
      "Iteration 219: tag train_accuracy, simple_value 0.16481\n",
      "Iteration 219: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510660983, Iteration 1000\n",
      "batches :1000 0.03454882959750566\n",
      "Iteration 220: tag train_accuracy, simple_value 0.16513\n",
      "Iteration 220: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510661288, Iteration 1001\n",
      "batches :1001 0.03455027015837609\n",
      "Iteration 221: tag train_accuracy, simple_value 0.16488\n",
      "Iteration 221: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510661594, Iteration 1002\n",
      "batches :1002 0.0345526412297208\n",
      "Iteration 222: tag train_accuracy, simple_value 0.16484\n",
      "Iteration 222: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510661900, Iteration 1003\n",
      "batches :1003 0.03455562273148999\n",
      "Iteration 223: tag train_accuracy, simple_value 0.1648\n",
      "Iteration 223: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510662205, Iteration 1004\n",
      "batches :1004 0.03455944224593362\n",
      "Iteration 224: tag train_accuracy, simple_value 0.16455\n",
      "Iteration 224: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510662523, Iteration 1005\n",
      "batches :1005 0.034564984656042526\n",
      "Iteration 225: tag train_accuracy, simple_value 0.16472\n",
      "Iteration 225: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510662829, Iteration 1006\n",
      "batches :1006 0.03456530273290335\n",
      "Iteration 226: tag train_accuracy, simple_value 0.16448\n",
      "Iteration 226: tag train_loss, simple_value 0.03457\n",
      "Timestamp 1612510663134, Iteration 1007\n",
      "batches :1007 0.03456070262626929\n",
      "Iteration 227: tag train_accuracy, simple_value 0.16437\n",
      "Iteration 227: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510663441, Iteration 1008\n",
      "batches :1008 0.03455785853102019\n",
      "Iteration 228: tag train_accuracy, simple_value 0.16434\n",
      "Iteration 228: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510663746, Iteration 1009\n",
      "batches :1009 0.03455472233253796\n",
      "Iteration 229: tag train_accuracy, simple_value 0.16478\n",
      "Iteration 229: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510664051, Iteration 1010\n",
      "batches :1010 0.03454942416561686\n",
      "Iteration 230: tag train_accuracy, simple_value 0.16515\n",
      "Iteration 230: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510664357, Iteration 1011\n",
      "batches :1011 0.03455010127453577\n",
      "Iteration 231: tag train_accuracy, simple_value 0.16491\n",
      "Iteration 231: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510664662, Iteration 1012\n",
      "batches :1012 0.03454865055993713\n",
      "Iteration 232: tag train_accuracy, simple_value 0.16514\n",
      "Iteration 232: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510664969, Iteration 1013\n",
      "batches :1013 0.03455385313141499\n",
      "Iteration 233: tag train_accuracy, simple_value 0.1651\n",
      "Iteration 233: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510665275, Iteration 1014\n",
      "batches :1014 0.03455036418496543\n",
      "Iteration 234: tag train_accuracy, simple_value 0.16506\n",
      "Iteration 234: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510665580, Iteration 1015\n",
      "batches :1015 0.03455849587917328\n",
      "Iteration 235: tag train_accuracy, simple_value 0.16489\n",
      "Iteration 235: tag train_loss, simple_value 0.03456\n",
      "Timestamp 1612510665887, Iteration 1016\n",
      "batches :1016 0.034553160938292235\n",
      "Iteration 236: tag train_accuracy, simple_value 0.16499\n",
      "Iteration 236: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510666192, Iteration 1017\n",
      "batches :1017 0.03454769118237093\n",
      "Iteration 237: tag train_accuracy, simple_value 0.16528\n",
      "Iteration 237: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510666498, Iteration 1018\n",
      "batches :1018 0.034554715135267804\n",
      "Iteration 238: tag train_accuracy, simple_value 0.16518\n",
      "Iteration 238: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510666803, Iteration 1019\n",
      "batches :1019 0.03455414987757615\n",
      "Iteration 239: tag train_accuracy, simple_value 0.16501\n",
      "Iteration 239: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510667109, Iteration 1020\n",
      "batches :1020 0.03454524812599023\n",
      "Iteration 240: tag train_accuracy, simple_value 0.16517\n",
      "Iteration 240: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510667428, Iteration 1021\n",
      "batches :1021 0.03454684260537021\n",
      "Iteration 241: tag train_accuracy, simple_value 0.16539\n",
      "Iteration 241: tag train_loss, simple_value 0.03455\n",
      "Timestamp 1612510667734, Iteration 1022\n",
      "batches :1022 0.03454011036775821\n",
      "Iteration 242: tag train_accuracy, simple_value 0.16535\n",
      "Iteration 242: tag train_loss, simple_value 0.03454\n",
      "Timestamp 1612510668039, Iteration 1023\n",
      "batches :1023 0.03454332111916915\n",
      "Iteration 243: tag train_accuracy, simple_value 0.16525\n",
      "Iteration 243: tag train_loss, simple_value 0.03454\n",
      "Timestamp 1612510668346, Iteration 1024\n",
      "batches :1024 0.03454122764104214\n",
      "Iteration 244: tag train_accuracy, simple_value 0.16528\n",
      "Iteration 244: tag train_loss, simple_value 0.03454\n",
      "Timestamp 1612510668652, Iteration 1025\n",
      "batches :1025 0.034534339941277796\n",
      "Iteration 245: tag train_accuracy, simple_value 0.16569\n",
      "Iteration 245: tag train_loss, simple_value 0.03453\n",
      "Timestamp 1612510668957, Iteration 1026\n",
      "batches :1026 0.03453184597618211\n",
      "Iteration 246: tag train_accuracy, simple_value 0.16578\n",
      "Iteration 246: tag train_loss, simple_value 0.03453\n",
      "Timestamp 1612510669263, Iteration 1027\n",
      "batches :1027 0.034531479769269464\n",
      "Iteration 247: tag train_accuracy, simple_value 0.1658\n",
      "Iteration 247: tag train_loss, simple_value 0.03453\n",
      "Timestamp 1612510669569, Iteration 1028\n",
      "batches :1028 0.03452765041842095\n",
      "Iteration 248: tag train_accuracy, simple_value 0.16602\n",
      "Iteration 248: tag train_loss, simple_value 0.03453\n",
      "Timestamp 1612510669874, Iteration 1029\n",
      "batches :1029 0.03452301622334733\n",
      "Iteration 249: tag train_accuracy, simple_value 0.16623\n",
      "Iteration 249: tag train_loss, simple_value 0.03452\n",
      "Timestamp 1612510670178, Iteration 1030\n",
      "batches :1030 0.03452122786641121\n",
      "Iteration 250: tag train_accuracy, simple_value 0.16625\n",
      "Iteration 250: tag train_loss, simple_value 0.03452\n",
      "Timestamp 1612510670484, Iteration 1031\n",
      "batches :1031 0.03451770147776224\n",
      "Iteration 251: tag train_accuracy, simple_value 0.16615\n",
      "Iteration 251: tag train_loss, simple_value 0.03452\n",
      "Timestamp 1612510670790, Iteration 1032\n",
      "batches :1032 0.03451199869492224\n",
      "Iteration 252: tag train_accuracy, simple_value 0.16605\n",
      "Iteration 252: tag train_loss, simple_value 0.03451\n",
      "Timestamp 1612510671096, Iteration 1033\n",
      "batches :1033 0.034510061822154305\n",
      "Iteration 253: tag train_accuracy, simple_value 0.16613\n",
      "Iteration 253: tag train_loss, simple_value 0.03451\n",
      "Timestamp 1612510671402, Iteration 1034\n",
      "batches :1034 0.034506843520665735\n",
      "Iteration 254: tag train_accuracy, simple_value 0.16615\n",
      "Iteration 254: tag train_loss, simple_value 0.03451\n",
      "Timestamp 1612510671707, Iteration 1035\n",
      "batches :1035 0.0344995510198322\n",
      "Iteration 255: tag train_accuracy, simple_value 0.16648\n",
      "Iteration 255: tag train_loss, simple_value 0.0345\n",
      "Timestamp 1612510672013, Iteration 1036\n",
      "batches :1036 0.03449694506707601\n",
      "Iteration 256: tag train_accuracy, simple_value 0.1665\n",
      "Iteration 256: tag train_loss, simple_value 0.0345\n",
      "Timestamp 1612510672333, Iteration 1037\n",
      "batches :1037 0.03449751952742788\n",
      "Iteration 257: tag train_accuracy, simple_value 0.16683\n",
      "Iteration 257: tag train_loss, simple_value 0.0345\n",
      "Timestamp 1612510672638, Iteration 1038\n",
      "batches :1038 0.034491292323715006\n",
      "Iteration 258: tag train_accuracy, simple_value 0.16703\n",
      "Iteration 258: tag train_loss, simple_value 0.03449\n",
      "Timestamp 1612510672944, Iteration 1039\n",
      "batches :1039 0.03448957719743022\n",
      "Iteration 259: tag train_accuracy, simple_value 0.16717\n",
      "Iteration 259: tag train_loss, simple_value 0.03449\n",
      "Timestamp 1612510673250, Iteration 1040\n",
      "batches :1040 0.034479918560156454\n",
      "Iteration 260: tag train_accuracy, simple_value 0.16725\n",
      "Iteration 260: tag train_loss, simple_value 0.03448\n",
      "Timestamp 1612510673555, Iteration 1041\n",
      "batches :1041 0.03447361694384809\n",
      "Iteration 261: tag train_accuracy, simple_value 0.16733\n",
      "Iteration 261: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510673861, Iteration 1042\n",
      "batches :1042 0.03446645845609312\n",
      "Iteration 262: tag train_accuracy, simple_value 0.16722\n",
      "Iteration 262: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510674165, Iteration 1043\n",
      "batches :1043 0.03447596642397656\n",
      "Iteration 263: tag train_accuracy, simple_value 0.16694\n",
      "Iteration 263: tag train_loss, simple_value 0.03448\n",
      "Timestamp 1612510674470, Iteration 1044\n",
      "batches :1044 0.03447368076409806\n",
      "Iteration 264: tag train_accuracy, simple_value 0.16708\n",
      "Iteration 264: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510674776, Iteration 1045\n",
      "batches :1045 0.034473232854649705\n",
      "Iteration 265: tag train_accuracy, simple_value 0.16704\n",
      "Iteration 265: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510675080, Iteration 1046\n",
      "batches :1046 0.03447288903582813\n",
      "Iteration 266: tag train_accuracy, simple_value 0.167\n",
      "Iteration 266: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510675387, Iteration 1047\n",
      "batches :1047 0.03447003104490287\n",
      "Iteration 267: tag train_accuracy, simple_value 0.16702\n",
      "Iteration 267: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510675693, Iteration 1048\n",
      "batches :1048 0.03446287311502357\n",
      "Iteration 268: tag train_accuracy, simple_value 0.16715\n",
      "Iteration 268: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510675999, Iteration 1049\n",
      "batches :1049 0.03446442401209728\n",
      "Iteration 269: tag train_accuracy, simple_value 0.16711\n",
      "Iteration 269: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510676305, Iteration 1050\n",
      "batches :1050 0.03446438714034027\n",
      "Iteration 270: tag train_accuracy, simple_value 0.16684\n",
      "Iteration 270: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510676611, Iteration 1051\n",
      "batches :1051 0.03447220950109053\n",
      "Iteration 271: tag train_accuracy, simple_value 0.16692\n",
      "Iteration 271: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510676917, Iteration 1052\n",
      "batches :1052 0.03447367449510185\n",
      "Iteration 272: tag train_accuracy, simple_value 0.16688\n",
      "Iteration 272: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510677235, Iteration 1053\n",
      "batches :1053 0.03447000821540644\n",
      "Iteration 273: tag train_accuracy, simple_value 0.16707\n",
      "Iteration 273: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510677541, Iteration 1054\n",
      "batches :1054 0.03446825173595091\n",
      "Iteration 274: tag train_accuracy, simple_value 0.16708\n",
      "Iteration 274: tag train_loss, simple_value 0.03447\n",
      "Timestamp 1612510677846, Iteration 1055\n",
      "batches :1055 0.03446498985994946\n",
      "Iteration 275: tag train_accuracy, simple_value 0.16744\n",
      "Iteration 275: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510678153, Iteration 1056\n",
      "batches :1056 0.03446177941193615\n",
      "Iteration 276: tag train_accuracy, simple_value 0.16746\n",
      "Iteration 276: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510678458, Iteration 1057\n",
      "batches :1057 0.03446248780250119\n",
      "Iteration 277: tag train_accuracy, simple_value 0.16759\n",
      "Iteration 277: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510678764, Iteration 1058\n",
      "batches :1058 0.03446149647825485\n",
      "Iteration 278: tag train_accuracy, simple_value 0.16749\n",
      "Iteration 278: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510679070, Iteration 1059\n",
      "batches :1059 0.03446134513637925\n",
      "Iteration 279: tag train_accuracy, simple_value 0.16745\n",
      "Iteration 279: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510679375, Iteration 1060\n",
      "batches :1060 0.03445848133414984\n",
      "Iteration 280: tag train_accuracy, simple_value 0.16747\n",
      "Iteration 280: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510679681, Iteration 1061\n",
      "batches :1061 0.034455268222878414\n",
      "Iteration 281: tag train_accuracy, simple_value 0.16754\n",
      "Iteration 281: tag train_loss, simple_value 0.03446\n",
      "Timestamp 1612510679987, Iteration 1062\n",
      "batches :1062 0.034454952793341154\n",
      "Iteration 282: tag train_accuracy, simple_value 0.16761\n",
      "Iteration 282: tag train_loss, simple_value 0.03445\n",
      "Timestamp 1612510680292, Iteration 1063\n",
      "batches :1063 0.03444947826251545\n",
      "Iteration 283: tag train_accuracy, simple_value 0.16773\n",
      "Iteration 283: tag train_loss, simple_value 0.03445\n",
      "Timestamp 1612510680598, Iteration 1064\n",
      "batches :1064 0.03444306982654921\n",
      "Iteration 284: tag train_accuracy, simple_value 0.1678\n",
      "Iteration 284: tag train_loss, simple_value 0.03444\n",
      "Timestamp 1612510680904, Iteration 1065\n",
      "batches :1065 0.034439336521583694\n",
      "Iteration 285: tag train_accuracy, simple_value 0.16798\n",
      "Iteration 285: tag train_loss, simple_value 0.03444\n",
      "Timestamp 1612510681210, Iteration 1066\n",
      "batches :1066 0.03443194412580737\n",
      "Iteration 286: tag train_accuracy, simple_value 0.16805\n",
      "Iteration 286: tag train_loss, simple_value 0.03443\n",
      "Timestamp 1612510681516, Iteration 1067\n",
      "batches :1067 0.03442678194272394\n",
      "Iteration 287: tag train_accuracy, simple_value 0.16812\n",
      "Iteration 287: tag train_loss, simple_value 0.03443\n",
      "Timestamp 1612510681822, Iteration 1068\n",
      "batches :1068 0.034428060869686306\n",
      "Iteration 288: tag train_accuracy, simple_value 0.16791\n",
      "Iteration 288: tag train_loss, simple_value 0.03443\n",
      "Timestamp 1612510682140, Iteration 1069\n",
      "batches :1069 0.0344260433836998\n",
      "Iteration 289: tag train_accuracy, simple_value 0.16804\n",
      "Iteration 289: tag train_loss, simple_value 0.03443\n",
      "Timestamp 1612510682446, Iteration 1070\n",
      "batches :1070 0.03442227565522852\n",
      "Iteration 290: tag train_accuracy, simple_value 0.1681\n",
      "Iteration 290: tag train_loss, simple_value 0.03442\n",
      "Timestamp 1612510682756, Iteration 1071\n",
      "batches :1071 0.03441603797971178\n",
      "Iteration 291: tag train_accuracy, simple_value 0.16817\n",
      "Iteration 291: tag train_loss, simple_value 0.03442\n",
      "Timestamp 1612510683063, Iteration 1072\n",
      "batches :1072 0.03441175850337907\n",
      "Iteration 292: tag train_accuracy, simple_value 0.16813\n",
      "Iteration 292: tag train_loss, simple_value 0.03441\n",
      "Timestamp 1612510683369, Iteration 1073\n",
      "batches :1073 0.034404424664408684\n",
      "Iteration 293: tag train_accuracy, simple_value 0.16846\n",
      "Iteration 293: tag train_loss, simple_value 0.0344\n",
      "Timestamp 1612510683677, Iteration 1074\n",
      "batches :1074 0.034397243921245844\n",
      "Iteration 294: tag train_accuracy, simple_value 0.16885\n",
      "Iteration 294: tag train_loss, simple_value 0.0344\n",
      "Timestamp 1612510683986, Iteration 1075\n",
      "batches :1075 0.03439658793099856\n",
      "Iteration 295: tag train_accuracy, simple_value 0.1688\n",
      "Iteration 295: tag train_loss, simple_value 0.0344\n",
      "Timestamp 1612510684293, Iteration 1076\n",
      "batches :1076 0.03439783228158548\n",
      "Iteration 296: tag train_accuracy, simple_value 0.16881\n",
      "Iteration 296: tag train_loss, simple_value 0.0344\n",
      "Timestamp 1612510684600, Iteration 1077\n",
      "batches :1077 0.034391210035041524\n",
      "Iteration 297: tag train_accuracy, simple_value 0.16903\n",
      "Iteration 297: tag train_loss, simple_value 0.03439\n",
      "Timestamp 1612510684907, Iteration 1078\n",
      "batches :1078 0.03438727076371644\n",
      "Iteration 298: tag train_accuracy, simple_value 0.1691\n",
      "Iteration 298: tag train_loss, simple_value 0.03439\n",
      "Timestamp 1612510685213, Iteration 1079\n",
      "batches :1079 0.03438072014961354\n",
      "Iteration 299: tag train_accuracy, simple_value 0.16911\n",
      "Iteration 299: tag train_loss, simple_value 0.03438\n",
      "Timestamp 1612510685520, Iteration 1080\n",
      "batches :1080 0.034378644401828445\n",
      "Iteration 300: tag train_accuracy, simple_value 0.16906\n",
      "Iteration 300: tag train_loss, simple_value 0.03438\n",
      "Timestamp 1612510685827, Iteration 1081\n",
      "batches :1081 0.034376354721000424\n",
      "Iteration 301: tag train_accuracy, simple_value 0.16923\n",
      "Iteration 301: tag train_loss, simple_value 0.03438\n",
      "Timestamp 1612510686133, Iteration 1082\n",
      "batches :1082 0.034370198833527946\n",
      "Iteration 302: tag train_accuracy, simple_value 0.16939\n",
      "Iteration 302: tag train_loss, simple_value 0.03437\n",
      "Timestamp 1612510686440, Iteration 1083\n",
      "batches :1083 0.03436880920192983\n",
      "Iteration 303: tag train_accuracy, simple_value 0.1694\n",
      "Iteration 303: tag train_loss, simple_value 0.03437\n",
      "Timestamp 1612510686747, Iteration 1084\n",
      "batches :1084 0.034365304512903094\n",
      "Iteration 304: tag train_accuracy, simple_value 0.16946\n",
      "Iteration 304: tag train_loss, simple_value 0.03437\n",
      "Timestamp 1612510687067, Iteration 1085\n",
      "batches :1085 0.034363103291538896\n",
      "Iteration 305: tag train_accuracy, simple_value 0.16962\n",
      "Iteration 305: tag train_loss, simple_value 0.03436\n",
      "Timestamp 1612510687372, Iteration 1086\n",
      "batches :1086 0.03436128624708824\n",
      "Iteration 306: tag train_accuracy, simple_value 0.16958\n",
      "Iteration 306: tag train_loss, simple_value 0.03436\n",
      "Timestamp 1612510687678, Iteration 1087\n",
      "batches :1087 0.03436376392258883\n",
      "Iteration 307: tag train_accuracy, simple_value 0.16964\n",
      "Iteration 307: tag train_loss, simple_value 0.03436\n",
      "Timestamp 1612510687985, Iteration 1088\n",
      "batches :1088 0.03435810369911132\n",
      "Iteration 308: tag train_accuracy, simple_value 0.16985\n",
      "Iteration 308: tag train_loss, simple_value 0.03436\n",
      "Timestamp 1612510688291, Iteration 1089\n",
      "batches :1089 0.03435533672185392\n",
      "Iteration 309: tag train_accuracy, simple_value 0.1698\n",
      "Iteration 309: tag train_loss, simple_value 0.03436\n",
      "Timestamp 1612510688597, Iteration 1090\n",
      "batches :1090 0.03434976549638856\n",
      "Iteration 310: tag train_accuracy, simple_value 0.17006\n",
      "Iteration 310: tag train_loss, simple_value 0.03435\n",
      "Timestamp 1612510688904, Iteration 1091\n",
      "batches :1091 0.034344391951223664\n",
      "Iteration 311: tag train_accuracy, simple_value 0.17017\n",
      "Iteration 311: tag train_loss, simple_value 0.03434\n",
      "Timestamp 1612510689211, Iteration 1092\n",
      "batches :1092 0.034339832834517345\n",
      "Iteration 312: tag train_accuracy, simple_value 0.17012\n",
      "Iteration 312: tag train_loss, simple_value 0.03434\n",
      "Timestamp 1612510689520, Iteration 1093\n",
      "batches :1093 0.034332566998732356\n",
      "Iteration 313: tag train_accuracy, simple_value 0.17048\n",
      "Iteration 313: tag train_loss, simple_value 0.03433\n",
      "Timestamp 1612510689829, Iteration 1094\n",
      "batches :1094 0.0343321264857889\n",
      "Iteration 314: tag train_accuracy, simple_value 0.17043\n",
      "Iteration 314: tag train_loss, simple_value 0.03433\n",
      "Timestamp 1612510690136, Iteration 1095\n",
      "batches :1095 0.03433740632165046\n",
      "Iteration 315: tag train_accuracy, simple_value 0.17034\n",
      "Iteration 315: tag train_loss, simple_value 0.03434\n",
      "Timestamp 1612510690442, Iteration 1096\n",
      "batches :1096 0.034332992013873936\n",
      "Iteration 316: tag train_accuracy, simple_value 0.17029\n",
      "Iteration 316: tag train_loss, simple_value 0.03433\n",
      "Timestamp 1612510690748, Iteration 1097\n",
      "batches :1097 0.03432923583578236\n",
      "Iteration 317: tag train_accuracy, simple_value 0.17049\n",
      "Iteration 317: tag train_loss, simple_value 0.03433\n",
      "Timestamp 1612510691054, Iteration 1098\n",
      "batches :1098 0.03432101826622801\n",
      "Iteration 318: tag train_accuracy, simple_value 0.17079\n",
      "Iteration 318: tag train_loss, simple_value 0.03432\n",
      "Timestamp 1612510691360, Iteration 1099\n",
      "batches :1099 0.03431849498126574\n",
      "Iteration 319: tag train_accuracy, simple_value 0.17085\n",
      "Iteration 319: tag train_loss, simple_value 0.03432\n",
      "Timestamp 1612510691666, Iteration 1100\n",
      "batches :1100 0.03431906967889518\n",
      "Iteration 320: tag train_accuracy, simple_value 0.17075\n",
      "Iteration 320: tag train_loss, simple_value 0.03432\n",
      "Timestamp 1612510691986, Iteration 1101\n",
      "batches :1101 0.034315087968872345\n",
      "Iteration 321: tag train_accuracy, simple_value 0.17071\n",
      "Iteration 321: tag train_loss, simple_value 0.03432\n",
      "Timestamp 1612510692291, Iteration 1102\n",
      "batches :1102 0.034309260269499715\n",
      "Iteration 322: tag train_accuracy, simple_value 0.17061\n",
      "Iteration 322: tag train_loss, simple_value 0.03431\n",
      "Timestamp 1612510692598, Iteration 1103\n",
      "batches :1103 0.0343088380323475\n",
      "Iteration 323: tag train_accuracy, simple_value 0.17057\n",
      "Iteration 323: tag train_loss, simple_value 0.03431\n",
      "Timestamp 1612510692905, Iteration 1104\n",
      "batches :1104 0.03431261757411706\n",
      "Iteration 324: tag train_accuracy, simple_value 0.17038\n",
      "Iteration 324: tag train_loss, simple_value 0.03431\n",
      "Timestamp 1612510693212, Iteration 1105\n",
      "batches :1105 0.03430706193813911\n",
      "Iteration 325: tag train_accuracy, simple_value 0.17067\n",
      "Iteration 325: tag train_loss, simple_value 0.03431\n",
      "Timestamp 1612510693517, Iteration 1106\n",
      "batches :1106 0.034306751929817754\n",
      "Iteration 326: tag train_accuracy, simple_value 0.17049\n",
      "Iteration 326: tag train_loss, simple_value 0.03431\n",
      "Timestamp 1612510693823, Iteration 1107\n",
      "batches :1107 0.03430138555688596\n",
      "Iteration 327: tag train_accuracy, simple_value 0.17063\n",
      "Iteration 327: tag train_loss, simple_value 0.0343\n",
      "Timestamp 1612510694128, Iteration 1108\n",
      "batches :1108 0.03429397253501343\n",
      "Iteration 328: tag train_accuracy, simple_value 0.17092\n",
      "Iteration 328: tag train_loss, simple_value 0.03429\n",
      "Timestamp 1612510694434, Iteration 1109\n",
      "batches :1109 0.03428372900400843\n",
      "Iteration 329: tag train_accuracy, simple_value 0.17126\n",
      "Iteration 329: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510694740, Iteration 1110\n",
      "batches :1110 0.034282635028163595\n",
      "Iteration 330: tag train_accuracy, simple_value 0.1714\n",
      "Iteration 330: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510695045, Iteration 1111\n",
      "batches :1111 0.03428314017340136\n",
      "Iteration 331: tag train_accuracy, simple_value 0.17154\n",
      "Iteration 331: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510695352, Iteration 1112\n",
      "batches :1112 0.03428619923855526\n",
      "Iteration 332: tag train_accuracy, simple_value 0.17136\n",
      "Iteration 332: tag train_loss, simple_value 0.03429\n",
      "Timestamp 1612510695658, Iteration 1113\n",
      "batches :1113 0.03428352715911808\n",
      "Iteration 333: tag train_accuracy, simple_value 0.17127\n",
      "Iteration 333: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510695966, Iteration 1114\n",
      "batches :1114 0.034282018692639776\n",
      "Iteration 334: tag train_accuracy, simple_value 0.17131\n",
      "Iteration 334: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510696271, Iteration 1115\n",
      "batches :1115 0.03428090883057509\n",
      "Iteration 335: tag train_accuracy, simple_value 0.17136\n",
      "Iteration 335: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510696576, Iteration 1116\n",
      "batches :1116 0.034276395925276336\n",
      "Iteration 336: tag train_accuracy, simple_value 0.17132\n",
      "Iteration 336: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510696897, Iteration 1117\n",
      "batches :1117 0.034275995296611274\n",
      "Iteration 337: tag train_accuracy, simple_value 0.17132\n",
      "Iteration 337: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510697202, Iteration 1118\n",
      "batches :1118 0.034277525118734005\n",
      "Iteration 338: tag train_accuracy, simple_value 0.17132\n",
      "Iteration 338: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510697508, Iteration 1119\n",
      "batches :1119 0.03427523476755725\n",
      "Iteration 339: tag train_accuracy, simple_value 0.17123\n",
      "Iteration 339: tag train_loss, simple_value 0.03428\n",
      "Timestamp 1612510697815, Iteration 1120\n",
      "batches :1120 0.03426533037358347\n",
      "Iteration 340: tag train_accuracy, simple_value 0.17155\n",
      "Iteration 340: tag train_loss, simple_value 0.03427\n",
      "Timestamp 1612510698120, Iteration 1121\n",
      "batches :1121 0.034262816353408825\n",
      "Iteration 341: tag train_accuracy, simple_value 0.17169\n",
      "Iteration 341: tag train_loss, simple_value 0.03426\n",
      "Timestamp 1612510698427, Iteration 1122\n",
      "batches :1122 0.03426044410889783\n",
      "Iteration 342: tag train_accuracy, simple_value 0.17174\n",
      "Iteration 342: tag train_loss, simple_value 0.03426\n",
      "Timestamp 1612510698733, Iteration 1123\n",
      "batches :1123 0.034258844005079725\n",
      "Iteration 343: tag train_accuracy, simple_value 0.17183\n",
      "Iteration 343: tag train_loss, simple_value 0.03426\n",
      "Timestamp 1612510699042, Iteration 1124\n",
      "batches :1124 0.03425430450696758\n",
      "Iteration 344: tag train_accuracy, simple_value 0.17183\n",
      "Iteration 344: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510699347, Iteration 1125\n",
      "batches :1125 0.034254634331749835\n",
      "Iteration 345: tag train_accuracy, simple_value 0.17183\n",
      "Iteration 345: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510699652, Iteration 1126\n",
      "batches :1126 0.03425607885734711\n",
      "Iteration 346: tag train_accuracy, simple_value 0.17174\n",
      "Iteration 346: tag train_loss, simple_value 0.03426\n",
      "Timestamp 1612510699959, Iteration 1127\n",
      "batches :1127 0.03425302467596119\n",
      "Iteration 347: tag train_accuracy, simple_value 0.17197\n",
      "Iteration 347: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510700267, Iteration 1128\n",
      "batches :1128 0.034254314137044654\n",
      "Iteration 348: tag train_accuracy, simple_value 0.17179\n",
      "Iteration 348: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510700573, Iteration 1129\n",
      "batches :1129 0.03425299870997753\n",
      "Iteration 349: tag train_accuracy, simple_value 0.17183\n",
      "Iteration 349: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510700878, Iteration 1130\n",
      "batches :1130 0.034253770874014926\n",
      "Iteration 350: tag train_accuracy, simple_value 0.1717\n",
      "Iteration 350: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510701185, Iteration 1131\n",
      "batches :1131 0.03425262155717085\n",
      "Iteration 351: tag train_accuracy, simple_value 0.17156\n",
      "Iteration 351: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510701490, Iteration 1132\n",
      "batches :1132 0.03424899019195105\n",
      "Iteration 352: tag train_accuracy, simple_value 0.17148\n",
      "Iteration 352: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510701808, Iteration 1133\n",
      "batches :1133 0.03424871078146247\n",
      "Iteration 353: tag train_accuracy, simple_value 0.17148\n",
      "Iteration 353: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510702114, Iteration 1134\n",
      "batches :1134 0.03424911328859753\n",
      "Iteration 354: tag train_accuracy, simple_value 0.17143\n",
      "Iteration 354: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510702419, Iteration 1135\n",
      "batches :1135 0.034248349810360186\n",
      "Iteration 355: tag train_accuracy, simple_value 0.1713\n",
      "Iteration 355: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510702726, Iteration 1136\n",
      "batches :1136 0.034248097899129214\n",
      "Iteration 356: tag train_accuracy, simple_value 0.1713\n",
      "Iteration 356: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510703033, Iteration 1137\n",
      "batches :1137 0.0342522480209418\n",
      "Iteration 357: tag train_accuracy, simple_value 0.17117\n",
      "Iteration 357: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510703340, Iteration 1138\n",
      "batches :1138 0.034256815187133555\n",
      "Iteration 358: tag train_accuracy, simple_value 0.17135\n",
      "Iteration 358: tag train_loss, simple_value 0.03426\n",
      "Timestamp 1612510703646, Iteration 1139\n",
      "batches :1139 0.034254445873387676\n",
      "Iteration 359: tag train_accuracy, simple_value 0.1714\n",
      "Iteration 359: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510703953, Iteration 1140\n",
      "batches :1140 0.03424858184427851\n",
      "Iteration 360: tag train_accuracy, simple_value 0.17148\n",
      "Iteration 360: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510704260, Iteration 1141\n",
      "batches :1141 0.03424842600450126\n",
      "Iteration 361: tag train_accuracy, simple_value 0.17153\n",
      "Iteration 361: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510704566, Iteration 1142\n",
      "batches :1142 0.03424719300697357\n",
      "Iteration 362: tag train_accuracy, simple_value 0.17162\n",
      "Iteration 362: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510704876, Iteration 1143\n",
      "batches :1143 0.03424339584965187\n",
      "Iteration 363: tag train_accuracy, simple_value 0.17149\n",
      "Iteration 363: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510705182, Iteration 1144\n",
      "batches :1144 0.034245618910702706\n",
      "Iteration 364: tag train_accuracy, simple_value 0.17162\n",
      "Iteration 364: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510705489, Iteration 1145\n",
      "batches :1145 0.03424613879457729\n",
      "Iteration 365: tag train_accuracy, simple_value 0.17158\n",
      "Iteration 365: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510705796, Iteration 1146\n",
      "batches :1146 0.034245145781973345\n",
      "Iteration 366: tag train_accuracy, simple_value 0.17153\n",
      "Iteration 366: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510706104, Iteration 1147\n",
      "batches :1147 0.03424869479271469\n",
      "Iteration 367: tag train_accuracy, simple_value 0.17141\n",
      "Iteration 367: tag train_loss, simple_value 0.03425\n",
      "Timestamp 1612510706412, Iteration 1148\n",
      "batches :1148 0.03424361319301407\n",
      "Iteration 368: tag train_accuracy, simple_value 0.17154\n",
      "Iteration 368: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510706733, Iteration 1149\n",
      "batches :1149 0.0342432635313854\n",
      "Iteration 369: tag train_accuracy, simple_value 0.17166\n",
      "Iteration 369: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510707039, Iteration 1150\n",
      "batches :1150 0.034239700512104745\n",
      "Iteration 370: tag train_accuracy, simple_value 0.17188\n",
      "Iteration 370: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510707344, Iteration 1151\n",
      "batches :1151 0.0342407486680142\n",
      "Iteration 371: tag train_accuracy, simple_value 0.172\n",
      "Iteration 371: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510707651, Iteration 1152\n",
      "batches :1152 0.03424295107583686\n",
      "Iteration 372: tag train_accuracy, simple_value 0.17188\n",
      "Iteration 372: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510707956, Iteration 1153\n",
      "batches :1153 0.034239060642452405\n",
      "Iteration 373: tag train_accuracy, simple_value 0.17217\n",
      "Iteration 373: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510708261, Iteration 1154\n",
      "batches :1154 0.03423612027424064\n",
      "Iteration 374: tag train_accuracy, simple_value 0.17213\n",
      "Iteration 374: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510708566, Iteration 1155\n",
      "batches :1155 0.03423619077106317\n",
      "Iteration 375: tag train_accuracy, simple_value 0.172\n",
      "Iteration 375: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510708871, Iteration 1156\n",
      "batches :1156 0.03423707667203184\n",
      "Iteration 376: tag train_accuracy, simple_value 0.17183\n",
      "Iteration 376: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510709177, Iteration 1157\n",
      "batches :1157 0.034232905322305406\n",
      "Iteration 377: tag train_accuracy, simple_value 0.17208\n",
      "Iteration 377: tag train_loss, simple_value 0.03423\n",
      "Timestamp 1612510709482, Iteration 1158\n",
      "batches :1158 0.03423572755936119\n",
      "Iteration 378: tag train_accuracy, simple_value 0.17204\n",
      "Iteration 378: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510709787, Iteration 1159\n",
      "batches :1159 0.03423670660163765\n",
      "Iteration 379: tag train_accuracy, simple_value 0.17204\n",
      "Iteration 379: tag train_loss, simple_value 0.03424\n",
      "Timestamp 1612510710094, Iteration 1160\n",
      "batches :1160 0.03423348666707936\n",
      "Iteration 380: tag train_accuracy, simple_value 0.17212\n",
      "Iteration 380: tag train_loss, simple_value 0.03423\n",
      "Timestamp 1612510710401, Iteration 1161\n",
      "batches :1161 0.03423270662840739\n",
      "Iteration 381: tag train_accuracy, simple_value 0.1722\n",
      "Iteration 381: tag train_loss, simple_value 0.03423\n",
      "Timestamp 1612510710709, Iteration 1162\n",
      "batches :1162 0.03423238653416565\n",
      "Iteration 382: tag train_accuracy, simple_value 0.17249\n",
      "Iteration 382: tag train_loss, simple_value 0.03423\n",
      "Timestamp 1612510711014, Iteration 1163\n",
      "batches :1163 0.03422947726443449\n",
      "Iteration 383: tag train_accuracy, simple_value 0.17245\n",
      "Iteration 383: tag train_loss, simple_value 0.03423\n",
      "Timestamp 1612510711319, Iteration 1164\n",
      "batches :1164 0.03422466085855073\n",
      "Iteration 384: tag train_accuracy, simple_value 0.17257\n",
      "Iteration 384: tag train_loss, simple_value 0.03422\n",
      "Timestamp 1612510711637, Iteration 1165\n",
      "batches :1165 0.034224074000081456\n",
      "Iteration 385: tag train_accuracy, simple_value 0.17261\n",
      "Iteration 385: tag train_loss, simple_value 0.03422\n",
      "Timestamp 1612510711942, Iteration 1166\n",
      "batches :1166 0.03421887919943722\n",
      "Iteration 386: tag train_accuracy, simple_value 0.17248\n",
      "Iteration 386: tag train_loss, simple_value 0.03422\n",
      "Timestamp 1612510712246, Iteration 1167\n",
      "batches :1167 0.034217607927838345\n",
      "Iteration 387: tag train_accuracy, simple_value 0.1726\n",
      "Iteration 387: tag train_loss, simple_value 0.03422\n",
      "Timestamp 1612510712553, Iteration 1168\n",
      "batches :1168 0.03421445564875744\n",
      "Iteration 388: tag train_accuracy, simple_value 0.17264\n",
      "Iteration 388: tag train_loss, simple_value 0.03421\n",
      "Timestamp 1612510712859, Iteration 1169\n",
      "batches :1169 0.03421653183248938\n",
      "Iteration 389: tag train_accuracy, simple_value 0.17264\n",
      "Iteration 389: tag train_loss, simple_value 0.03422\n",
      "Timestamp 1612510713166, Iteration 1170\n",
      "batches :1170 0.034216819937603594\n",
      "Iteration 390: tag train_accuracy, simple_value 0.17268\n",
      "Iteration 390: tag train_loss, simple_value 0.03422\n",
      "Timestamp 1612510713473, Iteration 1171\n",
      "batches :1171 0.034215796276774554\n",
      "Iteration 391: tag train_accuracy, simple_value 0.17279\n",
      "Iteration 391: tag train_loss, simple_value 0.03422\n",
      "Timestamp 1612510713779, Iteration 1172\n",
      "batches :1172 0.03421290399867813\n",
      "Iteration 392: tag train_accuracy, simple_value 0.17267\n",
      "Iteration 392: tag train_loss, simple_value 0.03421\n",
      "Timestamp 1612510714084, Iteration 1173\n",
      "batches :1173 0.03421248135846535\n",
      "Iteration 393: tag train_accuracy, simple_value 0.17271\n",
      "Iteration 393: tag train_loss, simple_value 0.03421\n",
      "Timestamp 1612510714389, Iteration 1174\n",
      "batches :1174 0.03421114354399104\n",
      "Iteration 394: tag train_accuracy, simple_value 0.17271\n",
      "Iteration 394: tag train_loss, simple_value 0.03421\n",
      "Timestamp 1612510714694, Iteration 1175\n",
      "batches :1175 0.03420646019468579\n",
      "Iteration 395: tag train_accuracy, simple_value 0.17286\n",
      "Iteration 395: tag train_loss, simple_value 0.03421\n",
      "Timestamp 1612510715000, Iteration 1176\n",
      "batches :1176 0.03420640272793896\n",
      "Iteration 396: tag train_accuracy, simple_value 0.17278\n",
      "Iteration 396: tag train_loss, simple_value 0.03421\n",
      "Timestamp 1612510715307, Iteration 1177\n",
      "batches :1177 0.03420253934559204\n",
      "Iteration 397: tag train_accuracy, simple_value 0.1729\n",
      "Iteration 397: tag train_loss, simple_value 0.0342\n",
      "Timestamp 1612510715614, Iteration 1178\n",
      "batches :1178 0.034198377660508435\n",
      "Iteration 398: tag train_accuracy, simple_value 0.17301\n",
      "Iteration 398: tag train_loss, simple_value 0.0342\n",
      "Timestamp 1612510715921, Iteration 1179\n",
      "batches :1179 0.03420115522293369\n",
      "Iteration 399: tag train_accuracy, simple_value 0.17297\n",
      "Iteration 399: tag train_loss, simple_value 0.0342\n",
      "Timestamp 1612510716228, Iteration 1180\n",
      "batches :1180 0.034199794414453207\n",
      "Iteration 400: tag train_accuracy, simple_value 0.17293\n",
      "Iteration 400: tag train_loss, simple_value 0.0342\n",
      "Timestamp 1612510716549, Iteration 1181\n",
      "batches :1181 0.0341995020450425\n",
      "Iteration 401: tag train_accuracy, simple_value 0.17289\n",
      "Iteration 401: tag train_loss, simple_value 0.0342\n",
      "Timestamp 1612510716856, Iteration 1182\n",
      "batches :1182 0.03419211794117197\n",
      "Iteration 402: tag train_accuracy, simple_value 0.17308\n",
      "Iteration 402: tag train_loss, simple_value 0.03419\n",
      "Timestamp 1612510717163, Iteration 1183\n",
      "batches :1183 0.03418936012844294\n",
      "Iteration 403: tag train_accuracy, simple_value 0.17319\n",
      "Iteration 403: tag train_loss, simple_value 0.03419\n",
      "Timestamp 1612510717469, Iteration 1184\n",
      "batches :1184 0.034188098569243854\n",
      "Iteration 404: tag train_accuracy, simple_value 0.17319\n",
      "Iteration 404: tag train_loss, simple_value 0.03419\n",
      "Timestamp 1612510717776, Iteration 1185\n",
      "batches :1185 0.03418203070759773\n",
      "Iteration 405: tag train_accuracy, simple_value 0.17346\n",
      "Iteration 405: tag train_loss, simple_value 0.03418\n",
      "Timestamp 1612510718082, Iteration 1186\n",
      "batches :1186 0.034178052290246404\n",
      "Iteration 406: tag train_accuracy, simple_value 0.17338\n",
      "Iteration 406: tag train_loss, simple_value 0.03418\n",
      "Timestamp 1612510718389, Iteration 1187\n",
      "batches :1187 0.034176363259627134\n",
      "Iteration 407: tag train_accuracy, simple_value 0.1733\n",
      "Iteration 407: tag train_loss, simple_value 0.03418\n",
      "Timestamp 1612510718695, Iteration 1188\n",
      "batches :1188 0.034174434557630154\n",
      "Iteration 408: tag train_accuracy, simple_value 0.17325\n",
      "Iteration 408: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510719001, Iteration 1189\n",
      "batches :1189 0.03417710510872687\n",
      "Iteration 409: tag train_accuracy, simple_value 0.17314\n",
      "Iteration 409: tag train_loss, simple_value 0.03418\n",
      "Timestamp 1612510719307, Iteration 1190\n",
      "batches :1190 0.034175907457020226\n",
      "Iteration 410: tag train_accuracy, simple_value 0.17306\n",
      "Iteration 410: tag train_loss, simple_value 0.03418\n",
      "Timestamp 1612510719613, Iteration 1191\n",
      "batches :1191 0.03417473783095678\n",
      "Iteration 411: tag train_accuracy, simple_value 0.17313\n",
      "Iteration 411: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510719919, Iteration 1192\n",
      "batches :1192 0.034174041081111405\n",
      "Iteration 412: tag train_accuracy, simple_value 0.1732\n",
      "Iteration 412: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510720225, Iteration 1193\n",
      "batches :1193 0.034170333250960194\n",
      "Iteration 413: tag train_accuracy, simple_value 0.17324\n",
      "Iteration 413: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510720531, Iteration 1194\n",
      "batches :1194 0.034172477227622185\n",
      "Iteration 414: tag train_accuracy, simple_value 0.17323\n",
      "Iteration 414: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510720837, Iteration 1195\n",
      "batches :1195 0.03417081946888602\n",
      "Iteration 415: tag train_accuracy, simple_value 0.17312\n",
      "Iteration 415: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510721143, Iteration 1196\n",
      "batches :1196 0.03416600742807182\n",
      "Iteration 416: tag train_accuracy, simple_value 0.17326\n",
      "Iteration 416: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510721463, Iteration 1197\n",
      "batches :1197 0.03416190182955431\n",
      "Iteration 417: tag train_accuracy, simple_value 0.17334\n",
      "Iteration 417: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510721768, Iteration 1198\n",
      "batches :1198 0.034163121142954916\n",
      "Iteration 418: tag train_accuracy, simple_value 0.17315\n",
      "Iteration 418: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510722074, Iteration 1199\n",
      "batches :1199 0.03416397856428401\n",
      "Iteration 419: tag train_accuracy, simple_value 0.17296\n",
      "Iteration 419: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510722380, Iteration 1200\n",
      "batches :1200 0.0341641656078753\n",
      "Iteration 420: tag train_accuracy, simple_value 0.17299\n",
      "Iteration 420: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510722687, Iteration 1201\n",
      "batches :1201 0.03416822955118506\n",
      "Iteration 421: tag train_accuracy, simple_value 0.17288\n",
      "Iteration 421: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510722996, Iteration 1202\n",
      "batches :1202 0.03416798656620968\n",
      "Iteration 422: tag train_accuracy, simple_value 0.17269\n",
      "Iteration 422: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510723303, Iteration 1203\n",
      "batches :1203 0.03416275611724132\n",
      "Iteration 423: tag train_accuracy, simple_value 0.17261\n",
      "Iteration 423: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510723610, Iteration 1204\n",
      "batches :1204 0.03416145676395522\n",
      "Iteration 424: tag train_accuracy, simple_value 0.1725\n",
      "Iteration 424: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510723917, Iteration 1205\n",
      "batches :1205 0.03416476477594937\n",
      "Iteration 425: tag train_accuracy, simple_value 0.17246\n",
      "Iteration 425: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510724222, Iteration 1206\n",
      "batches :1206 0.034165272978899625\n",
      "Iteration 426: tag train_accuracy, simple_value 0.17254\n",
      "Iteration 426: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510724528, Iteration 1207\n",
      "batches :1207 0.03416569968052994\n",
      "Iteration 427: tag train_accuracy, simple_value 0.17264\n",
      "Iteration 427: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510724835, Iteration 1208\n",
      "batches :1208 0.03416943115782793\n",
      "Iteration 428: tag train_accuracy, simple_value 0.17261\n",
      "Iteration 428: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510725141, Iteration 1209\n",
      "batches :1209 0.034168515849363555\n",
      "Iteration 429: tag train_accuracy, simple_value 0.1726\n",
      "Iteration 429: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510725448, Iteration 1210\n",
      "batches :1210 0.03416801362709944\n",
      "Iteration 430: tag train_accuracy, simple_value 0.1726\n",
      "Iteration 430: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510725753, Iteration 1211\n",
      "batches :1211 0.03416466718137679\n",
      "Iteration 431: tag train_accuracy, simple_value 0.17271\n",
      "Iteration 431: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510726060, Iteration 1212\n",
      "batches :1212 0.03415858973231581\n",
      "Iteration 432: tag train_accuracy, simple_value 0.17292\n",
      "Iteration 432: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510726381, Iteration 1213\n",
      "batches :1213 0.034157490542730616\n",
      "Iteration 433: tag train_accuracy, simple_value 0.17292\n",
      "Iteration 433: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510726688, Iteration 1214\n",
      "batches :1214 0.03416212359743734\n",
      "Iteration 434: tag train_accuracy, simple_value 0.17306\n",
      "Iteration 434: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510726995, Iteration 1215\n",
      "batches :1215 0.034167419579522365\n",
      "Iteration 435: tag train_accuracy, simple_value 0.17295\n",
      "Iteration 435: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510727302, Iteration 1216\n",
      "batches :1216 0.03416838568695094\n",
      "Iteration 436: tag train_accuracy, simple_value 0.17277\n",
      "Iteration 436: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510727608, Iteration 1217\n",
      "batches :1217 0.03416555200957052\n",
      "Iteration 437: tag train_accuracy, simple_value 0.1728\n",
      "Iteration 437: tag train_loss, simple_value 0.03417\n",
      "Timestamp 1612510727913, Iteration 1218\n",
      "batches :1218 0.034160396254294\n",
      "Iteration 438: tag train_accuracy, simple_value 0.17287\n",
      "Iteration 438: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510728218, Iteration 1219\n",
      "batches :1219 0.0341600578931714\n",
      "Iteration 439: tag train_accuracy, simple_value 0.1728\n",
      "Iteration 439: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510728523, Iteration 1220\n",
      "batches :1220 0.03416076136583632\n",
      "Iteration 440: tag train_accuracy, simple_value 0.17276\n",
      "Iteration 440: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510728829, Iteration 1221\n",
      "batches :1221 0.03416095398653662\n",
      "Iteration 441: tag train_accuracy, simple_value 0.17258\n",
      "Iteration 441: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510729136, Iteration 1222\n",
      "batches :1222 0.03415905723002701\n",
      "Iteration 442: tag train_accuracy, simple_value 0.17265\n",
      "Iteration 442: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510729443, Iteration 1223\n",
      "batches :1223 0.0341560483382464\n",
      "Iteration 443: tag train_accuracy, simple_value 0.17286\n",
      "Iteration 443: tag train_loss, simple_value 0.03416\n",
      "Timestamp 1612510729749, Iteration 1224\n",
      "batches :1224 0.03415225422731391\n",
      "Iteration 444: tag train_accuracy, simple_value 0.17279\n",
      "Iteration 444: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510730057, Iteration 1225\n",
      "batches :1225 0.03415325911862127\n",
      "Iteration 445: tag train_accuracy, simple_value 0.17282\n",
      "Iteration 445: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510730363, Iteration 1226\n",
      "batches :1226 0.03415184792949747\n",
      "Iteration 446: tag train_accuracy, simple_value 0.17289\n",
      "Iteration 446: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510730669, Iteration 1227\n",
      "batches :1227 0.03415019574348025\n",
      "Iteration 447: tag train_accuracy, simple_value 0.17282\n",
      "Iteration 447: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510730977, Iteration 1228\n",
      "batches :1228 0.034151104220654815\n",
      "Iteration 448: tag train_accuracy, simple_value 0.17275\n",
      "Iteration 448: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510731298, Iteration 1229\n",
      "batches :1229 0.03415328914710302\n",
      "Iteration 449: tag train_accuracy, simple_value 0.17268\n",
      "Iteration 449: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510731605, Iteration 1230\n",
      "batches :1230 0.03415274443725745\n",
      "Iteration 450: tag train_accuracy, simple_value 0.17281\n",
      "Iteration 450: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510731913, Iteration 1231\n",
      "batches :1231 0.03415158917991126\n",
      "Iteration 451: tag train_accuracy, simple_value 0.17267\n",
      "Iteration 451: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510732218, Iteration 1232\n",
      "batches :1232 0.0341491007567507\n",
      "Iteration 452: tag train_accuracy, simple_value 0.17277\n",
      "Iteration 452: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510732524, Iteration 1233\n",
      "batches :1233 0.03414714805140401\n",
      "Iteration 453: tag train_accuracy, simple_value 0.17263\n",
      "Iteration 453: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510732829, Iteration 1234\n",
      "batches :1234 0.03415017053156697\n",
      "Iteration 454: tag train_accuracy, simple_value 0.1727\n",
      "Iteration 454: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510733136, Iteration 1235\n",
      "batches :1235 0.034149921055023486\n",
      "Iteration 455: tag train_accuracy, simple_value 0.17263\n",
      "Iteration 455: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510733443, Iteration 1236\n",
      "batches :1236 0.034150063485037865\n",
      "Iteration 456: tag train_accuracy, simple_value 0.17273\n",
      "Iteration 456: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510733751, Iteration 1237\n",
      "batches :1237 0.03415274796946304\n",
      "Iteration 457: tag train_accuracy, simple_value 0.17256\n",
      "Iteration 457: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510734058, Iteration 1238\n",
      "batches :1238 0.034149969517208605\n",
      "Iteration 458: tag train_accuracy, simple_value 0.17252\n",
      "Iteration 458: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510734364, Iteration 1239\n",
      "batches :1239 0.034150928745861925\n",
      "Iteration 459: tag train_accuracy, simple_value 0.17245\n",
      "Iteration 459: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510734670, Iteration 1240\n",
      "batches :1240 0.03414842019100552\n",
      "Iteration 460: tag train_accuracy, simple_value 0.17238\n",
      "Iteration 460: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510734975, Iteration 1241\n",
      "batches :1241 0.03414263952838625\n",
      "Iteration 461: tag train_accuracy, simple_value 0.17276\n",
      "Iteration 461: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510735281, Iteration 1242\n",
      "batches :1242 0.034145000467310734\n",
      "Iteration 462: tag train_accuracy, simple_value 0.17262\n",
      "Iteration 462: tag train_loss, simple_value 0.03415\n",
      "Timestamp 1612510735585, Iteration 1243\n",
      "batches :1243 0.0341430537837113\n",
      "Iteration 463: tag train_accuracy, simple_value 0.17255\n",
      "Iteration 463: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510735891, Iteration 1244\n",
      "batches :1244 0.03413978386830924\n",
      "Iteration 464: tag train_accuracy, simple_value 0.17268\n",
      "Iteration 464: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510736208, Iteration 1245\n",
      "batches :1245 0.034140178360926206\n",
      "Iteration 465: tag train_accuracy, simple_value 0.17268\n",
      "Iteration 465: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510736515, Iteration 1246\n",
      "batches :1246 0.034137082921716785\n",
      "Iteration 466: tag train_accuracy, simple_value 0.17268\n",
      "Iteration 466: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510736821, Iteration 1247\n",
      "batches :1247 0.03413760129210781\n",
      "Iteration 467: tag train_accuracy, simple_value 0.17264\n",
      "Iteration 467: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510737128, Iteration 1248\n",
      "batches :1248 0.03414276842441824\n",
      "Iteration 468: tag train_accuracy, simple_value 0.17261\n",
      "Iteration 468: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510737433, Iteration 1249\n",
      "batches :1249 0.034140701923988014\n",
      "Iteration 469: tag train_accuracy, simple_value 0.17261\n",
      "Iteration 469: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510737739, Iteration 1250\n",
      "batches :1250 0.034139724004458874\n",
      "Iteration 470: tag train_accuracy, simple_value 0.17271\n",
      "Iteration 470: tag train_loss, simple_value 0.03414\n",
      "Timestamp 1612510738044, Iteration 1251\n",
      "batches :1251 0.03413453710813178\n",
      "Iteration 471: tag train_accuracy, simple_value 0.173\n",
      "Iteration 471: tag train_loss, simple_value 0.03413\n",
      "Timestamp 1612510738351, Iteration 1252\n",
      "batches :1252 0.03413089207705047\n",
      "Iteration 472: tag train_accuracy, simple_value 0.173\n",
      "Iteration 472: tag train_loss, simple_value 0.03413\n",
      "Timestamp 1612510738658, Iteration 1253\n",
      "batches :1253 0.034127744249448214\n",
      "Iteration 473: tag train_accuracy, simple_value 0.1732\n",
      "Iteration 473: tag train_loss, simple_value 0.03413\n",
      "Timestamp 1612510738966, Iteration 1254\n",
      "batches :1254 0.03412487581843817\n",
      "Iteration 474: tag train_accuracy, simple_value 0.17329\n",
      "Iteration 474: tag train_loss, simple_value 0.03412\n",
      "Timestamp 1612510739273, Iteration 1255\n",
      "batches :1255 0.034121469941578414\n",
      "Iteration 475: tag train_accuracy, simple_value 0.17332\n",
      "Iteration 475: tag train_loss, simple_value 0.03412\n",
      "Timestamp 1612510739580, Iteration 1256\n",
      "batches :1256 0.034119314208877184\n",
      "Iteration 476: tag train_accuracy, simple_value 0.17319\n",
      "Iteration 476: tag train_loss, simple_value 0.03412\n",
      "Timestamp 1612510739886, Iteration 1257\n",
      "batches :1257 0.03411652125902396\n",
      "Iteration 477: tag train_accuracy, simple_value 0.17345\n",
      "Iteration 477: tag train_loss, simple_value 0.03412\n",
      "Timestamp 1612510740191, Iteration 1258\n",
      "batches :1258 0.03411835870046995\n",
      "Iteration 478: tag train_accuracy, simple_value 0.17344\n",
      "Iteration 478: tag train_loss, simple_value 0.03412\n",
      "Timestamp 1612510740497, Iteration 1259\n",
      "batches :1259 0.03411646424510783\n",
      "Iteration 479: tag train_accuracy, simple_value 0.17357\n",
      "Iteration 479: tag train_loss, simple_value 0.03412\n",
      "Timestamp 1612510740802, Iteration 1260\n",
      "batches :1260 0.03411550319287926\n",
      "Iteration 480: tag train_accuracy, simple_value 0.17347\n",
      "Iteration 480: tag train_loss, simple_value 0.03412\n",
      "Timestamp 1612510741121, Iteration 1261\n",
      "batches :1261 0.03411466482709202\n",
      "Iteration 481: tag train_accuracy, simple_value 0.1734\n",
      "Iteration 481: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510741427, Iteration 1262\n",
      "batches :1262 0.03411307331962952\n",
      "Iteration 482: tag train_accuracy, simple_value 0.1734\n",
      "Iteration 482: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510741733, Iteration 1263\n",
      "batches :1263 0.034109425509259814\n",
      "Iteration 483: tag train_accuracy, simple_value 0.1734\n",
      "Iteration 483: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510742040, Iteration 1264\n",
      "batches :1264 0.034106825193590366\n",
      "Iteration 484: tag train_accuracy, simple_value 0.17359\n",
      "Iteration 484: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510742345, Iteration 1265\n",
      "batches :1265 0.03410932072198268\n",
      "Iteration 485: tag train_accuracy, simple_value 0.17352\n",
      "Iteration 485: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510742651, Iteration 1266\n",
      "batches :1266 0.03411082117032612\n",
      "Iteration 486: tag train_accuracy, simple_value 0.17351\n",
      "Iteration 486: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510742958, Iteration 1267\n",
      "batches :1267 0.034108744287821305\n",
      "Iteration 487: tag train_accuracy, simple_value 0.17345\n",
      "Iteration 487: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510743264, Iteration 1268\n",
      "batches :1268 0.03411000789921792\n",
      "Iteration 488: tag train_accuracy, simple_value 0.17338\n",
      "Iteration 488: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510743572, Iteration 1269\n",
      "batches :1269 0.034108406957429366\n",
      "Iteration 489: tag train_accuracy, simple_value 0.17334\n",
      "Iteration 489: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510743878, Iteration 1270\n",
      "batches :1270 0.034107505378066276\n",
      "Iteration 490: tag train_accuracy, simple_value 0.17344\n",
      "Iteration 490: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510744185, Iteration 1271\n",
      "batches :1271 0.034106222612976786\n",
      "Iteration 491: tag train_accuracy, simple_value 0.17347\n",
      "Iteration 491: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510744491, Iteration 1272\n",
      "batches :1272 0.0341109740207108\n",
      "Iteration 492: tag train_accuracy, simple_value 0.17356\n",
      "Iteration 492: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510744797, Iteration 1273\n",
      "batches :1273 0.03411231294999742\n",
      "Iteration 493: tag train_accuracy, simple_value 0.17352\n",
      "Iteration 493: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510745104, Iteration 1274\n",
      "batches :1274 0.03411339741456605\n",
      "Iteration 494: tag train_accuracy, simple_value 0.17346\n",
      "Iteration 494: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510745413, Iteration 1275\n",
      "batches :1275 0.03411430251417738\n",
      "Iteration 495: tag train_accuracy, simple_value 0.17339\n",
      "Iteration 495: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510745720, Iteration 1276\n",
      "batches :1276 0.03410897255995341\n",
      "Iteration 496: tag train_accuracy, simple_value 0.17348\n",
      "Iteration 496: tag train_loss, simple_value 0.03411\n",
      "Timestamp 1612510746041, Iteration 1277\n",
      "batches :1277 0.03410493787115728\n",
      "Iteration 497: tag train_accuracy, simple_value 0.17342\n",
      "Iteration 497: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510746348, Iteration 1278\n",
      "batches :1278 0.034101275323205685\n",
      "Iteration 498: tag train_accuracy, simple_value 0.17351\n",
      "Iteration 498: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510746655, Iteration 1279\n",
      "batches :1279 0.034101086181545545\n",
      "Iteration 499: tag train_accuracy, simple_value 0.1736\n",
      "Iteration 499: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510746961, Iteration 1280\n",
      "batches :1280 0.03410035829246044\n",
      "Iteration 500: tag train_accuracy, simple_value 0.17341\n",
      "Iteration 500: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510747268, Iteration 1281\n",
      "batches :1281 0.034099567614629595\n",
      "Iteration 501: tag train_accuracy, simple_value 0.17328\n",
      "Iteration 501: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510747575, Iteration 1282\n",
      "batches :1282 0.034099527096665236\n",
      "Iteration 502: tag train_accuracy, simple_value 0.17321\n",
      "Iteration 502: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510747882, Iteration 1283\n",
      "batches :1283 0.03409969682988542\n",
      "Iteration 503: tag train_accuracy, simple_value 0.17318\n",
      "Iteration 503: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510748188, Iteration 1284\n",
      "batches :1284 0.034100068858750755\n",
      "Iteration 504: tag train_accuracy, simple_value 0.17302\n",
      "Iteration 504: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510748495, Iteration 1285\n",
      "batches :1285 0.034097192178268246\n",
      "Iteration 505: tag train_accuracy, simple_value 0.17305\n",
      "Iteration 505: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510748802, Iteration 1286\n",
      "batches :1286 0.034100894392891365\n",
      "Iteration 506: tag train_accuracy, simple_value 0.17299\n",
      "Iteration 506: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510749108, Iteration 1287\n",
      "batches :1287 0.03409895233266217\n",
      "Iteration 507: tag train_accuracy, simple_value 0.17302\n",
      "Iteration 507: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510749415, Iteration 1288\n",
      "batches :1288 0.03409795879613696\n",
      "Iteration 508: tag train_accuracy, simple_value 0.17295\n",
      "Iteration 508: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510749722, Iteration 1289\n",
      "batches :1289 0.034098144203483005\n",
      "Iteration 509: tag train_accuracy, simple_value 0.17292\n",
      "Iteration 509: tag train_loss, simple_value 0.0341\n",
      "Timestamp 1612510750029, Iteration 1290\n",
      "batches :1290 0.03409203507152258\n",
      "Iteration 510: tag train_accuracy, simple_value 0.17319\n",
      "Iteration 510: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510750337, Iteration 1291\n",
      "batches :1291 0.03408939738387931\n",
      "Iteration 511: tag train_accuracy, simple_value 0.17328\n",
      "Iteration 511: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510750644, Iteration 1292\n",
      "batches :1292 0.03408985114947427\n",
      "Iteration 512: tag train_accuracy, simple_value 0.17331\n",
      "Iteration 512: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510750965, Iteration 1293\n",
      "batches :1293 0.03408934764171902\n",
      "Iteration 513: tag train_accuracy, simple_value 0.1734\n",
      "Iteration 513: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510751273, Iteration 1294\n",
      "batches :1294 0.03408733598800948\n",
      "Iteration 514: tag train_accuracy, simple_value 0.17333\n",
      "Iteration 514: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510751578, Iteration 1295\n",
      "batches :1295 0.034088153616317264\n",
      "Iteration 515: tag train_accuracy, simple_value 0.17321\n",
      "Iteration 515: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510751884, Iteration 1296\n",
      "batches :1296 0.034087840677058515\n",
      "Iteration 516: tag train_accuracy, simple_value 0.17312\n",
      "Iteration 516: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510752191, Iteration 1297\n",
      "batches :1297 0.034085759219337014\n",
      "Iteration 517: tag train_accuracy, simple_value 0.17311\n",
      "Iteration 517: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510752498, Iteration 1298\n",
      "batches :1298 0.0340811137872313\n",
      "Iteration 518: tag train_accuracy, simple_value 0.17314\n",
      "Iteration 518: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510752805, Iteration 1299\n",
      "batches :1299 0.034081609750988395\n",
      "Iteration 519: tag train_accuracy, simple_value 0.17314\n",
      "Iteration 519: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510753113, Iteration 1300\n",
      "batches :1300 0.034082788995538765\n",
      "Iteration 520: tag train_accuracy, simple_value 0.17305\n",
      "Iteration 520: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510753419, Iteration 1301\n",
      "batches :1301 0.03408467374212911\n",
      "Iteration 521: tag train_accuracy, simple_value 0.1731\n",
      "Iteration 521: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510753725, Iteration 1302\n",
      "batches :1302 0.034089859032653755\n",
      "Iteration 522: tag train_accuracy, simple_value 0.17286\n",
      "Iteration 522: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510754032, Iteration 1303\n",
      "batches :1303 0.03408828400517057\n",
      "Iteration 523: tag train_accuracy, simple_value 0.17298\n",
      "Iteration 523: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510754338, Iteration 1304\n",
      "batches :1304 0.03408506766445082\n",
      "Iteration 524: tag train_accuracy, simple_value 0.17307\n",
      "Iteration 524: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510754646, Iteration 1305\n",
      "batches :1305 0.03408430816871779\n",
      "Iteration 525: tag train_accuracy, simple_value 0.17301\n",
      "Iteration 525: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510754954, Iteration 1306\n",
      "batches :1306 0.03408535363811277\n",
      "Iteration 526: tag train_accuracy, simple_value 0.17286\n",
      "Iteration 526: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510755261, Iteration 1307\n",
      "batches :1307 0.034083536499139255\n",
      "Iteration 527: tag train_accuracy, simple_value 0.17285\n",
      "Iteration 527: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510755567, Iteration 1308\n",
      "batches :1308 0.034083217791648523\n",
      "Iteration 528: tag train_accuracy, simple_value 0.17288\n",
      "Iteration 528: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510755886, Iteration 1309\n",
      "batches :1309 0.034086067947807296\n",
      "Iteration 529: tag train_accuracy, simple_value 0.17282\n",
      "Iteration 529: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510756192, Iteration 1310\n",
      "batches :1310 0.03408497096092071\n",
      "Iteration 530: tag train_accuracy, simple_value 0.17291\n",
      "Iteration 530: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510756498, Iteration 1311\n",
      "batches :1311 0.03408578531413204\n",
      "Iteration 531: tag train_accuracy, simple_value 0.17288\n",
      "Iteration 531: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510756806, Iteration 1312\n",
      "batches :1312 0.03408633976390487\n",
      "Iteration 532: tag train_accuracy, simple_value 0.17302\n",
      "Iteration 532: tag train_loss, simple_value 0.03409\n",
      "Timestamp 1612510757113, Iteration 1313\n",
      "batches :1313 0.034083014008140025\n",
      "Iteration 533: tag train_accuracy, simple_value 0.17305\n",
      "Iteration 533: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510757420, Iteration 1314\n",
      "batches :1314 0.03408001968811514\n",
      "Iteration 534: tag train_accuracy, simple_value 0.17325\n",
      "Iteration 534: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510757726, Iteration 1315\n",
      "batches :1315 0.0340787474936414\n",
      "Iteration 535: tag train_accuracy, simple_value 0.17316\n",
      "Iteration 535: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510758033, Iteration 1316\n",
      "batches :1316 0.034077497463284145\n",
      "Iteration 536: tag train_accuracy, simple_value 0.17327\n",
      "Iteration 536: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510758339, Iteration 1317\n",
      "batches :1317 0.034075564359708425\n",
      "Iteration 537: tag train_accuracy, simple_value 0.17333\n",
      "Iteration 537: tag train_loss, simple_value 0.03408\n",
      "Timestamp 1612510758646, Iteration 1318\n",
      "batches :1318 0.03407308201696793\n",
      "Iteration 538: tag train_accuracy, simple_value 0.17333\n",
      "Iteration 538: tag train_loss, simple_value 0.03407\n",
      "Timestamp 1612510758952, Iteration 1319\n",
      "batches :1319 0.03406945527539846\n",
      "Iteration 539: tag train_accuracy, simple_value 0.1735\n",
      "Iteration 539: tag train_loss, simple_value 0.03407\n",
      "Timestamp 1612510759258, Iteration 1320\n",
      "batches :1320 0.03406828008592129\n",
      "Iteration 540: tag train_accuracy, simple_value 0.17344\n",
      "Iteration 540: tag train_loss, simple_value 0.03407\n",
      "Timestamp 1612510759565, Iteration 1321\n",
      "batches :1321 0.03406687448569014\n",
      "Iteration 541: tag train_accuracy, simple_value 0.17343\n",
      "Iteration 541: tag train_loss, simple_value 0.03407\n",
      "Timestamp 1612510759871, Iteration 1322\n",
      "batches :1322 0.03406501762101571\n",
      "Iteration 542: tag train_accuracy, simple_value 0.17343\n",
      "Iteration 542: tag train_loss, simple_value 0.03407\n",
      "Timestamp 1612510760178, Iteration 1323\n",
      "batches :1323 0.03406541033015067\n",
      "Iteration 543: tag train_accuracy, simple_value 0.17349\n",
      "Iteration 543: tag train_loss, simple_value 0.03407\n",
      "Timestamp 1612510760483, Iteration 1324\n",
      "batches :1324 0.034063296372016126\n",
      "Iteration 544: tag train_accuracy, simple_value 0.17343\n",
      "Iteration 544: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510760804, Iteration 1325\n",
      "batches :1325 0.03406273003565062\n",
      "Iteration 545: tag train_accuracy, simple_value 0.17345\n",
      "Iteration 545: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510761112, Iteration 1326\n",
      "batches :1326 0.03406248426562919\n",
      "Iteration 546: tag train_accuracy, simple_value 0.17351\n",
      "Iteration 546: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510761418, Iteration 1327\n",
      "batches :1327 0.034064037497180906\n",
      "Iteration 547: tag train_accuracy, simple_value 0.1735\n",
      "Iteration 547: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510761726, Iteration 1328\n",
      "batches :1328 0.03406384288856801\n",
      "Iteration 548: tag train_accuracy, simple_value 0.17361\n",
      "Iteration 548: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510762034, Iteration 1329\n",
      "batches :1329 0.034063662987589186\n",
      "Iteration 549: tag train_accuracy, simple_value 0.17347\n",
      "Iteration 549: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510762341, Iteration 1330\n",
      "batches :1330 0.03406349216672507\n",
      "Iteration 550: tag train_accuracy, simple_value 0.17358\n",
      "Iteration 550: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510762648, Iteration 1331\n",
      "batches :1331 0.03406031024785959\n",
      "Iteration 551: tag train_accuracy, simple_value 0.17366\n",
      "Iteration 551: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510762957, Iteration 1332\n",
      "batches :1332 0.03405715995535687\n",
      "Iteration 552: tag train_accuracy, simple_value 0.17366\n",
      "Iteration 552: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510763268, Iteration 1333\n",
      "batches :1333 0.03405761535516915\n",
      "Iteration 553: tag train_accuracy, simple_value 0.17354\n",
      "Iteration 553: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510763575, Iteration 1334\n",
      "batches :1334 0.034056765044155106\n",
      "Iteration 554: tag train_accuracy, simple_value 0.17348\n",
      "Iteration 554: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510763889, Iteration 1335\n",
      "batches :1335 0.034056463723515604\n",
      "Iteration 555: tag train_accuracy, simple_value 0.17334\n",
      "Iteration 555: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510764197, Iteration 1336\n",
      "batches :1336 0.03405513592305587\n",
      "Iteration 556: tag train_accuracy, simple_value 0.17348\n",
      "Iteration 556: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510764506, Iteration 1337\n",
      "batches :1337 0.03405737868197516\n",
      "Iteration 557: tag train_accuracy, simple_value 0.17345\n",
      "Iteration 557: tag train_loss, simple_value 0.03406\n",
      "Timestamp 1612510764813, Iteration 1338\n",
      "batches :1338 0.03405436198477463\n",
      "Iteration 558: tag train_accuracy, simple_value 0.17356\n",
      "Iteration 558: tag train_loss, simple_value 0.03405\n",
      "Timestamp 1612510765121, Iteration 1339\n",
      "batches :1339 0.03405317174993815\n",
      "Iteration 559: tag train_accuracy, simple_value 0.17355\n",
      "Iteration 559: tag train_loss, simple_value 0.03405\n",
      "Timestamp 1612510765429, Iteration 1340\n",
      "batches :1340 0.034050645546189376\n",
      "Iteration 560: tag train_accuracy, simple_value 0.17366\n",
      "Iteration 560: tag train_loss, simple_value 0.03405\n",
      "Timestamp 1612510765751, Iteration 1341\n",
      "batches :1341 0.034047168923571784\n",
      "Iteration 561: tag train_accuracy, simple_value 0.17374\n",
      "Iteration 561: tag train_loss, simple_value 0.03405\n",
      "Timestamp 1612510766058, Iteration 1342\n",
      "batches :1342 0.03404294611932544\n",
      "Iteration 562: tag train_accuracy, simple_value 0.17399\n",
      "Iteration 562: tag train_loss, simple_value 0.03404\n",
      "Timestamp 1612510766364, Iteration 1343\n",
      "batches :1343 0.03403916334333047\n",
      "Iteration 563: tag train_accuracy, simple_value 0.17398\n",
      "Iteration 563: tag train_loss, simple_value 0.03404\n",
      "Timestamp 1612510766672, Iteration 1344\n",
      "batches :1344 0.034036004704469484\n",
      "Iteration 564: tag train_accuracy, simple_value 0.17401\n",
      "Iteration 564: tag train_loss, simple_value 0.03404\n",
      "Timestamp 1612510766980, Iteration 1345\n",
      "batches :1345 0.03403358095515091\n",
      "Iteration 565: tag train_accuracy, simple_value 0.17412\n",
      "Iteration 565: tag train_loss, simple_value 0.03403\n",
      "Timestamp 1612510767287, Iteration 1346\n",
      "batches :1346 0.03403111625554915\n",
      "Iteration 566: tag train_accuracy, simple_value 0.17403\n",
      "Iteration 566: tag train_loss, simple_value 0.03403\n",
      "Timestamp 1612510767594, Iteration 1347\n",
      "batches :1347 0.03403059814972852\n",
      "Iteration 567: tag train_accuracy, simple_value 0.17402\n",
      "Iteration 567: tag train_loss, simple_value 0.03403\n",
      "Timestamp 1612510767901, Iteration 1348\n",
      "batches :1348 0.03402988465083107\n",
      "Iteration 568: tag train_accuracy, simple_value 0.1741\n",
      "Iteration 568: tag train_loss, simple_value 0.03403\n",
      "Timestamp 1612510768208, Iteration 1349\n",
      "batches :1349 0.03402642441523934\n",
      "Iteration 569: tag train_accuracy, simple_value 0.17399\n",
      "Iteration 569: tag train_loss, simple_value 0.03403\n",
      "Timestamp 1612510768516, Iteration 1350\n",
      "batches :1350 0.034025682927223674\n",
      "Iteration 570: tag train_accuracy, simple_value 0.17401\n",
      "Iteration 570: tag train_loss, simple_value 0.03403\n",
      "Timestamp 1612510768824, Iteration 1351\n",
      "batches :1351 0.03402231212203014\n",
      "Iteration 571: tag train_accuracy, simple_value 0.17393\n",
      "Iteration 571: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510769131, Iteration 1352\n",
      "batches :1352 0.0340211145397458\n",
      "Iteration 572: tag train_accuracy, simple_value 0.17392\n",
      "Iteration 572: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510769438, Iteration 1353\n",
      "batches :1353 0.034020370239778754\n",
      "Iteration 573: tag train_accuracy, simple_value 0.17392\n",
      "Iteration 573: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510769746, Iteration 1354\n",
      "batches :1354 0.03401677280612524\n",
      "Iteration 574: tag train_accuracy, simple_value 0.17403\n",
      "Iteration 574: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510770053, Iteration 1355\n",
      "batches :1355 0.03401561073634935\n",
      "Iteration 575: tag train_accuracy, simple_value 0.17391\n",
      "Iteration 575: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510770359, Iteration 1356\n",
      "batches :1356 0.034018588743896946\n",
      "Iteration 576: tag train_accuracy, simple_value 0.17372\n",
      "Iteration 576: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510770679, Iteration 1357\n",
      "batches :1357 0.034018834604134386\n",
      "Iteration 577: tag train_accuracy, simple_value 0.17369\n",
      "Iteration 577: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510770985, Iteration 1358\n",
      "batches :1358 0.034018045297749724\n",
      "Iteration 578: tag train_accuracy, simple_value 0.17374\n",
      "Iteration 578: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510771291, Iteration 1359\n",
      "batches :1359 0.03401395460407351\n",
      "Iteration 579: tag train_accuracy, simple_value 0.17384\n",
      "Iteration 579: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510771598, Iteration 1360\n",
      "batches :1360 0.03401405463187859\n",
      "Iteration 580: tag train_accuracy, simple_value 0.17392\n",
      "Iteration 580: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510771907, Iteration 1361\n",
      "batches :1361 0.03401350004281111\n",
      "Iteration 581: tag train_accuracy, simple_value 0.17403\n",
      "Iteration 581: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510772213, Iteration 1362\n",
      "batches :1362 0.03401562204117218\n",
      "Iteration 582: tag train_accuracy, simple_value 0.17386\n",
      "Iteration 582: tag train_loss, simple_value 0.03402\n",
      "Timestamp 1612510772520, Iteration 1363\n",
      "batches :1363 0.03401353433217029\n",
      "Iteration 583: tag train_accuracy, simple_value 0.17394\n",
      "Iteration 583: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510772826, Iteration 1364\n",
      "batches :1364 0.034013071314316905\n",
      "Iteration 584: tag train_accuracy, simple_value 0.17391\n",
      "Iteration 584: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510773134, Iteration 1365\n",
      "batches :1365 0.03401327085418579\n",
      "Iteration 585: tag train_accuracy, simple_value 0.17382\n",
      "Iteration 585: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510773442, Iteration 1366\n",
      "batches :1366 0.034012035933981166\n",
      "Iteration 586: tag train_accuracy, simple_value 0.17387\n",
      "Iteration 586: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510773748, Iteration 1367\n",
      "batches :1367 0.03400867398085749\n",
      "Iteration 587: tag train_accuracy, simple_value 0.17387\n",
      "Iteration 587: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510774055, Iteration 1368\n",
      "batches :1368 0.034009208833025835\n",
      "Iteration 588: tag train_accuracy, simple_value 0.17381\n",
      "Iteration 588: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510774362, Iteration 1369\n",
      "batches :1369 0.03400571884647492\n",
      "Iteration 589: tag train_accuracy, simple_value 0.17384\n",
      "Iteration 589: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510774669, Iteration 1370\n",
      "batches :1370 0.034009070662876305\n",
      "Iteration 590: tag train_accuracy, simple_value 0.17381\n",
      "Iteration 590: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510774976, Iteration 1371\n",
      "batches :1371 0.03400762046981987\n",
      "Iteration 591: tag train_accuracy, simple_value 0.17375\n",
      "Iteration 591: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510775282, Iteration 1372\n",
      "batches :1372 0.034008824272786044\n",
      "Iteration 592: tag train_accuracy, simple_value 0.1737\n",
      "Iteration 592: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510775603, Iteration 1373\n",
      "batches :1373 0.03400637393481213\n",
      "Iteration 593: tag train_accuracy, simple_value 0.17388\n",
      "Iteration 593: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510775909, Iteration 1374\n",
      "batches :1374 0.034005801798619006\n",
      "Iteration 594: tag train_accuracy, simple_value 0.17398\n",
      "Iteration 594: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510776215, Iteration 1375\n",
      "batches :1375 0.034006452084589404\n",
      "Iteration 595: tag train_accuracy, simple_value 0.17398\n",
      "Iteration 595: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510776523, Iteration 1376\n",
      "batches :1376 0.034007345620017726\n",
      "Iteration 596: tag train_accuracy, simple_value 0.17384\n",
      "Iteration 596: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510776830, Iteration 1377\n",
      "batches :1377 0.0340069043988259\n",
      "Iteration 597: tag train_accuracy, simple_value 0.17384\n",
      "Iteration 597: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510777137, Iteration 1378\n",
      "batches :1378 0.03400517726647216\n",
      "Iteration 598: tag train_accuracy, simple_value 0.1737\n",
      "Iteration 598: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510777443, Iteration 1379\n",
      "batches :1379 0.034006352286058195\n",
      "Iteration 599: tag train_accuracy, simple_value 0.17365\n",
      "Iteration 599: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510777749, Iteration 1380\n",
      "batches :1380 0.03400667828818162\n",
      "Iteration 600: tag train_accuracy, simple_value 0.1737\n",
      "Iteration 600: tag train_loss, simple_value 0.03401\n",
      "Timestamp 1612510778056, Iteration 1381\n",
      "batches :1381 0.0340042598669521\n",
      "Iteration 601: tag train_accuracy, simple_value 0.17377\n",
      "Iteration 601: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510778364, Iteration 1382\n",
      "batches :1382 0.03400465084914353\n",
      "Iteration 602: tag train_accuracy, simple_value 0.17385\n",
      "Iteration 602: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510778673, Iteration 1383\n",
      "batches :1383 0.034004422457607625\n",
      "Iteration 603: tag train_accuracy, simple_value 0.17374\n",
      "Iteration 603: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510778979, Iteration 1384\n",
      "batches :1384 0.034004810482066196\n",
      "Iteration 604: tag train_accuracy, simple_value 0.17376\n",
      "Iteration 604: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510779284, Iteration 1385\n",
      "batches :1385 0.034004521345304066\n",
      "Iteration 605: tag train_accuracy, simple_value 0.17368\n",
      "Iteration 605: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510779590, Iteration 1386\n",
      "batches :1386 0.03400263262808126\n",
      "Iteration 606: tag train_accuracy, simple_value 0.1736\n",
      "Iteration 606: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510779896, Iteration 1387\n",
      "batches :1387 0.03400298636375974\n",
      "Iteration 607: tag train_accuracy, simple_value 0.17368\n",
      "Iteration 607: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510780201, Iteration 1388\n",
      "batches :1388 0.03400036179779196\n",
      "Iteration 608: tag train_accuracy, simple_value 0.17373\n",
      "Iteration 608: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510780519, Iteration 1389\n",
      "batches :1389 0.03399900726863903\n",
      "Iteration 609: tag train_accuracy, simple_value 0.17367\n",
      "Iteration 609: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510780824, Iteration 1390\n",
      "batches :1390 0.0340001962773624\n",
      "Iteration 610: tag train_accuracy, simple_value 0.17374\n",
      "Iteration 610: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510781129, Iteration 1391\n",
      "batches :1391 0.03400181494226432\n",
      "Iteration 611: tag train_accuracy, simple_value 0.17374\n",
      "Iteration 611: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510781437, Iteration 1392\n",
      "batches :1392 0.0340001884653093\n",
      "Iteration 612: tag train_accuracy, simple_value 0.17369\n",
      "Iteration 612: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510781743, Iteration 1393\n",
      "batches :1393 0.033998377554325256\n",
      "Iteration 613: tag train_accuracy, simple_value 0.17376\n",
      "Iteration 613: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510782049, Iteration 1394\n",
      "batches :1394 0.033999355855710345\n",
      "Iteration 614: tag train_accuracy, simple_value 0.17366\n",
      "Iteration 614: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510782354, Iteration 1395\n",
      "batches :1395 0.033997324072733155\n",
      "Iteration 615: tag train_accuracy, simple_value 0.17368\n",
      "Iteration 615: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510782659, Iteration 1396\n",
      "batches :1396 0.03399644042788581\n",
      "Iteration 616: tag train_accuracy, simple_value 0.17378\n",
      "Iteration 616: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510782965, Iteration 1397\n",
      "batches :1397 0.033995579487404136\n",
      "Iteration 617: tag train_accuracy, simple_value 0.17367\n",
      "Iteration 617: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510783270, Iteration 1398\n",
      "batches :1398 0.03399558516647245\n",
      "Iteration 618: tag train_accuracy, simple_value 0.17349\n",
      "Iteration 618: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510783576, Iteration 1399\n",
      "batches :1399 0.03399329359894423\n",
      "Iteration 619: tag train_accuracy, simple_value 0.17359\n",
      "Iteration 619: tag train_loss, simple_value 0.03399\n",
      "Timestamp 1612510783883, Iteration 1400\n",
      "batches :1400 0.03399346998982852\n",
      "Iteration 620: tag train_accuracy, simple_value 0.17356\n",
      "Iteration 620: tag train_loss, simple_value 0.03399\n",
      "Timestamp 1612510784188, Iteration 1401\n",
      "batches :1401 0.03399502845921954\n",
      "Iteration 621: tag train_accuracy, simple_value 0.17354\n",
      "Iteration 621: tag train_loss, simple_value 0.034\n",
      "Timestamp 1612510784494, Iteration 1402\n",
      "batches :1402 0.03399077265065199\n",
      "Iteration 622: tag train_accuracy, simple_value 0.17348\n",
      "Iteration 622: tag train_loss, simple_value 0.03399\n",
      "Timestamp 1612510784801, Iteration 1403\n",
      "batches :1403 0.03398697134436229\n",
      "Iteration 623: tag train_accuracy, simple_value 0.17353\n",
      "Iteration 623: tag train_loss, simple_value 0.03399\n",
      "Timestamp 1612510785106, Iteration 1404\n",
      "batches :1404 0.03398604598493339\n",
      "Iteration 624: tag train_accuracy, simple_value 0.17338\n",
      "Iteration 624: tag train_loss, simple_value 0.03399\n",
      "Timestamp 1612510785424, Iteration 1405\n",
      "batches :1405 0.0339844441652298\n",
      "Iteration 625: tag train_accuracy, simple_value 0.17338\n",
      "Iteration 625: tag train_loss, simple_value 0.03398\n",
      "Timestamp 1612510785729, Iteration 1406\n",
      "batches :1406 0.03398067296479647\n",
      "Iteration 626: tag train_accuracy, simple_value 0.17335\n",
      "Iteration 626: tag train_loss, simple_value 0.03398\n",
      "Timestamp 1612510786036, Iteration 1407\n",
      "batches :1407 0.033978920762666294\n",
      "Iteration 627: tag train_accuracy, simple_value 0.1734\n",
      "Iteration 627: tag train_loss, simple_value 0.03398\n",
      "Timestamp 1612510786343, Iteration 1408\n",
      "batches :1408 0.033976154740021865\n",
      "Iteration 628: tag train_accuracy, simple_value 0.17347\n",
      "Iteration 628: tag train_loss, simple_value 0.03398\n",
      "Timestamp 1612510786649, Iteration 1409\n",
      "batches :1409 0.03397789560199163\n",
      "Iteration 629: tag train_accuracy, simple_value 0.17354\n",
      "Iteration 629: tag train_loss, simple_value 0.03398\n",
      "Timestamp 1612510786954, Iteration 1410\n",
      "batches :1410 0.0339766118143286\n",
      "Iteration 630: tag train_accuracy, simple_value 0.17354\n",
      "Iteration 630: tag train_loss, simple_value 0.03398\n",
      "Timestamp 1612510787260, Iteration 1411\n",
      "batches :1411 0.03397614495288551\n",
      "Iteration 631: tag train_accuracy, simple_value 0.17344\n",
      "Iteration 631: tag train_loss, simple_value 0.03398\n",
      "Timestamp 1612510787566, Iteration 1412\n",
      "batches :1412 0.03397413076762157\n",
      "Iteration 632: tag train_accuracy, simple_value 0.17341\n",
      "Iteration 632: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510787872, Iteration 1413\n",
      "batches :1413 0.03397136745977364\n",
      "Iteration 633: tag train_accuracy, simple_value 0.17328\n",
      "Iteration 633: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510788177, Iteration 1414\n",
      "batches :1414 0.03396963478544909\n",
      "Iteration 634: tag train_accuracy, simple_value 0.17335\n",
      "Iteration 634: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510788483, Iteration 1415\n",
      "batches :1415 0.03396965231247774\n",
      "Iteration 635: tag train_accuracy, simple_value 0.1733\n",
      "Iteration 635: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510788790, Iteration 1416\n",
      "batches :1416 0.03396976121215138\n",
      "Iteration 636: tag train_accuracy, simple_value 0.17332\n",
      "Iteration 636: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510789097, Iteration 1417\n",
      "batches :1417 0.033969318807078305\n",
      "Iteration 637: tag train_accuracy, simple_value 0.1733\n",
      "Iteration 637: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510789402, Iteration 1418\n",
      "batches :1418 0.03396591585024397\n",
      "Iteration 638: tag train_accuracy, simple_value 0.17327\n",
      "Iteration 638: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510789708, Iteration 1419\n",
      "batches :1419 0.03396450708356449\n",
      "Iteration 639: tag train_accuracy, simple_value 0.17337\n",
      "Iteration 639: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510790013, Iteration 1420\n",
      "batches :1420 0.033965692104538904\n",
      "Iteration 640: tag train_accuracy, simple_value 0.17341\n",
      "Iteration 640: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510790331, Iteration 1421\n",
      "batches :1421 0.033965606521379185\n",
      "Iteration 641: tag train_accuracy, simple_value 0.17353\n",
      "Iteration 641: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510790636, Iteration 1422\n",
      "batches :1422 0.033967516167420095\n",
      "Iteration 642: tag train_accuracy, simple_value 0.17348\n",
      "Iteration 642: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510790941, Iteration 1423\n",
      "batches :1423 0.033966327210653624\n",
      "Iteration 643: tag train_accuracy, simple_value 0.17353\n",
      "Iteration 643: tag train_loss, simple_value 0.03397\n",
      "Timestamp 1612510791249, Iteration 1424\n",
      "batches :1424 0.03396495841212154\n",
      "Iteration 644: tag train_accuracy, simple_value 0.17355\n",
      "Iteration 644: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510791555, Iteration 1425\n",
      "batches :1425 0.03396401377611382\n",
      "Iteration 645: tag train_accuracy, simple_value 0.17364\n",
      "Iteration 645: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510791860, Iteration 1426\n",
      "batches :1426 0.03396369203140861\n",
      "Iteration 646: tag train_accuracy, simple_value 0.17359\n",
      "Iteration 646: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510792168, Iteration 1427\n",
      "batches :1427 0.0339629229106343\n",
      "Iteration 647: tag train_accuracy, simple_value 0.17373\n",
      "Iteration 647: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510792477, Iteration 1428\n",
      "batches :1428 0.03396265320249914\n",
      "Iteration 648: tag train_accuracy, simple_value 0.17378\n",
      "Iteration 648: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510792785, Iteration 1429\n",
      "batches :1429 0.03396051424126412\n",
      "Iteration 649: tag train_accuracy, simple_value 0.17387\n",
      "Iteration 649: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510793092, Iteration 1430\n",
      "batches :1430 0.03395751539331216\n",
      "Iteration 650: tag train_accuracy, simple_value 0.17397\n",
      "Iteration 650: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510793398, Iteration 1431\n",
      "batches :1431 0.033956373073217874\n",
      "Iteration 651: tag train_accuracy, simple_value 0.17404\n",
      "Iteration 651: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510793705, Iteration 1432\n",
      "batches :1432 0.03395402297217605\n",
      "Iteration 652: tag train_accuracy, simple_value 0.1741\n",
      "Iteration 652: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510794014, Iteration 1433\n",
      "batches :1433 0.03395505479977653\n",
      "Iteration 653: tag train_accuracy, simple_value 0.17415\n",
      "Iteration 653: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510794320, Iteration 1434\n",
      "batches :1434 0.03395552736698487\n",
      "Iteration 654: tag train_accuracy, simple_value 0.17419\n",
      "Iteration 654: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510794627, Iteration 1435\n",
      "batches :1435 0.033957936495315026\n",
      "Iteration 655: tag train_accuracy, simple_value 0.17421\n",
      "Iteration 655: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510794934, Iteration 1436\n",
      "batches :1436 0.033958334634761986\n",
      "Iteration 656: tag train_accuracy, simple_value 0.17419\n",
      "Iteration 656: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510795252, Iteration 1437\n",
      "batches :1437 0.033958960715766366\n",
      "Iteration 657: tag train_accuracy, simple_value 0.17411\n",
      "Iteration 657: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510795556, Iteration 1438\n",
      "batches :1438 0.033957581689700166\n",
      "Iteration 658: tag train_accuracy, simple_value 0.17423\n",
      "Iteration 658: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510795862, Iteration 1439\n",
      "batches :1439 0.03395864449452739\n",
      "Iteration 659: tag train_accuracy, simple_value 0.17413\n",
      "Iteration 659: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510796168, Iteration 1440\n",
      "batches :1440 0.03395569004457105\n",
      "Iteration 660: tag train_accuracy, simple_value 0.17422\n",
      "Iteration 660: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510796473, Iteration 1441\n",
      "batches :1441 0.033955315683099035\n",
      "Iteration 661: tag train_accuracy, simple_value 0.17424\n",
      "Iteration 661: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510796779, Iteration 1442\n",
      "batches :1442 0.03395722854285982\n",
      "Iteration 662: tag train_accuracy, simple_value 0.17428\n",
      "Iteration 662: tag train_loss, simple_value 0.03396\n",
      "Timestamp 1612510797085, Iteration 1443\n",
      "batches :1443 0.03395461757438395\n",
      "Iteration 663: tag train_accuracy, simple_value 0.17433\n",
      "Iteration 663: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510797390, Iteration 1444\n",
      "batches :1444 0.03395439264173788\n",
      "Iteration 664: tag train_accuracy, simple_value 0.17437\n",
      "Iteration 664: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510797695, Iteration 1445\n",
      "batches :1445 0.03395350824733426\n",
      "Iteration 665: tag train_accuracy, simple_value 0.17437\n",
      "Iteration 665: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510798000, Iteration 1446\n",
      "batches :1446 0.03395436877558181\n",
      "Iteration 666: tag train_accuracy, simple_value 0.17429\n",
      "Iteration 666: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510798305, Iteration 1447\n",
      "batches :1447 0.033954441212285226\n",
      "Iteration 667: tag train_accuracy, simple_value 0.17431\n",
      "Iteration 667: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510798612, Iteration 1448\n",
      "batches :1448 0.033950878145496646\n",
      "Iteration 668: tag train_accuracy, simple_value 0.17438\n",
      "Iteration 668: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510798919, Iteration 1449\n",
      "batches :1449 0.033950154083503915\n",
      "Iteration 669: tag train_accuracy, simple_value 0.17433\n",
      "Iteration 669: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510799224, Iteration 1450\n",
      "batches :1450 0.03394921106745058\n",
      "Iteration 670: tag train_accuracy, simple_value 0.1743\n",
      "Iteration 670: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510799529, Iteration 1451\n",
      "batches :1451 0.03394744614619434\n",
      "Iteration 671: tag train_accuracy, simple_value 0.17427\n",
      "Iteration 671: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510799835, Iteration 1452\n",
      "batches :1452 0.03394590046567222\n",
      "Iteration 672: tag train_accuracy, simple_value 0.17427\n",
      "Iteration 672: tag train_loss, simple_value 0.03395\n",
      "Timestamp 1612510800154, Iteration 1453\n",
      "batches :1453 0.033943142852625556\n",
      "Iteration 673: tag train_accuracy, simple_value 0.17436\n",
      "Iteration 673: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510800459, Iteration 1454\n",
      "batches :1454 0.033944765144533505\n",
      "Iteration 674: tag train_accuracy, simple_value 0.17424\n",
      "Iteration 674: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510800764, Iteration 1455\n",
      "batches :1455 0.03394375767972734\n",
      "Iteration 675: tag train_accuracy, simple_value 0.17426\n",
      "Iteration 675: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510801070, Iteration 1456\n",
      "batches :1456 0.03394119620058664\n",
      "Iteration 676: tag train_accuracy, simple_value 0.1743\n",
      "Iteration 676: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510801377, Iteration 1457\n",
      "batches :1457 0.03394194613135196\n",
      "Iteration 677: tag train_accuracy, simple_value 0.17428\n",
      "Iteration 677: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510801683, Iteration 1458\n",
      "batches :1458 0.03394254761305706\n",
      "Iteration 678: tag train_accuracy, simple_value 0.17429\n",
      "Iteration 678: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510801989, Iteration 1459\n",
      "batches :1459 0.03393845949788332\n",
      "Iteration 679: tag train_accuracy, simple_value 0.17445\n",
      "Iteration 679: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510802294, Iteration 1460\n",
      "batches :1460 0.03393896433579571\n",
      "Iteration 680: tag train_accuracy, simple_value 0.17443\n",
      "Iteration 680: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510802599, Iteration 1461\n",
      "batches :1461 0.03394059149791841\n",
      "Iteration 681: tag train_accuracy, simple_value 0.17438\n",
      "Iteration 681: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510802904, Iteration 1462\n",
      "batches :1462 0.03393998650883125\n",
      "Iteration 682: tag train_accuracy, simple_value 0.1743\n",
      "Iteration 682: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510803209, Iteration 1463\n",
      "batches :1463 0.03394023915902762\n",
      "Iteration 683: tag train_accuracy, simple_value 0.17421\n",
      "Iteration 683: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510803517, Iteration 1464\n",
      "batches :1464 0.033939250499794356\n",
      "Iteration 684: tag train_accuracy, simple_value 0.17423\n",
      "Iteration 684: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510803823, Iteration 1465\n",
      "batches :1465 0.033937083608912726\n",
      "Iteration 685: tag train_accuracy, simple_value 0.17425\n",
      "Iteration 685: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510804128, Iteration 1466\n",
      "batches :1466 0.03393748751773605\n",
      "Iteration 686: tag train_accuracy, simple_value 0.1742\n",
      "Iteration 686: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510804434, Iteration 1467\n",
      "batches :1467 0.0339343276259854\n",
      "Iteration 687: tag train_accuracy, simple_value 0.17424\n",
      "Iteration 687: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510804740, Iteration 1468\n",
      "batches :1468 0.03393221889665827\n",
      "Iteration 688: tag train_accuracy, simple_value 0.17435\n",
      "Iteration 688: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510805059, Iteration 1469\n",
      "batches :1469 0.03393239071065525\n",
      "Iteration 689: tag train_accuracy, simple_value 0.17437\n",
      "Iteration 689: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510805365, Iteration 1470\n",
      "batches :1470 0.03393415261221969\n",
      "Iteration 690: tag train_accuracy, simple_value 0.17428\n",
      "Iteration 690: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510805670, Iteration 1471\n",
      "batches :1471 0.033934701460530896\n",
      "Iteration 691: tag train_accuracy, simple_value 0.17425\n",
      "Iteration 691: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510805977, Iteration 1472\n",
      "batches :1472 0.033935812727219794\n",
      "Iteration 692: tag train_accuracy, simple_value 0.17422\n",
      "Iteration 692: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510806283, Iteration 1473\n",
      "batches :1473 0.03393438682605178\n",
      "Iteration 693: tag train_accuracy, simple_value 0.17424\n",
      "Iteration 693: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510806588, Iteration 1474\n",
      "batches :1474 0.03393682004932196\n",
      "Iteration 694: tag train_accuracy, simple_value 0.17431\n",
      "Iteration 694: tag train_loss, simple_value 0.03394\n",
      "Timestamp 1612510806894, Iteration 1475\n",
      "batches :1475 0.03393494390326438\n",
      "Iteration 695: tag train_accuracy, simple_value 0.17433\n",
      "Iteration 695: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510807199, Iteration 1476\n",
      "batches :1476 0.03393385796433036\n",
      "Iteration 696: tag train_accuracy, simple_value 0.17428\n",
      "Iteration 696: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510807505, Iteration 1477\n",
      "batches :1477 0.0339305043712237\n",
      "Iteration 697: tag train_accuracy, simple_value 0.17427\n",
      "Iteration 697: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510807810, Iteration 1478\n",
      "batches :1478 0.03392996738816913\n",
      "Iteration 698: tag train_accuracy, simple_value 0.17427\n",
      "Iteration 698: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510808116, Iteration 1479\n",
      "batches :1479 0.03392673891906404\n",
      "Iteration 699: tag train_accuracy, simple_value 0.17431\n",
      "Iteration 699: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510808422, Iteration 1480\n",
      "batches :1480 0.03392539962061814\n",
      "Iteration 700: tag train_accuracy, simple_value 0.17442\n",
      "Iteration 700: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510808728, Iteration 1481\n",
      "batches :1481 0.03392408211076481\n",
      "Iteration 701: tag train_accuracy, simple_value 0.17446\n",
      "Iteration 701: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510809035, Iteration 1482\n",
      "batches :1482 0.033923666971998334\n",
      "Iteration 702: tag train_accuracy, simple_value 0.17446\n",
      "Iteration 702: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510809341, Iteration 1483\n",
      "batches :1483 0.033925858730315145\n",
      "Iteration 703: tag train_accuracy, simple_value 0.17454\n",
      "Iteration 703: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510809646, Iteration 1484\n",
      "batches :1484 0.03392537156733769\n",
      "Iteration 704: tag train_accuracy, simple_value 0.17458\n",
      "Iteration 704: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510809966, Iteration 1485\n",
      "batches :1485 0.03392501710154486\n",
      "Iteration 705: tag train_accuracy, simple_value 0.17456\n",
      "Iteration 705: tag train_loss, simple_value 0.03393\n",
      "Timestamp 1612510810271, Iteration 1486\n",
      "batches :1486 0.03392364006626707\n",
      "Iteration 706: tag train_accuracy, simple_value 0.17451\n",
      "Iteration 706: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510810577, Iteration 1487\n",
      "batches :1487 0.03392488933583297\n",
      "Iteration 707: tag train_accuracy, simple_value 0.17437\n",
      "Iteration 707: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510810884, Iteration 1488\n",
      "batches :1488 0.03392109286439957\n",
      "Iteration 708: tag train_accuracy, simple_value 0.17446\n",
      "Iteration 708: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510811191, Iteration 1489\n",
      "batches :1489 0.033919451687332604\n",
      "Iteration 709: tag train_accuracy, simple_value 0.17445\n",
      "Iteration 709: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510811496, Iteration 1490\n",
      "batches :1490 0.03391978510878455\n",
      "Iteration 710: tag train_accuracy, simple_value 0.17434\n",
      "Iteration 710: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510811801, Iteration 1491\n",
      "batches :1491 0.03391930182711988\n",
      "Iteration 711: tag train_accuracy, simple_value 0.17429\n",
      "Iteration 711: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510812107, Iteration 1492\n",
      "batches :1492 0.03391868484028605\n",
      "Iteration 712: tag train_accuracy, simple_value 0.17442\n",
      "Iteration 712: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510812414, Iteration 1493\n",
      "batches :1493 0.03391694171461283\n",
      "Iteration 713: tag train_accuracy, simple_value 0.1745\n",
      "Iteration 713: tag train_loss, simple_value 0.03392\n",
      "Timestamp 1612510812720, Iteration 1494\n",
      "batches :1494 0.03391333860384316\n",
      "Iteration 714: tag train_accuracy, simple_value 0.17465\n",
      "Iteration 714: tag train_loss, simple_value 0.03391\n",
      "Timestamp 1612510813025, Iteration 1495\n",
      "batches :1495 0.03391173897193862\n",
      "Iteration 715: tag train_accuracy, simple_value 0.17458\n",
      "Iteration 715: tag train_loss, simple_value 0.03391\n",
      "Timestamp 1612510813330, Iteration 1496\n",
      "batches :1496 0.03391185961663723\n",
      "Iteration 716: tag train_accuracy, simple_value 0.17456\n",
      "Iteration 716: tag train_loss, simple_value 0.03391\n",
      "Timestamp 1612510813636, Iteration 1497\n",
      "batches :1497 0.0339096578843424\n",
      "Iteration 717: tag train_accuracy, simple_value 0.17473\n",
      "Iteration 717: tag train_loss, simple_value 0.03391\n",
      "Timestamp 1612510813941, Iteration 1498\n",
      "batches :1498 0.033908971935552146\n",
      "Iteration 718: tag train_accuracy, simple_value 0.17481\n",
      "Iteration 718: tag train_loss, simple_value 0.03391\n",
      "Timestamp 1612510814246, Iteration 1499\n",
      "batches :1499 0.03390679451972453\n",
      "Iteration 719: tag train_accuracy, simple_value 0.1749\n",
      "Iteration 719: tag train_loss, simple_value 0.03391\n",
      "Timestamp 1612510814552, Iteration 1500\n",
      "batches :1500 0.03390589771378372\n",
      "Iteration 720: tag train_accuracy, simple_value 0.17498\n",
      "Iteration 720: tag train_loss, simple_value 0.03391\n",
      "Timestamp 1612510814870, Iteration 1501\n",
      "batches :1501 0.03390414227477059\n",
      "Iteration 721: tag train_accuracy, simple_value 0.17491\n",
      "Iteration 721: tag train_loss, simple_value 0.0339\n",
      "Timestamp 1612510815175, Iteration 1502\n",
      "batches :1502 0.033902378226540096\n",
      "Iteration 722: tag train_accuracy, simple_value 0.17499\n",
      "Iteration 722: tag train_loss, simple_value 0.0339\n",
      "Timestamp 1612510815481, Iteration 1503\n",
      "batches :1503 0.033902087971506276\n",
      "Iteration 723: tag train_accuracy, simple_value 0.17492\n",
      "Iteration 723: tag train_loss, simple_value 0.0339\n",
      "Timestamp 1612510815787, Iteration 1504\n",
      "batches :1504 0.03390080837831313\n",
      "Iteration 724: tag train_accuracy, simple_value 0.17498\n",
      "Iteration 724: tag train_loss, simple_value 0.0339\n",
      "Timestamp 1612510816092, Iteration 1505\n",
      "batches :1505 0.033898318260908125\n",
      "Iteration 725: tag train_accuracy, simple_value 0.17502\n",
      "Iteration 725: tag train_loss, simple_value 0.0339\n",
      "Timestamp 1612510816397, Iteration 1506\n",
      "batches :1506 0.033896177581231784\n",
      "Iteration 726: tag train_accuracy, simple_value 0.17508\n",
      "Iteration 726: tag train_loss, simple_value 0.0339\n",
      "Timestamp 1612510816703, Iteration 1507\n",
      "batches :1507 0.03389494702158473\n",
      "Iteration 727: tag train_accuracy, simple_value 0.17499\n",
      "Iteration 727: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510817008, Iteration 1508\n",
      "batches :1508 0.03389234780487451\n",
      "Iteration 728: tag train_accuracy, simple_value 0.17492\n",
      "Iteration 728: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510817313, Iteration 1509\n",
      "batches :1509 0.033893899309904975\n",
      "Iteration 729: tag train_accuracy, simple_value 0.1749\n",
      "Iteration 729: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510817619, Iteration 1510\n",
      "batches :1510 0.033893016807428775\n",
      "Iteration 730: tag train_accuracy, simple_value 0.17485\n",
      "Iteration 730: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510817924, Iteration 1511\n",
      "batches :1511 0.033893095510317425\n",
      "Iteration 731: tag train_accuracy, simple_value 0.17482\n",
      "Iteration 731: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510818229, Iteration 1512\n",
      "batches :1512 0.03389193548673326\n",
      "Iteration 732: tag train_accuracy, simple_value 0.17491\n",
      "Iteration 732: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510818535, Iteration 1513\n",
      "batches :1513 0.033893354562343714\n",
      "Iteration 733: tag train_accuracy, simple_value 0.17486\n",
      "Iteration 733: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510818840, Iteration 1514\n",
      "batches :1514 0.033893492504223494\n",
      "Iteration 734: tag train_accuracy, simple_value 0.17483\n",
      "Iteration 734: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510819146, Iteration 1515\n",
      "batches :1515 0.03389315927008382\n",
      "Iteration 735: tag train_accuracy, simple_value 0.17477\n",
      "Iteration 735: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510819451, Iteration 1516\n",
      "batches :1516 0.03389119443929066\n",
      "Iteration 736: tag train_accuracy, simple_value 0.1748\n",
      "Iteration 736: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510819769, Iteration 1517\n",
      "batches :1517 0.033888446058436036\n",
      "Iteration 737: tag train_accuracy, simple_value 0.17476\n",
      "Iteration 737: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510820075, Iteration 1518\n",
      "batches :1518 0.033888041589479785\n",
      "Iteration 738: tag train_accuracy, simple_value 0.17471\n",
      "Iteration 738: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510820380, Iteration 1519\n",
      "batches :1519 0.033886672840712034\n",
      "Iteration 739: tag train_accuracy, simple_value 0.17462\n",
      "Iteration 739: tag train_loss, simple_value 0.03389\n",
      "Timestamp 1612510820686, Iteration 1520\n",
      "batches :1520 0.03388297919032944\n",
      "Iteration 740: tag train_accuracy, simple_value 0.17483\n",
      "Iteration 740: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510820991, Iteration 1521\n",
      "batches :1521 0.033880165932692496\n",
      "Iteration 741: tag train_accuracy, simple_value 0.17493\n",
      "Iteration 741: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510821297, Iteration 1522\n",
      "batches :1522 0.03388040248967326\n",
      "Iteration 742: tag train_accuracy, simple_value 0.17497\n",
      "Iteration 742: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510821602, Iteration 1523\n",
      "batches :1523 0.03388296707732601\n",
      "Iteration 743: tag train_accuracy, simple_value 0.17495\n",
      "Iteration 743: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510821907, Iteration 1524\n",
      "batches :1524 0.03388386966550462\n",
      "Iteration 744: tag train_accuracy, simple_value 0.17496\n",
      "Iteration 744: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510822213, Iteration 1525\n",
      "batches :1525 0.033881813805275315\n",
      "Iteration 745: tag train_accuracy, simple_value 0.17513\n",
      "Iteration 745: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510822518, Iteration 1526\n",
      "batches :1526 0.03388162936775598\n",
      "Iteration 746: tag train_accuracy, simple_value 0.17514\n",
      "Iteration 746: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510822823, Iteration 1527\n",
      "batches :1527 0.033882031217217445\n",
      "Iteration 747: tag train_accuracy, simple_value 0.17512\n",
      "Iteration 747: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510823128, Iteration 1528\n",
      "batches :1528 0.03388033120240518\n",
      "Iteration 748: tag train_accuracy, simple_value 0.17518\n",
      "Iteration 748: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510823434, Iteration 1529\n",
      "batches :1529 0.0338787151716938\n",
      "Iteration 749: tag train_accuracy, simple_value 0.17519\n",
      "Iteration 749: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510823739, Iteration 1530\n",
      "batches :1530 0.03388212075084448\n",
      "Iteration 750: tag train_accuracy, simple_value 0.17523\n",
      "Iteration 750: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510824045, Iteration 1531\n",
      "batches :1531 0.03388134281234084\n",
      "Iteration 751: tag train_accuracy, simple_value 0.17522\n",
      "Iteration 751: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510824350, Iteration 1532\n",
      "batches :1532 0.03388290598919179\n",
      "Iteration 752: tag train_accuracy, simple_value 0.17522\n",
      "Iteration 752: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510824669, Iteration 1533\n",
      "batches :1533 0.0338818309371017\n",
      "Iteration 753: tag train_accuracy, simple_value 0.17517\n",
      "Iteration 753: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510824973, Iteration 1534\n",
      "batches :1534 0.03388074471983337\n",
      "Iteration 754: tag train_accuracy, simple_value 0.17519\n",
      "Iteration 754: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510825279, Iteration 1535\n",
      "batches :1535 0.03387961806791113\n",
      "Iteration 755: tag train_accuracy, simple_value 0.17512\n",
      "Iteration 755: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510825584, Iteration 1536\n",
      "batches :1536 0.03387845724712683\n",
      "Iteration 756: tag train_accuracy, simple_value 0.17508\n",
      "Iteration 756: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510825889, Iteration 1537\n",
      "batches :1537 0.033877267628967214\n",
      "Iteration 757: tag train_accuracy, simple_value 0.17505\n",
      "Iteration 757: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510826195, Iteration 1538\n",
      "batches :1538 0.03387513874457427\n",
      "Iteration 758: tag train_accuracy, simple_value 0.17505\n",
      "Iteration 758: tag train_loss, simple_value 0.03388\n",
      "Timestamp 1612510826500, Iteration 1539\n",
      "batches :1539 0.03387490728911754\n",
      "Iteration 759: tag train_accuracy, simple_value 0.17507\n",
      "Iteration 759: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510826805, Iteration 1540\n",
      "batches :1540 0.033873322383059484\n",
      "Iteration 760: tag train_accuracy, simple_value 0.175\n",
      "Iteration 760: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510827110, Iteration 1541\n",
      "batches :1541 0.03387002093024777\n",
      "Iteration 761: tag train_accuracy, simple_value 0.17506\n",
      "Iteration 761: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510827415, Iteration 1542\n",
      "batches :1542 0.03386681707541695\n",
      "Iteration 762: tag train_accuracy, simple_value 0.17514\n",
      "Iteration 762: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510827721, Iteration 1543\n",
      "batches :1543 0.033867298592453236\n",
      "Iteration 763: tag train_accuracy, simple_value 0.17511\n",
      "Iteration 763: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510828026, Iteration 1544\n",
      "batches :1544 0.03386773854367318\n",
      "Iteration 764: tag train_accuracy, simple_value 0.17507\n",
      "Iteration 764: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510828331, Iteration 1545\n",
      "batches :1545 0.03386651684402251\n",
      "Iteration 765: tag train_accuracy, simple_value 0.175\n",
      "Iteration 765: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510828636, Iteration 1546\n",
      "batches :1546 0.033864938483789915\n",
      "Iteration 766: tag train_accuracy, simple_value 0.17508\n",
      "Iteration 766: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510828942, Iteration 1547\n",
      "batches :1547 0.03386195537038002\n",
      "Iteration 767: tag train_accuracy, simple_value 0.17522\n",
      "Iteration 767: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510829247, Iteration 1548\n",
      "batches :1548 0.03386295309125368\n",
      "Iteration 768: tag train_accuracy, simple_value 0.17521\n",
      "Iteration 768: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510829566, Iteration 1549\n",
      "batches :1549 0.03386479711356259\n",
      "Iteration 769: tag train_accuracy, simple_value 0.17519\n",
      "Iteration 769: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510829871, Iteration 1550\n",
      "batches :1550 0.03386619536375458\n",
      "Iteration 770: tag train_accuracy, simple_value 0.17514\n",
      "Iteration 770: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510830177, Iteration 1551\n",
      "batches :1551 0.03386883103517011\n",
      "Iteration 771: tag train_accuracy, simple_value 0.17506\n",
      "Iteration 771: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510830482, Iteration 1552\n",
      "batches :1552 0.033866238864766036\n",
      "Iteration 772: tag train_accuracy, simple_value 0.17511\n",
      "Iteration 772: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510830787, Iteration 1553\n",
      "batches :1553 0.033864497610718435\n",
      "Iteration 773: tag train_accuracy, simple_value 0.17515\n",
      "Iteration 773: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510831092, Iteration 1554\n",
      "batches :1554 0.033866489486581136\n",
      "Iteration 774: tag train_accuracy, simple_value 0.17513\n",
      "Iteration 774: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510831398, Iteration 1555\n",
      "batches :1555 0.03386531612805782\n",
      "Iteration 775: tag train_accuracy, simple_value 0.17508\n",
      "Iteration 775: tag train_loss, simple_value 0.03387\n",
      "Timestamp 1612510831704, Iteration 1556\n",
      "batches :1556 0.033864416102830745\n",
      "Iteration 776: tag train_accuracy, simple_value 0.17508\n",
      "Iteration 776: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510832008, Iteration 1557\n",
      "batches :1557 0.03386286013020128\n",
      "Iteration 777: tag train_accuracy, simple_value 0.17509\n",
      "Iteration 777: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510832312, Iteration 1558\n",
      "batches :1558 0.03385976316756868\n",
      "Iteration 778: tag train_accuracy, simple_value 0.17517\n",
      "Iteration 778: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510832615, Iteration 1559\n",
      "batches :1559 0.03385917701554007\n",
      "Iteration 779: tag train_accuracy, simple_value 0.17506\n",
      "Iteration 779: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510832918, Iteration 1560\n",
      "batches :1560 0.03385838756624323\n",
      "Iteration 780: tag train_accuracy, simple_value 0.17512\n",
      "Iteration 780: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510833221, Iteration 1561\n",
      "batches :1561 0.03385771343319959\n",
      "Iteration 781: tag train_accuracy, simple_value 0.17504\n",
      "Iteration 781: tag train_loss, simple_value 0.03386\n",
      "Timestamp 1612510833568, Iteration 1562\n",
      "batches :1562 0.031692586839199066\n",
      "Iteration 0: tag train_accuracy, simple_value 0.21875\n",
      "Iteration 0: tag train_loss, simple_value 0.03169\n",
      "--------END TRAINING: completed 1562 iterations --------\n",
      "--------END TRAINING: completed epoch 2 in time 508.3792588710785 --------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Spectrum Conductor logs for training run - shows various information including environment variables\n",
    "\n",
    "r = requests.get(sc_rest_url+'/instances/'+sig_id+'/applications/'+driver_id+'/logs/stdout/download',\n",
    "                 auth=myauth, headers={'Accept': 'application/octet-stream'}, verify=False)\n",
    "\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output from the training can be found in the `$DLI_WORK_DIR` referenced in this log, in the directory *under* `batchworkdir`. The structure of the files contained in this directory (which you will access via the API) are the following.\n",
    "\n",
    "```\n",
    "$ tree -h\n",
    ".\n",
    " [   6]  checkpoint\n",
    " [ 247]  log\n",
    "  [   6]  0-97eb84d4-6e4b-4bb7-95e0-fc7bfda461dc.<wmla_server>\n",
    "  [   6]  1-a111dd6d-c406-48f2-89ce-1ef526d5b34b.<wmla_server>\n",
    "  [  50]  driver-20200227104231-0007-3655c5b5-5d81-43ac-a8c6-c243635f60df.<wmla_server>\n",
    "      [ 19K]  evaluation-metrics.txt\n",
    "      [7.1K]  stdout\n",
    " [  19]  model\n",
    "  [ 214]  train\n",
    "      [4.7K]  model_epoch_10_optimizer_state.pth\n",
    "      [ 43M]  model_epoch_10.pth\n",
    "      [4.7K]  model_epoch_5_optimizer_state.pth\n",
    "      [ 43M]  model_epoch_5.pth\n",
    "      [4.7K]  model_epoch_final_optimizer_state.pth\n",
    "      [ 43M]  model_epoch_final.pth\n",
    " [  25]  _submitted_code\n",
    "  [ 133]  pytorch_edt\n",
    "      [1.6K]  edtcallback.py\n",
    "      [2.0K]  elog.py\n",
    "      [4.1K]  emetrics.py\n",
    "      [  67]  __pycache__\n",
    "       [2.0K]  edtcallback.cpython-36.pyc\n",
    "       [2.4K]  elog.cpython-36.pyc\n",
    "      [3.1K]  pytorch_mnist_EDT.py\n",
    "      [4.4K]  pytorch_mnist.py\n",
    " [2.5K]  val_dict_list.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Training Driver Stderr Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shows various information including environment variables\n",
    "import pprint\n",
    "r = requests.get(sc_rest_url+'/instances/'+sig_id+'/applications/'+driver_id+'/logs/stderr/download',\n",
    "                 auth=myauth, headers={'Accept': 'application/octet-stream'}, verify=False)\n",
    "\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information and useful links\n",
    "[Back to top](#Contents)\n",
    "\n",
    "**WML Accelerator Introduction videos:**\n",
    "- WML Accelerator overview video (1 minute): http://ibm.biz/wmla-video\n",
    "- Overview of adapting your code for Elastic Distributed Training via API: [video](https://youtu.be/RnZtYNX6meM) | [PDF](docs/wmla_api_pieces.pdf) (screenshot below)\n",
    "\n",
    "**Additional WML Accelerator information & documentation**\n",
    "- [Learning path: Get started with Watson Machine Learning Accelerator](http://ibm.biz/wmla-learning-path)\n",
    "- [IBM Documentation on Watson Machine Learning Accelerator](https://www.ibm.com/docs/en/wmla/1.2.3)\n",
    "- [Blog: Expert Q&A: Accelerate deep learning on IBM Cloud Pak for Data](https://www.ibm.com/blogs/journey-to-ai/2020/10/expert-qa-accelerate-deep-learning-on-ibm-cloud-pak-for-data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "[Back to top](#Contents)\n",
    "\n",
    "\n",
    "#### This is version 1.0 and its content is copyright of IBM.   All rights reserved.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
