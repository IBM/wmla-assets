{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61cc2996-0474-4263-8d8c-17a6121f20d3"
   },
   "source": [
    "# Automated Hyperparameter Optimization Training using WMLA API\n",
    "\n",
    "#### Notebook created by Kelvin Lui, Nov 2020\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "In this notebook, you will learn how to submit a model and dataset to the Watson Machine Learning Accelerator (WMLA) API to run Hyper Parameter Optimization (HPO). In this particular example, we will be using the Pytorch MNIST HPO model as our training model, inject hyperparameters for the sub-training during search and submit a tuning metric for better results, and then query for the best job results. This notebook runs on Python 3.6 or 3.7.\n",
    "\n",
    "\n",
    "![options](https://github.com/IBM/wmla-learning-path/raw/master/shared-images/WMLA-RestAPI-Demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f166873b-0d91-49cd-be73-c54ab30334b0"
   },
   "source": [
    "\n",
    "\n",
    "![SpectrumComputeFamily_Conductor-HorizontalColorWhite.png](https://raw.githubusercontent.com/IBM/wmla-learning-path/master/shared-images/hpo.png)\n",
    "\n",
    "\n",
    "For this notebook you will use a model and dataset that have already been set up to leverage the API.  For details on the API see [API Documentation](https://www.ibm.com/support/knowledgecenter/en/SSFHA8_1.2.1/cm/deeplearning.html) in the Knowledge Center (KC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8871b562-dab2-4b34-b687-c2bc5690d5a8"
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Setup](#setup)<br>\n",
    "\n",
    "2. [Configuring environment and project details](#configure)<br>\n",
    "\n",
    "3. [Health Check](#health)<br>\n",
    "\n",
    "4. [Training with the HPO API](#train)<br>\n",
    "\n",
    "5. [Deploy the HPO task](#deploy)<br>\n",
    "\n",
    "6. [Find best job results](#best)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9a9d8ae-50dc-452b-890e-7df2d573394f"
   },
   "source": [
    "<a id = \"setup\"></a>\n",
    "## Step 1: Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d378fa8-8a9f-490d-a4eb-439e04e4cf27"
   },
   "source": [
    "First, we must import the required modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9198850-8c95-4511-8bb6-95883d18bcc3"
   },
   "source": [
    "To use the WMLA API, we will be using the Python requests library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T14:11:35.863409Z",
     "start_time": "2020-05-29T14:11:35.439838Z"
    },
    "id": "7f448e46-94fb-42b3-abb1-c8864b8798e9"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import tarfile\n",
    "import tempfile\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pprint\n",
    "import base64\n",
    "\n",
    "# utility print function\n",
    "def nprint(mystring) :\n",
    "    print(\"**{}** : {}\".format(sys._getframe(1).f_code.co_name,mystring))\n",
    "\n",
    "# utility makedir\n",
    "def makeDirIfNotExist(directory) :\n",
    "    if not os.path.exists(directory):  \n",
    "        nprint(\"Making directory {}\".format(directory))\n",
    "        os.makedirs(directory) \n",
    "    else :\n",
    "        nprint(\"Directory {} already exists .. \".format(directory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "394c93d6-78a4-4c83-891b-0a8815a2e497"
   },
   "source": [
    "<a id = \"configure\"></a>\n",
    "## Step 2: Configuring environment and project details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32fc507f-b95c-4990-ba27-6d229302815b"
   },
   "source": [
    "Provide your credentials in this cell, including your cluster url, username and password, and instance group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T14:12:10.675602Z",
     "start_time": "2020-05-29T14:12:10.667892Z"
    },
    "id": "6cd0c833-ff30-49b2-9f1b-4adc24fe49b5"
   },
   "outputs": [],
   "source": [
    "def getconfig(cfg_in={}):\n",
    "    cfg = {}\n",
    "    #cfg[\"master_host\"] = 'wmla-console-twmla.apps.wml1x180.ma.platformlab.ibm.com' # <=enter your host url here\n",
    "    cfg[\"master_host\"] = 'wmla-console-liqbj.apps.wml1x210.ma.platformlab.ibm.com'\n",
    "    # ==== CLASS ENTER User login details below =====\n",
    "    cfg[\"wmla_user\"] = 'admin'  # <=enter your id here\n",
    "    cfg[\"wmla_pwd\"] = 'password'  # <=enter your pwd here\n",
    "    #cfg[\"code_dir\"] = \"/home/wsuser/works/pytorch_hpo\"\n",
    "    cfg[\"data_path\"] = 'pytorch_mnist' # <= enter the path of dataset\n",
    "\n",
    "    # overwrite configs if passed\n",
    "    for (k,v) in cfg_in.items() :\n",
    "        nprint(\"Overriding Config {}:{} with {}\".format(k,cfg[k],v))\n",
    "        cfg[k] = v\n",
    "    return cfg\n",
    "\n",
    "# cfg is used as a global variable throughout this notebook\n",
    "cfg=getconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "178dd87a-b87e-47d3-ac52-444eb63bbd76"
   },
   "source": [
    "Here we will get and print out the API endpoints and setup requests session.   The following sections use the Watson ML Accelerator API to complete the various tasks required. We've given examples of a number of tasks but you should refer to the documentation at to see more details of what is possible and sample output you might expect.\n",
    "\n",
    "    - https://www.ibm.com/support/knowledgecenter/SSFHA8_2.2.0/cm/deeplearning.html\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T14:12:14.952471Z",
     "start_time": "2020-05-29T14:12:14.941641Z"
    },
    "id": "3393237c-7268-46af-867b-de6b07de3f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YWRtaW46cGFzc3dvcmQ=\n",
      "DL API Endpoints : https://wmla-console-liqbj.apps.wml1x210.ma.platformlab.ibm.com/platform/rest/deeplearning/v1\n"
     ]
    }
   ],
   "source": [
    "# REST call variables\n",
    "\n",
    "\n",
    "#commonHeaders={'accept': 'application/json'}\n",
    "s=cfg[\"wmla_user\"] + \":\" + cfg[\"wmla_pwd\"]\n",
    "es = base64.b64encode(s.encode('utf-8')).decode(\"utf-8\")\n",
    "print(es)\n",
    "commonHeaders={'Authorization': 'Basic '+es}\n",
    "\n",
    "\n",
    "auth_url = 'https://{}/auth/v1/logon'.format(cfg[\"master_host\"])\n",
    "#print(auth_url)\n",
    "auth_body = {'username': cfg[\"wmla_user\"], 'password': cfg[\"wmla_pwd\"]}\n",
    "                            \n",
    "a=requests.get(auth_url,headers=commonHeaders, verify=False, json=auth_body)\n",
    "access_token=a.json()['accessToken']\n",
    "\n",
    "#get api endpoint\n",
    "def get_ep(mode=\"dl\") :\n",
    "    if mode==\"dl\" :\n",
    "        dl_rest_url =  'https://' + cfg[\"master_host\"] +'/platform/rest/deeplearning/v1'\n",
    "        return dl_rest_url\n",
    "    else :\n",
    "        nprint(\"Error mode : {} not supported\".format(mode))\n",
    "\n",
    "\n",
    "print (\"DL API Endpoints : {}\".format(get_ep(\"dl\")))\n",
    "\n",
    "# Setup Requests session\n",
    "commonHeaders={'accept': 'application/json', 'X-Auth-Token': access_token}\n",
    "req = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b99ccfa4-8484-48bf-9c30-4226b9470c50"
   },
   "source": [
    "<a id = \"health\"></a>\n",
    "## Step 3: Health Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "883f9208-d810-46b6-9855-7d94d4946399"
   },
   "source": [
    "In this step, we will check if there are any existing HPO tasks and also verify the platform health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccff4fb3-8f56-4079-a215-088e7b10b221"
   },
   "source": [
    "Rest API: `GET platform/rest/deeplearning/v1/hypersearch`\n",
    "- `Description`: Get all the HPO tasks that the login user can access.\n",
    "- `OUTPUT`: A list of HPO tasks and each one with the same format which can be found in the api doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T14:12:20.514534Z",
     "start_time": "2020-05-29T14:12:20.355413Z"
    },
    "id": "14ab97e9-68dd-48c5-903f-b5c4d27b18df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**hpo_health_check** : getTuneStatusUrl: https://wmla-console-liqbj.apps.wml1x210.ma.platformlab.ibm.com/platform/rest/deeplearning/v1/hypersearch\n",
      "**hpo_health_check** : Hpo task: admin-hpo-5835408259613542, State: FINISHED\n",
      "**hpo_health_check** : Hpo task: admin-hpo-5843973081207573, State: FINISHED\n",
      "**hpo_health_check** : Hpo task: admin-hpo-5844576052554724, State: FINISHED\n",
      "**hpo_health_check** : Hpo task: admin-hpo-6397378866788100, State: FINISHED\n",
      "**hpo_health_check** : Hpo task: admin-hpo-6421166456056555, State: FINISHED\n",
      "**hpo_health_check** : Hpo task: admin-hpo-6462458951734459, State: FINISHED\n",
      "**hpo_health_check** : Hpo task: admin-hpo-6466373825972604, State: FINISHED\n",
      "**hpo_health_check** : Hpo task: admin-hpo-6466845542476112, State: FINISHED\n"
     ]
    }
   ],
   "source": [
    "def hpo_health_check():\n",
    "    getTuneStatusUrl = get_ep(\"dl\") + '/hypersearch'\n",
    "    nprint ('getTuneStatusUrl: %s' %getTuneStatusUrl)\n",
    "    r = req.get(getTuneStatusUrl, headers=commonHeaders, verify=False, json=auth_body)\n",
    "    \n",
    "    if not r.ok:\n",
    "        nprint('check hpo task status failed: code=%s, %s'%(r.status_code, r.content))\n",
    "    else:\n",
    "        if len(r.json()) == 0:\n",
    "            nprint('There is no hpo task been created')\n",
    "        for item in r.json():\n",
    "            nprint('Hpo task: %s, State: %s'%(item['hpoName'], item['state']))\n",
    "            #print('Best:%s'%json.dumps(item.get('best'), sort_keys=True, indent=4))\n",
    "\n",
    "hpo_health_check()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b9948d1-7146-4b5f-abb8-0f951d17c897"
   },
   "source": [
    "<a id = \"train\"></a>\n",
    "## Step 4: Training with the HPO API\n",
    "\n",
    "\n",
    "The WMLA framework requires 2 changes to your code to support the HPO API, and these are:\n",
    "\n",
    "* Inject hyperparameters for the sub-training during search\n",
    "* Retrieve sub-training result metric\n",
    "\n",
    "Note that the code sections below show a comparison between the \"before\" and \"HPO enabled\" versions of the code by using `diff`.\n",
    "\n",
    "\n",
    "1. Import the dependent libararies:\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "![image1](https://github.com/IBM/wmla-learning-path/raw/dev/shared-images/hpo_update_model_0.png)\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "2. Get the WMLA cluster `DLI_DATA_FS`, `RESULT_DIR` and `LOG_DIR` for the HPO training job. The `DLI_DATA_FS` can be used for shared data placement, the `RESULT_DIR` can be used for final model saving, and the `LOG_DIR` can be used for user logs and monitoring.\n",
    "\n",
    "&nbsp;\n",
    "**Note**: `DLI_DATA_FS` is set when installing the DLI cluster; `RESULT_DIR` and `LOG_DIR` is generated by WMLA for each HPO experiment.\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "![image1](https://github.com/IBM/wmla-learning-path/raw/dev/shared-images/hpo_update_model_1.png)\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "3. Replace the hyperparameter definition code by reading hyperparameters from the `config.json` file. the `config.json` is generated by WMLA HPO, which contains a set of hyperparameter candidates for each tuning jobs. The hyperparameters and the search space is defined when submitting the HPO task. For example, here the hyperparameter `learning_rate` is set to tune:\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "![image2](https://github.com/IBM/wmla-learning-path/raw/dev/shared-images/hpo_update_model_2.png)\n",
    "\n",
    "&nbsp;\n",
    "Then you could use the hyperparameter you get from `config.json` where you want:\n",
    "&nbsp;\n",
    "![image2](https://github.com/IBM/wmla-learning-path/raw/dev/shared-images/hpo_update_model_2_2.png)\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "4.  Write the tuning result into `val_dict_list.json` under `RESULT_DIR`. WMLA HPO will read this file for each tuning job to get the metric values. Define a `test_metrics` list to store all metric values and pass the epoch parameter to the test function. Then you can add the metric values to the `test_metrics` list during the training test process. Please note that the metric names should be specified when submitting the HPO task, and be consistent with the code here.\n",
    "&nbsp;\n",
    "For example, at the HPO task submit request, `loss` will be used as the objective metric the tuning will try to minimize the `loss`:\n",
    "\n",
    "```\n",
    "'algoDef': # Define the parameters for search algorithms  \n",
    "{\n",
    "    # Name of the search algorithm, one of Random, Bayesian, Tpe, Hyperband  \n",
    "    'algorithm': 'Random',   \n",
    "    # Name of the target metric that we are trying to optimize when searching hyper-parameters.\n",
    "    # It is the same metric name that the model update part 2 trying to dump.\n",
    "    'objectiveMetric' : 'loss',\n",
    "    # Strategy as how to optimize the hyper-parameters, minimize means to find better hyper-parameters to\n",
    "    # make the above objectiveMetric as small as possible, maximize means the opposite.\n",
    "    'objective' : 'minimize',\n",
    "    ...\n",
    "}\n",
    "```\n",
    "&nbsp;\n",
    "The code change:\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "![image2](https://github.com/IBM/wmla-learning-path/raw/dev/shared-images/hpo_update_model_3.png)\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "5. After the training completes, write the metric list into the `val_dict_list.json` file. \n",
    "&nbsp;\n",
    "&nbsp;\n",
    "![image2](https://github.com/IBM/wmla-learning-path/raw/dev/shared-images/hpo_update_model_5.png)\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T01:56:10.791959Z",
     "start_time": "2020-07-21T01:56:10.789523Z"
    },
    "id": "26558cf1-3266-4a22-aaf5-5932ff235a89"
   },
   "source": [
    "\n",
    "## Create script to pass to WMLA via API\n",
    "\n",
    "This section creates a script external to this notebook which runs the batch jobs on the WMLA servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9afc5cc7-e59a-4bef-a12e-cfc0118e63da"
   },
   "outputs": [],
   "source": [
    "# Create a working directory to save our script in if it doesn't exist already\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "working_directory = f'script_for_upload' \n",
    "model_main = f'WMLA_HPO_pytorch.py'\n",
    "\n",
    "Path(working_directory).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aedb708572b4f4087601703d05aace5"
   },
   "source": [
    "The next cell uses the writefile magic to create a script externally to this notebook in the location defined in working_directory above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d2681739d08c494086670e833da66b3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script_for_upload/WMLA_HPO_pytorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {working_directory}/{model_main}\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# HPO - import dependent lib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# get dataset from DLI_DATA_FS\n",
    "dataDir = os.environ[\"DLI_DATA_FS\"]\n",
    "\n",
    "\n",
    "if dataDir is not None:\n",
    "    print(\"dataDir is: %s\"%dataDir)\n",
    "else:\n",
    "    print(\"Warning: not found DATA_DIR from os env!\")\n",
    "\n",
    "model_path = os.environ[\"RESULT_DIR\"]+\"/model/saved_model\"\n",
    "print (\"model_path: %s\" %model_path)\n",
    "\n",
    "# HPO - get hpo experiment hyper-parameter values from config.json\n",
    "# The hyperparameters and the search space is defined when submitting the HPO task\n",
    "# WMLA HPO will generate hpo experiment candidates and writes to config.json\n",
    "try:\n",
    "    hyper_params = json.loads(open(\"config.json\").read())\n",
    "    print('hyper_params: ', hyper_params)\n",
    "    learning_rate = float(hyper_params.get(\"learning_rate\", \"0.01\"))\n",
    "except:\n",
    "    print('failed to get hyper-parameters from config.json')\n",
    "    learning_rate = 0.001\n",
    "    pass\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "test_metrics = []\n",
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    test_metrics.append((epoch, {\"loss\": float(test_loss)}))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(dataDir, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(dataDir, train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader, epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "    # HPO - dump metric values to val_dict_list.json start\n",
    "    training_out =[]\n",
    "    for test_metric in test_metrics:\n",
    "        out = {'steps':test_metric[0]}\n",
    "        for (metric,value) in test_metric[1].items():\n",
    "            out[metric] = value\n",
    "        training_out.append(out)\n",
    "    with open('{}/val_dict_list.json'.format(os.environ['RESULT_DIR']), 'w') as f:\n",
    "        json.dump(training_out, f)\n",
    "# HPO - dump metric values to val_dict_list.json end\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66cb50ca-4241-47f3-b653-837adac47399"
   },
   "source": [
    "<a id = \"deploy\"></a>\n",
    "## Step 5: Deploy the HPO task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "751e4151-21f1-4b9e-b8fa-40e6441ac6ab"
   },
   "source": [
    "Here we package up our model to send to the API for HPO.  \n",
    "\n",
    "\n",
    "\n",
    "REST API: `POST /platform/rest/deeplearning/v1/hypersearch`\n",
    "\n",
    "- Description: Start a new HPO task\n",
    "- Content-type: Multi-Form\n",
    "- Multi-Form Data:\n",
    "  - files: Model file\n",
    "  - form-filed: {‘data’: ‘String format of input parameters to start hpo task, let’s call it as **hpo_input** and show its specification later’}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ca84fd0-6ace-4851-a053-778fe2f21295"
   },
   "source": [
    "#### Package model files for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99aedabb-eeed-431e-8858-bc009623f5e9"
   },
   "source": [
    "Package the updated model files into a tar file ending with `.modelDir.tar`\n",
    "\n",
    "REST API expects a `modelDir.tar` with the model code inside ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2d46b796-f58a-4f14-bc20-8b1cc6644e24"
   },
   "outputs": [],
   "source": [
    "f = open(working_directory + \"/\" +model_main, 'rb')\n",
    "files = {model_main : f}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c191c685-e3e3-4f24-80e8-191c6bcf642b"
   },
   "source": [
    "#### Construct POST request data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c71a04f-7b3e-4229-ac50-113d5403b71b"
   },
   "source": [
    "**hpo_input** will be in Python `dict` or `json` format as shown below, and will convert to string when calling REST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T14:13:06.385371Z",
     "start_time": "2020-05-29T14:13:06.376200Z"
    },
    "id": "26059c7f-68c3-4467-a991-125b1ecfed59"
   },
   "outputs": [],
   "source": [
    "data =  {\n",
    "        'modelSpec': # Define the model training related parameters\n",
    "        {\n",
    "         \n",
    "            # These are the arguments we'll pass to the execution engine; they follow the same conventions\n",
    "            # of the dlicmd.py command line launcher\n",
    "            #\n",
    "            # See:\n",
    "            #   https://www.ibm.com/support/knowledgecenter/en/SSFHA8_1.2.1/cm/dlicmd.html\n",
    "            # In this example, args after --model-dir are all the required parameter for the original model itself.\n",
    "            #\n",
    "\n",
    "               'args': '--exec-start PyTorch  --workerDeviceNum 1 --cs-datastore-meta type=fs,data_path={} \\\n",
    "                     --appName hpo-pytorch-mnist-gpu  --model-main {} --batch-size 64 --epochs 2 '.format(cfg[\"data_path\"],model_main)  \n",
    "        },\n",
    "    \n",
    "        'algoDef': # Define the parameters for search algorithms\n",
    "        {\n",
    "            # Name of the search algorithm, one of Random, Bayesian, Tpe, Hyperband, ExperimentGridSearch\n",
    "            'algorithm': 'Random', \n",
    "            # Max running time of the hpo task in minutes, -1 means unlimited\n",
    "            'maxRunTime': 60,  \n",
    "            # Max number of training job to submitted for hpo task, -1 means unlimited’,\n",
    "            'maxJobNum': 2,            \n",
    "            # Max number of training job to run in parallel, default 1. It depends on both the\n",
    "            # avaiable resource and if the search algorithm support to run in parallel, current only Random\n",
    "            # fully supports to run in parallel, Hyperband and Tpe supports to to in parellel in some phase,\n",
    "            # Bayesian runs in sequence now.\n",
    "            'maxParalleJobNum': 2, \n",
    "            # Name of the target metric that we are trying to optimize when searching hyper-parameters.\n",
    "            # It is the same metric name that the model update part 2 trying to dump.\n",
    "            'objectiveMetric' : 'loss',\n",
    "            # Strategy as how to optimize the hyper-parameters, minimize means to find better hyper-parameters to\n",
    "            # make the above objectiveMetric as small as possible, maximize means the opposite.\n",
    "            'objective' : 'minimize',\n",
    "        },\n",
    "    \n",
    "        # Define the hyper-paremeters to search and the corresponding search space.\n",
    "        'hyperParams':\n",
    "        [\n",
    "             {\n",
    "                 # Hyperparameter name, which will be the hyper-parameter key in config.json\n",
    "                 'name': 'learning_rate',\n",
    "                 # One of Range, Discrete\n",
    "                 'type': 'Range',\n",
    "                 # one of int, double, str\n",
    "                 'dataType': 'DOUBLE',\n",
    "                 # lower bound and upper bound when type=range and dataType=double\n",
    "                 'minDbVal': 0.001,\n",
    "                 'maxDbVal': 0.1,\n",
    "                 # lower bound and upper bound when type=range and dataType=int\n",
    "                 'minIntVal': 0,\n",
    "                 'maxIntVal': 0,\n",
    "                 # Discrete value list when type=discrete\n",
    "                 'discreteDbVal': [],\n",
    "                 'discreteIntVal': [],\n",
    "                 'discreateStrVal': []\n",
    "                 #step size to split the Range space. ONLY valid when type is Range\n",
    "                 #'step': '0.002',\n",
    "             }\n",
    "         ]\n",
    "    }\n",
    "mydata={'data':json.dumps(data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9a1bad4-7507-4018-81dd-0de86c34e3ec"
   },
   "source": [
    "#### Submit the Post request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea7a0303-ec21-47d9-a2e2-d006a58402a7"
   },
   "source": [
    "Submit the HPO task through the Post call and an HPO name/id in string format will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T14:13:10.295370Z",
     "start_time": "2020-05-29T14:13:10.171662Z"
    },
    "id": "3e5cd107-a4f3-4946-b30d-54d248f335db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**submit_job** : startTuneUrl : https://wmla-console-liqbj.apps.wml1x210.ma.platformlab.ibm.com/platform/rest/deeplearning/v1/hypersearch\n",
      "**submit_job** : files : {'WMLA_HPO_pytorch.py': <_io.BufferedReader name='script_for_upload/WMLA_HPO_pytorch.py'>}\n",
      "\n",
      "Model submitted successfully: {'uid': 'admin-hpo-6487177646872135', 'href': '/platform/rest/deeplearning/v1/hypersearch/admin-hpo-6487177646872135'}\n",
      "hpo_job_id : {'uid': 'admin-hpo-6487177646872135', 'href': '/platform/rest/deeplearning/v1/hypersearch/admin-hpo-6487177646872135'}\n"
     ]
    }
   ],
   "source": [
    "def submit_job():\n",
    "    startTuneUrl=get_ep('dl') + '/hypersearch'\n",
    "    nprint(\"startTuneUrl : {}\".format(startTuneUrl))\n",
    "    nprint(\"files : {}\".format(files))\n",
    "    #nprint(\"myauth() : {}\".format(auth_body))\n",
    "    #print(\"hpo_job_id : {}\".format(hpo_job_id))\n",
    "    r = req.post(startTuneUrl, headers=commonHeaders, data=mydata, files=files, verify=False, json=auth_body)\n",
    "    hpo_name=None\n",
    "    if r.ok:\n",
    "        hpo_name = r.json()\n",
    "        print ('\\nModel submitted successfully: {}'.format(hpo_name))\n",
    "        \n",
    "    else:\n",
    "        print('\\nModel submission failed with code={}, {}'. format(r.status_code, r.content))\n",
    "    return hpo_name\n",
    "\n",
    "hpo_job_id = submit_job()\n",
    "print(\"hpo_job_id : {}\".format(hpo_job_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e39b1f6-7ae5-4892-833a-026fad571926"
   },
   "source": [
    "Print out task details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T14:24:00.222987Z",
     "start_time": "2020-05-29T14:13:14.717531Z"
    },
    "id": "d26072e1-43c6-46a6-a3cd-16313c52e56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing every 10 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hyperParams</th>\n",
       "      <th>state</th>\n",
       "      <th>metricVal</th>\n",
       "      <th>appId</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'name': 'learning_rate', 'dataType': 'double', 'userDefined': False, 'fixedVal': '0.07682903454845375'}]</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>0.065198</td>\n",
       "      <td>liqbj-605</td>\n",
       "      <td>2020-11-17 19:59:48</td>\n",
       "      <td>2020-11-17 20:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'name': 'learning_rate', 'dataType': 'double', 'userDefined': False, 'fixedVal': '0.04433426721204368'}]</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>0.089596</td>\n",
       "      <td>liqbj-606</td>\n",
       "      <td>2020-11-17 19:59:53</td>\n",
       "      <td>2020-11-17 20:00:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "\n",
       "                                                                                                  hyperParams  \\\n",
       "0  [{'name': 'learning_rate', 'dataType': 'double', 'userDefined': False, 'fixedVal': '0.07682903454845375'}]   \n",
       "1  [{'name': 'learning_rate', 'dataType': 'double', 'userDefined': False, 'fixedVal': '0.04433426721204368'}]   \n",
       "\n",
       "      state  metricVal      appId            startTime              endTime  \n",
       "0  FINISHED   0.065198  liqbj-605  2020-11-17 19:59:48  2020-11-17 20:00:58  \n",
       "1  FINISHED   0.089596  liqbj-606  2020-11-17 19:59:53  2020-11-17 20:00:58  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'algoDef': { 'algorithm': 'Random',\n",
      "               'maxJobNum': 2,\n",
      "               'maxParalleJobNum': 2,\n",
      "               'maxRunTime': 60,\n",
      "               'objective': 'minimize',\n",
      "               'objectiveMetric': 'loss'},\n",
      "  'best': { 'appId': 'liqbj-605',\n",
      "            'endTime': '2020-11-17 20:00:58',\n",
      "            'hyperParams': [ { 'dataType': 'double',\n",
      "                               'fixedVal': '0.07682903454845375',\n",
      "                               'name': 'learning_rate',\n",
      "                               'userDefined': False}],\n",
      "            'id': 0,\n",
      "            'metricVal': 0.06519806289672851,\n",
      "            'startTime': '2020-11-17 19:59:48',\n",
      "            'state': 'FINISHED'},\n",
      "  'complete': 2,\n",
      "  'createtime': '2020-11-17 19:59:46',\n",
      "  'creator': 'admin',\n",
      "  'duration': '00:01:00',\n",
      "  'experiments': [ { 'appId': 'liqbj-605',\n",
      "                     'endTime': '2020-11-17 20:00:58',\n",
      "                     'hyperParams': [ { 'dataType': 'double',\n",
      "                                        'fixedVal': '0.07682903454845375',\n",
      "                                        'name': 'learning_rate',\n",
      "                                        'userDefined': False}],\n",
      "                     'id': 0,\n",
      "                     'metricVal': 0.06519806289672851,\n",
      "                     'startTime': '2020-11-17 19:59:48',\n",
      "                     'state': 'FINISHED'},\n",
      "                   { 'appId': 'liqbj-606',\n",
      "                     'endTime': '2020-11-17 20:00:58',\n",
      "                     'hyperParams': [ { 'dataType': 'double',\n",
      "                                        'fixedVal': '0.04433426721204368',\n",
      "                                        'name': 'learning_rate',\n",
      "                                        'userDefined': False}],\n",
      "                     'id': 1,\n",
      "                     'metricVal': 0.08959646072387695,\n",
      "                     'startTime': '2020-11-17 19:59:53',\n",
      "                     'state': 'FINISHED'}],\n",
      "  'failed': 0,\n",
      "  'hpoName': 'admin-hpo-6487177646872135',\n",
      "  'hyperParams': [ { 'dataType': 'double',\n",
      "                     'maxDbVal': 0.1,\n",
      "                     'maxIntVal': 0,\n",
      "                     'minDbVal': 0.001,\n",
      "                     'minIntVal': 0,\n",
      "                     'name': 'learning_rate',\n",
      "                     'type': 'range',\n",
      "                     'userDefined': False}],\n",
      "  'progress': '2/2',\n",
      "  'running': 0,\n",
      "  'state': 'FINISHED'}\n"
     ]
    }
   ],
   "source": [
    "def query_job_status(job_id,refresh_rate=3) :\n",
    "\n",
    "    getHpoUrl = get_ep('dl')  +'/hypersearch/'+ hpo_job_id['uid']\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "    keep_running=True\n",
    "    res=None\n",
    "    while(keep_running):\n",
    "        res = req.get(getHpoUrl, headers=commonHeaders, verify=False, json=auth_body)\n",
    "        experiments=res.json()['experiments']\n",
    "        experiments = pd.DataFrame.from_dict(experiments)\n",
    "        pd.set_option('max_colwidth', 120)\n",
    "        clear_output()\n",
    "        print(\"Refreshing every {} seconds\".format(refresh_rate))\n",
    "        display(experiments)\n",
    "        pp.pprint(res.json())\n",
    "        if(res.json()['state'] not in ['SUBMITTED','RUNNING']) :\n",
    "            keep_running=False\n",
    "        time.sleep(refresh_rate)\n",
    "    return res\n",
    "job_status = query_job_status(hpo_job_id,refresh_rate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6250529-6651-4be2-8f07-4d4e65be908d"
   },
   "source": [
    "#### Notebook Complete \n",
    "Congratulations, you have completed our demonstration of using WMLA for distributed hyperparameter optimization search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06b4e6d6-d090-4609-9aa0-843a57e4aa50"
   },
   "source": [
    "Copyright © 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25009264-bf94-4780-a298-0b8c56b3b213"
   },
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
