{"cells": [{"metadata": {"id": "02ea1f6f090f46e5966468336006c630"}, "cell_type": "markdown", "source": "# Accelerate Deep Learning Model training with Watson Machine Learning Accelerator\n"}, {"metadata": {"id": "9bc04e069c7c4c86ac640afdecdefbad"}, "cell_type": "markdown", "source": "### Notebook created by Kelvin Lui,  Xue Yin Zhuang in Jan 2021\n\n### In this notebook, you will learn how to use the Watson Machine Learning Accelerator (WML-A) API and accelerate deep learning model training on GPU with Watson Machine Learning Accelerator.\n\nThis notebook uses the PyTorch Resnet18 model, which performs image classification using a basic computer vision image classification example. The model will be trained both on CPU and GPU to demonstrate that training models on GPU hardware deliver faster result times.\n\n\nThis notebook covers the following sections:\n\n1. [Setting up required packages](#setup)<br>\n\n2. [Configuring your environment and project details](#configure)<br>\n\n3. [Training the model on CPU](#cpu)<br>\n\n4. [Training the model on GPU with Watson Machine Learning Accelerator](#gpu)<br>"}, {"metadata": {"id": "7fcde3deab74495e871e4da0f83fdfac"}, "cell_type": "markdown", "source": "<a id = \"setup\"></a>\n## Step 1: Setting up required packages\n"}, {"metadata": {"id": "b4020a53992743179053ecb345e637a4"}, "cell_type": "markdown", "source": "#### First, install torchvision which is required to train the PyTorch Resnet18 model on CPU.\nNote: You will need to create a custom environment with 16VCPU and 32GB"}, {"metadata": {"id": "bd69fac3990e4ef28a38a29b0b83b614"}, "cell_type": "code", "source": "! pip install torchvision", "execution_count": 1, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: torchvision in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (0.8.2)\nRequirement already satisfied: torch==1.7.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision) (1.7.1)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision) (7.2.0)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision) (1.18.5)\nRequirement already satisfied: typing-extensions in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torch==1.7.1->torchvision) (3.7.4.2)\n"}]}, {"metadata": {"id": "199618d2bbe1488d8820cd8a62402e99"}, "cell_type": "code", "source": "import torchvision", "execution_count": 2, "outputs": []}, {"metadata": {"id": "01c07e0d7cc44bf787719d848a58c1f3"}, "cell_type": "markdown", "source": "#### Next, define helper methods:"}, {"metadata": {"id": "975a773eb5f7406085b30b08bc3d9922"}, "cell_type": "code", "source": "# import tarfile\nimport tempfile\nimport os\nimport json\nimport pprint\nimport pandas as pd\nfrom IPython.display import display, FileLink, clear_output\n\nimport requests\nfrom requests.packages.urllib3.exceptions import InsecureRequestWarning\nrequests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n\nfrom matplotlib import pyplot as plt\n%pylab inline\n\nimport base64\nimport json\nimport time\nimport urllib\nimport tarfile\n\n\ndef query_job_status(job_id,refresh_rate=3) :\n\n    execURL = dl_rest_url  +'/execs/'+ job_id['id']\n    pp = pprint.PrettyPrinter(indent=2)\n\n    keep_running=True\n    res=None\n    while(keep_running):\n        res = req.get(execURL, headers=commonHeaders, verify=False)\n        monitoring = pd.DataFrame(res.json(), index=[0])\n        pd.set_option('max_colwidth', 120)\n        clear_output()\n        print(\"Refreshing every {} seconds\".format(refresh_rate))\n        display(monitoring)\n        pp.pprint(res.json())\n        if(res.json()['state'] not in ['PENDING_CRD_SCHEDULER', 'SUBMITTED','RUNNING']) :\n            keep_running=False\n        time.sleep(refresh_rate)\n    return res\n\ndef query_executor_stdout_log(job_id) :\n\n    execURL = dl_rest_url  +'/scheduler/applications/'+ job_id['id'] + '/executor/1/logs/stdout?lastlines=1000'\n    #'https://{}/platform/rest/deeplearning/v1/scheduler/applications/wmla-267/driver/logs/stderr?lastlines=10'.format(hostname)\n    commonHeaders2={'accept': 'text/plain', 'X-Auth-Token': access_token}\n    print (execURL)\n    res = req.get(execURL, headers=commonHeaders2, verify=False)\n    print(res.text)\n    \n    \ndef query_train_metric(job_id) :\n\n    #execURL = dl_rest_url  +'/execs/'+ job_id['id'] + '/log'\n    execURL = dl_rest_url  +'/execs/'+ job_id['id'] + '/log'\n    #'https://{}/platform/rest/deeplearning/v1/scheduler/applications/wmla-267/driver/logs/stderr?lastlines=10'.format(hostname)\n    commonHeaders2={'accept': 'text/plain', 'X-Auth-Token': access_token}\n    print (execURL)\n    res = req.get(execURL, headers=commonHeaders2, verify=False)\n    print(res.text)\n\n    # save result file    \ndef download_trained_model(job_id) :\n\n    from IPython.display import display, FileLink\n\n    # save result file\n    commonHeaders3={'accept': 'application/octet-stream', 'X-Auth-Token': access_token}\n    execURL = dl_rest_url  +'/execs/'+ r.json()['id'] + '/result'\n    res = req.get(execURL, headers=commonHeaders3, verify=False, stream=True)\n    print (execURL)\n\n    tmpfile = '/project_data/data_asset/' +  r.json()['id'] +'.zip'\n    print ('Save model: ', tmpfile )\n    with open(tmpfile,'wb') as f:\n        f.write(res.content)\n        f.close()\n\ndef make_tarfile(output_filename, source_dir):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        tar.add(source_dir, arcname=os.path.basename(source_dir))", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "Populating the interactive namespace from numpy and matplotlib\n", "name": "stdout"}]}, {"metadata": {"id": "41e72dbf695c4f258de5e6d85685c7c2"}, "cell_type": "markdown", "source": "<a id = \"configure\"></a>\n## Step 2: Configuring your environment and project details"}, {"metadata": {"id": "fe8c33b48a9a4f10aeb34eabc2e1f1d3"}, "cell_type": "markdown", "source": "To set up your project details, provide your credentials in this cell. You must include your cluster URL, username, and password."}, {"metadata": {"id": "4fa3be661bab44b58d8e86d383a4df65"}, "cell_type": "code", "source": "hostname='wmla-console-xwmla.apps.wml1x180.ma.platformlab.ibm.com'  # please enter Watson Machine Learning Accelerator host name\nlogin='admin:password' # please enter the login and password\nes = base64.b64encode(login.encode('utf-8')).decode(\"utf-8\")\nprint(es)\ncommonHeaders={'Authorization': 'Basic '+es}\nreq = requests.Session()\nauth_url = 'https://{}/auth/v1/logon'.format(hostname)\nprint(auth_url)\na=requests.get(auth_url,headers=commonHeaders, verify=False)\naccess_token=a.json()['accessToken']\nprint(access_token)", "execution_count": 2, "outputs": [{"name": "stdout", "text": "YWRtaW46cGFzc3dvcmQ=\nhttps://wmla-console-xwmla.apps.wml1x180.ma.platformlab.ibm.com/auth/v1/logon\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwicm9sZSI6IkFkbWluIiwicGVybWlzc2lvbnMiOlsiYWRtaW5pc3RyYXRvciIsImNhbl9wcm92aXNpb24iLCJtYW5hZ2VfY2F0YWxvZyIsImFjY2Vzc19jYXRhbG9nIl0sImdyb3VwcyI6WzEwMDAwXSwic3ViIjoiYWRtaW4iLCJpc3MiOiJLTk9YU1NPIiwiYXVkIjoiRFNYIiwidWlkIjoiMTAwMDMzMDk5OSIsImF1dGhlbnRpY2F0b3IiOiJkZWZhdWx0IiwiaWF0IjoxNjEyODE4MDgyLCJleHAiOjE2MTI4NjEyNDZ9.XZoM-xN15afxDjLOLt6K2GnKqoXuJexV7rBB3jI9unwO01xmBd4VXX0z-3JuAry_QIX8o2w3kk3tUVVkvCzipOJHVLctooLxN7NN76E0kit9Gf0IsFCqFbigF6cSWUD6aa5fkdMRU_LWyUp40gpXQJ6OA6oteHT93i4Lc03tDRSkk0VU7UjpDSXPCXdEbT0fOB8Wbtoyr6jjdc2XrvJho3t6R8KYAf63VzlNHL_op_5kPWwgNaAKEXtzJNM0IgYMXBcYcIsmWZdROgr9in6wP_stIwpZza-ehhRICSJn5o5Ko72RbS-RELdNo6lZECK24ZRA_maUSL5CURIrHoaM6g\n", "output_type": "stream"}]}, {"metadata": {"id": "24d476b80c6c4f7d8abe927b3656e1de"}, "cell_type": "code", "source": "dl_rest_url = 'https://{}/platform/rest/deeplearning/v1'.format(hostname)\ncommonHeaders={'accept': 'application/json', 'X-Auth-Token': access_token}\nreq = requests.Session()", "execution_count": 3, "outputs": []}, {"metadata": {"id": "de65166a99d0485aa885f938ffcec89f"}, "cell_type": "markdown", "source": "<a id = \"cpu\"></a>\n## Step 3: Training the model on CPU"}, {"metadata": {"id": "8aea74018e6d4c26920e02a00637f4c6"}, "cell_type": "markdown", "source": "#### Prepare the model files for running on CPU:"}, {"metadata": {"id": "3e3692cc27324c4680410dffcec5cd73"}, "cell_type": "code", "source": "import os\n\nmodel_dir = f'/project_data/data_asset/pytorch-resnet/resnet' \nmodel_main = f'main.py'\nmodel_resnet = f'resnet.py'\n\nos.makedirs(model_dir, exist_ok=True)", "execution_count": 4, "outputs": []}, {"metadata": {"id": "a89ecd024ebf4acf9338ea0157415a61"}, "cell_type": "code", "source": "%%writefile {model_dir}/{model_main}\n\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Image Classification Using PyTorch Resnet with Watson Machine Learning Accelerator Notebook\n# This asset details the process of performing a basic computer vision image classification example using the notebook functionality within Watson Machine Learning Accelerator. In this asset, you will learn how to accelerate your training with pytorch resnet model upon the cifar10 dataset.\n#\n# Please refer to [Resnet Introduction](https://arxiv.org/abs/1512.03385) for more details.\n\n\n\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torchvision.models as models\n#from resnet import resnet18\nimport time\nimport numpy\n\nimport sys\nimport os\nimport glob\nimport argparse\n\nlog_interval = 10\n\nseed = 1\nuse_cuda = False\ncompleted_batch =0\ncompleted_test_batch =0\ncriterion = nn.CrossEntropyLoss()\n\n\nparser = argparse.ArgumentParser(description='Tensorflow MNIST Example')\nparser.add_argument('--batch-size', type=int, default=32, metavar='N',\n                    help='input batch size for training (default: 128)')\nparser.add_argument('--epochs', type=int, default=5, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--cuda', action='store_true', default=False,\n                    help='disables CUDA training')\nargs = parser.parse_args()\nprint(args)\n\n\n# ## Create the Resnet18 model\nprint(\"Use cuda: \", use_cuda)\n\n# ## Download the Cifar10 dataset\n# Below code will download the cifar10 dataset automatically to $DATA_DIR/cifar10.\n# You could also download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and upload it manually.\n\nmodel_dir = f'/project_data/data_asset/pytorch-resnet/resnet' \n#DATA_DIR = os.getenv(\"DATA_DIR\")\nprint(\"DATA_DIR: \", DATA_DIR)\n\ndef getDatasets():\n    train_data_dir = DATA_DIR + '/cifar10'\n    test_data_dir = DATA_DIR + '/cifar10'\n\n    transform_train = transforms.Compose([\n        transforms.Resize(224),\n        #transforms.RandomCrop(self.resolution, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    return (torchvision.datasets.CIFAR10(root=train_data_dir, train=True, download=True, transform = transform_train),\n            torchvision.datasets.CIFAR10(root=test_data_dir, train=False, download=True, transform = transform_test)\n            )\n\ntorch.manual_seed(seed)\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint ('device:', device)\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n\ntrain_dataset, test_dataset = getDatasets()\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n\n# ## Implement the customized train and test loop\n\n\ndef train(model, device, train_loader, optimizer, epoch):\n    global completed_batch\n    train_loss = 0\n    correct = 0\n    total = 0\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n\n        completed_batch += 1\n\n        print ('Train - batches : {}, average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n           completed_batch, train_loss/(batch_idx+1), correct, total, 100.*correct/total))\n\n\ndef test(model, device, test_loader, epoch):\n    global completed_test_batch\n    global completed_batch\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    completed_test_batch = completed_batch -  len(test_loader)\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(test_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            loss = criterion(output, target)\n\n            test_loss += loss.item() # sum up batch loss\n            _, pred = output.max(1) # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            total += target.size(0)\n\n            completed_test_batch += 1\n\n    test_loss /= len(test_loader.dataset)\n    test_acc = 100. * correct / len(test_loader.dataset)\n    # Output test info for per epoch\n    print('Test - batches: {}, average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)\\n'.format(\n        completed_batch, test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\n# ## Create the Resnet18 model\n#use_cuda = not args.no_cuda\nprint(\"Use cuda: \", use_cuda)\n\n\nmodel_type = \"resnet18\"\nprint(\"=> using pytorch build-in model '{}'\".format(model_type))\n\nmodel = models.resnet18()\n#model = models.resnet50()\n\n\n# Using pytorch built-in resnet18 model, the model is pre-trained on the ImageNet dataset,\n# which has 1000 classifications. To transfer it to cifar10 dataset, we can modify the last fully-connected layer output size to 10\n\nfor param in model.parameters():\n    param.requires_grad = True  # set False if you only want to train the last layer using pretrained model\n    # Replace the last fully-connected layer\n    # Parameters of newly constructed modules have requires_grad=True by default\n    model.fc = nn.Linear(512, 10)\n\n\n# (Optional) To use wmla pretrained resnet18 model for cifar10, load the model weight file. The pretrained model weight file can be downloaded [here](https://?).\n\nweightfile = DATA_DIR + \"/checkpoint/model_epoch_final.pth\"\nif os.path.exists(weightfile):\n    print (\"Initial weight file is \" + weightfile)\n    model.load_state_dict(torch.load(weightfile, map_location=lambda storage, loc: storage))\n\n\n# ## Run the model trainings\n#print(model)\nmodel.to(device)\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)\nepochs = args.epochs\nscheduler = optim.lr_scheduler.StepLR(optimizer, 30, 0.1, last_epoch=-1)\n\n# Output total iterations info for deep learning insights\nprint(\"Total iterations: %s\" % (len(train_loader) * epochs))\n\n#print(\"RESULT_DIR: \" + os.getenv(\"RESULT_DIR\"))\n#RESULT_DIR = os.getenv(\"RESULT_DIR\")\nos.makedirs(RESULT_DIR, exist_ok=True)\n\nfor epoch in range(1, epochs+1):\n    print(\"\\nRunning epoch %s ... It might take several minutes for each epoch to run.\" % epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader, epoch)\n    scheduler.step()\n\n    torch.save(model.state_dict(),  RESULT_DIR + \"/model_epoch_%d.pth\"%(epoch))\n\ntorch.save(model.state_dict(), RESULT_DIR + \"/model_epoch_final.pth\")\n", "execution_count": 5, "outputs": [{"name": "stdout", "text": "Overwriting /project_data/data_asset/pytorch-resnet/resnet/main.py\n", "output_type": "stream"}]}, {"metadata": {"id": "4f3ad43f9efb42a68057fac608bd21c8"}, "cell_type": "markdown", "source": "## Training results on CPU\n\n#### Training was run from a Cloud Pak for Data Notebook utilizing a CPU kernel. \n\n\nIn the custom environment that was created with **16vCPU** and **32GB**, it took **1560 seconds** (or approximately **26 minutes**) to complete 1 EPOCH training.\n"}, {"metadata": {"id": "789d930b39b14945bf4fa4850b0d2475"}, "cell_type": "code", "source": "import datetime\nstarttime = datetime.datetime.now()\n\n! python /project_data/data_asset/pytorch-resnet/resnet/main.py --epochs 1 \n\nendtime = datetime.datetime.now()\nprint(\"Training cost: \", (endtime - starttime).seconds, \" seconds.\")", "execution_count": 8, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Namespace(batch_size=32, cuda=False, epochs=1, lr=0.01)\nUse cuda:  False\nDATA_DIR:  /project_data/data_asset/pytorch-resnet/data\ndevice: cpu\nFiles already downloaded and verified\nFiles already downloaded and verified\nUse cuda:  False\n=> using pytorch build-in model 'resnet18'\nTotal iterations: 1563\n\nRunning epoch 1 ... It might take several minutes for each epoch to run.\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n  allow_unreachable=True)  # allow_unreachable flag\nTrain - batches : 1, average loss: 2.4301, accuracy: 2/32 (6%)\nTrain - batches : 2, average loss: 2.4638, accuracy: 6/64 (9%)\nTrain - batches : 3, average loss: 2.4309, accuracy: 9/96 (9%)\nTrain - batches : 4, average loss: 2.4093, accuracy: 13/128 (10%)\nTrain - batches : 5, average loss: 2.3974, accuracy: 15/160 (9%)\nTrain - batches : 6, average loss: 2.3684, accuracy: 22/192 (11%)\nTrain - batches : 7, average loss: 2.3450, accuracy: 30/224 (13%)\nTrain - batches : 8, average loss: 2.3538, accuracy: 33/256 (13%)\nTrain - batches : 9, average loss: 2.3480, accuracy: 37/288 (13%)\nTrain - batches : 10, average loss: 2.3373, accuracy: 41/320 (13%)\nTrain - batches : 11, average loss: 2.3271, accuracy: 49/352 (14%)\nTrain - batches : 12, average loss: 2.3184, accuracy: 55/384 (14%)\nTrain - batches : 13, average loss: 2.3178, accuracy: 58/416 (14%)\nTrain - batches : 14, average loss: 2.3133, accuracy: 62/448 (14%)\nTrain - batches : 15, average loss: 2.3064, accuracy: 64/480 (13%)\nTrain - batches : 16, average loss: 2.3092, accuracy: 69/512 (13%)\nTrain - batches : 17, average loss: 2.3095, accuracy: 73/544 (13%)\nTrain - batches : 18, average loss: 2.3043, accuracy: 79/576 (14%)\nTrain - batches : 19, average loss: 2.3015, accuracy: 83/608 (14%)\nTrain - batches : 20, average loss: 2.2980, accuracy: 91/640 (14%)\nTrain - batches : 21, average loss: 2.2958, accuracy: 95/672 (14%)\nTrain - batches : 22, average loss: 2.2897, accuracy: 103/704 (15%)\nTrain - batches : 23, average loss: 2.2879, accuracy: 107/736 (15%)\nTrain - batches : 24, average loss: 2.2811, accuracy: 113/768 (15%)\nTrain - batches : 25, average loss: 2.2774, accuracy: 117/800 (15%)\nTrain - batches : 26, average loss: 2.2748, accuracy: 121/832 (15%)\nTrain - batches : 27, average loss: 2.2731, accuracy: 127/864 (15%)\nTrain - batches : 28, average loss: 2.2714, accuracy: 132/896 (15%)\nTrain - batches : 29, average loss: 2.2674, accuracy: 134/928 (14%)\nTrain - batches : 30, average loss: 2.2612, accuracy: 140/960 (15%)\nTrain - batches : 31, average loss: 2.2559, accuracy: 148/992 (15%)\nTrain - batches : 32, average loss: 2.2505, accuracy: 154/1024 (15%)\nTrain - batches : 33, average loss: 2.2443, accuracy: 163/1056 (15%)\nTrain - batches : 34, average loss: 2.2382, accuracy: 173/1088 (16%)\nTrain - batches : 35, average loss: 2.2339, accuracy: 181/1120 (16%)\nTrain - batches : 36, average loss: 2.2321, accuracy: 185/1152 (16%)\nTrain - batches : 37, average loss: 2.2290, accuracy: 194/1184 (16%)\nTrain - batches : 38, average loss: 2.2273, accuracy: 199/1216 (16%)\nTrain - batches : 39, average loss: 2.2236, accuracy: 203/1248 (16%)\nTrain - batches : 40, average loss: 2.2229, accuracy: 207/1280 (16%)\nTrain - batches : 41, average loss: 2.2209, accuracy: 215/1312 (16%)\nTrain - batches : 42, average loss: 2.2176, accuracy: 221/1344 (16%)\nTrain - batches : 43, average loss: 2.2129, accuracy: 225/1376 (16%)\nTrain - batches : 44, average loss: 2.2106, accuracy: 232/1408 (16%)\nTrain - batches : 45, average loss: 2.2063, accuracy: 243/1440 (17%)\nTrain - batches : 46, average loss: 2.2065, accuracy: 248/1472 (17%)\nTrain - batches : 47, average loss: 2.2021, accuracy: 258/1504 (17%)\nTrain - batches : 48, average loss: 2.1985, accuracy: 266/1536 (17%)\nTrain - batches : 49, average loss: 2.1965, accuracy: 275/1568 (18%)\nTrain - batches : 50, average loss: 2.1944, accuracy: 284/1600 (18%)\nTrain - batches : 51, average loss: 2.1875, accuracy: 297/1632 (18%)\nTrain - batches : 52, average loss: 2.1885, accuracy: 304/1664 (18%)\nTrain - batches : 53, average loss: 2.1864, accuracy: 311/1696 (18%)\nTrain - batches : 54, average loss: 2.1860, accuracy: 314/1728 (18%)\nTrain - batches : 55, average loss: 2.1820, accuracy: 321/1760 (18%)\nTrain - batches : 56, average loss: 2.1812, accuracy: 326/1792 (18%)\nTrain - batches : 57, average loss: 2.1782, accuracy: 334/1824 (18%)\nTrain - batches : 58, average loss: 2.1759, accuracy: 342/1856 (18%)\nTrain - batches : 59, average loss: 2.1731, accuracy: 348/1888 (18%)\nTrain - batches : 60, average loss: 2.1699, accuracy: 354/1920 (18%)\nTrain - batches : 61, average loss: 2.1675, accuracy: 363/1952 (19%)\nTrain - batches : 62, average loss: 2.1620, accuracy: 370/1984 (19%)\nTrain - batches : 63, average loss: 2.1608, accuracy: 375/2016 (19%)\nTrain - batches : 64, average loss: 2.1566, accuracy: 382/2048 (19%)\nTrain - batches : 65, average loss: 2.1531, accuracy: 388/2080 (19%)\nTrain - batches : 66, average loss: 2.1500, accuracy: 398/2112 (19%)\nTrain - batches : 67, average loss: 2.1457, accuracy: 409/2144 (19%)\nTrain - batches : 68, average loss: 2.1437, accuracy: 416/2176 (19%)\nTrain - batches : 69, average loss: 2.1400, accuracy: 428/2208 (19%)\nTrain - batches : 70, average loss: 2.1382, accuracy: 438/2240 (20%)\nTrain - batches : 71, average loss: 2.1350, accuracy: 449/2272 (20%)\nTrain - batches : 72, average loss: 2.1302, accuracy: 461/2304 (20%)\nTrain - batches : 73, average loss: 2.1311, accuracy: 468/2336 (20%)\nTrain - batches : 74, average loss: 2.1303, accuracy: 474/2368 (20%)\nTrain - batches : 75, average loss: 2.1264, accuracy: 485/2400 (20%)\nTrain - batches : 76, average loss: 2.1266, accuracy: 487/2432 (20%)\nTrain - batches : 77, average loss: 2.1243, accuracy: 496/2464 (20%)\nTrain - batches : 78, average loss: 2.1257, accuracy: 502/2496 (20%)\nTrain - batches : 79, average loss: 2.1257, accuracy: 512/2528 (20%)\nTrain - batches : 80, average loss: 2.1253, accuracy: 518/2560 (20%)\nTrain - batches : 81, average loss: 2.1233, accuracy: 524/2592 (20%)\nTrain - batches : 82, average loss: 2.1208, accuracy: 536/2624 (20%)\nTrain - batches : 83, average loss: 2.1200, accuracy: 539/2656 (20%)\nTrain - batches : 84, average loss: 2.1167, accuracy: 550/2688 (20%)\nTrain - batches : 85, average loss: 2.1156, accuracy: 557/2720 (20%)\nTrain - batches : 86, average loss: 2.1134, accuracy: 563/2752 (20%)\nTrain - batches : 87, average loss: 2.1118, accuracy: 571/2784 (21%)\nTrain - batches : 88, average loss: 2.1096, accuracy: 579/2816 (21%)\nTrain - batches : 89, average loss: 2.1087, accuracy: 588/2848 (21%)\nTrain - batches : 90, average loss: 2.1095, accuracy: 593/2880 (21%)\nTrain - batches : 91, average loss: 2.1076, accuracy: 604/2912 (21%)\nTrain - batches : 92, average loss: 2.1058, accuracy: 618/2944 (21%)\nTrain - batches : 93, average loss: 2.1046, accuracy: 626/2976 (21%)\nTrain - batches : 94, average loss: 2.1022, accuracy: 637/3008 (21%)\nTrain - batches : 95, average loss: 2.0997, accuracy: 646/3040 (21%)\nTrain - batches : 96, average loss: 2.0955, accuracy: 660/3072 (21%)\nTrain - batches : 97, average loss: 2.0931, accuracy: 669/3104 (22%)\nTrain - batches : 98, average loss: 2.0898, accuracy: 682/3136 (22%)\nTrain - batches : 99, average loss: 2.0860, accuracy: 693/3168 (22%)\nTrain - batches : 100, average loss: 2.0855, accuracy: 702/3200 (22%)\nTrain - batches : 101, average loss: 2.0833, accuracy: 713/3232 (22%)\nTrain - batches : 102, average loss: 2.0805, accuracy: 727/3264 (22%)\nTrain - batches : 103, average loss: 2.0814, accuracy: 734/3296 (22%)\nTrain - batches : 104, average loss: 2.0799, accuracy: 743/3328 (22%)\nTrain - batches : 105, average loss: 2.0787, accuracy: 751/3360 (22%)\nTrain - batches : 106, average loss: 2.0776, accuracy: 758/3392 (22%)\nTrain - batches : 107, average loss: 2.0764, accuracy: 768/3424 (22%)\nTrain - batches : 108, average loss: 2.0745, accuracy: 778/3456 (23%)\nTrain - batches : 109, average loss: 2.0750, accuracy: 785/3488 (23%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 110, average loss: 2.0734, accuracy: 792/3520 (22%)\nTrain - batches : 111, average loss: 2.0729, accuracy: 798/3552 (22%)\nTrain - batches : 112, average loss: 2.0733, accuracy: 804/3584 (22%)\nTrain - batches : 113, average loss: 2.0718, accuracy: 817/3616 (23%)\nTrain - batches : 114, average loss: 2.0725, accuracy: 824/3648 (23%)\nTrain - batches : 115, average loss: 2.0705, accuracy: 835/3680 (23%)\nTrain - batches : 116, average loss: 2.0694, accuracy: 845/3712 (23%)\nTrain - batches : 117, average loss: 2.0700, accuracy: 851/3744 (23%)\nTrain - batches : 118, average loss: 2.0680, accuracy: 862/3776 (23%)\nTrain - batches : 119, average loss: 2.0669, accuracy: 873/3808 (23%)\nTrain - batches : 120, average loss: 2.0678, accuracy: 880/3840 (23%)\nTrain - batches : 121, average loss: 2.0653, accuracy: 887/3872 (23%)\nTrain - batches : 122, average loss: 2.0643, accuracy: 896/3904 (23%)\nTrain - batches : 123, average loss: 2.0622, accuracy: 904/3936 (23%)\nTrain - batches : 124, average loss: 2.0608, accuracy: 916/3968 (23%)\nTrain - batches : 125, average loss: 2.0599, accuracy: 923/4000 (23%)\nTrain - batches : 126, average loss: 2.0582, accuracy: 932/4032 (23%)\nTrain - batches : 127, average loss: 2.0562, accuracy: 942/4064 (23%)\nTrain - batches : 128, average loss: 2.0547, accuracy: 953/4096 (23%)\nTrain - batches : 129, average loss: 2.0551, accuracy: 960/4128 (23%)\nTrain - batches : 130, average loss: 2.0538, accuracy: 972/4160 (23%)\nTrain - batches : 131, average loss: 2.0538, accuracy: 982/4192 (23%)\nTrain - batches : 132, average loss: 2.0540, accuracy: 991/4224 (23%)\nTrain - batches : 133, average loss: 2.0527, accuracy: 1001/4256 (24%)\nTrain - batches : 134, average loss: 2.0516, accuracy: 1010/4288 (24%)\nTrain - batches : 135, average loss: 2.0500, accuracy: 1021/4320 (24%)\nTrain - batches : 136, average loss: 2.0472, accuracy: 1035/4352 (24%)\nTrain - batches : 137, average loss: 2.0459, accuracy: 1043/4384 (24%)\nTrain - batches : 138, average loss: 2.0452, accuracy: 1054/4416 (24%)\nTrain - batches : 139, average loss: 2.0417, accuracy: 1066/4448 (24%)\nTrain - batches : 140, average loss: 2.0406, accuracy: 1075/4480 (24%)\nTrain - batches : 141, average loss: 2.0400, accuracy: 1083/4512 (24%)\nTrain - batches : 142, average loss: 2.0395, accuracy: 1088/4544 (24%)\nTrain - batches : 143, average loss: 2.0392, accuracy: 1098/4576 (24%)\nTrain - batches : 144, average loss: 2.0368, accuracy: 1108/4608 (24%)\nTrain - batches : 145, average loss: 2.0363, accuracy: 1116/4640 (24%)\nTrain - batches : 146, average loss: 2.0349, accuracy: 1128/4672 (24%)\nTrain - batches : 147, average loss: 2.0318, accuracy: 1141/4704 (24%)\nTrain - batches : 148, average loss: 2.0301, accuracy: 1154/4736 (24%)\nTrain - batches : 149, average loss: 2.0304, accuracy: 1159/4768 (24%)\nTrain - batches : 150, average loss: 2.0314, accuracy: 1164/4800 (24%)\nTrain - batches : 151, average loss: 2.0320, accuracy: 1171/4832 (24%)\nTrain - batches : 152, average loss: 2.0310, accuracy: 1181/4864 (24%)\nTrain - batches : 153, average loss: 2.0305, accuracy: 1192/4896 (24%)\nTrain - batches : 154, average loss: 2.0301, accuracy: 1199/4928 (24%)\nTrain - batches : 155, average loss: 2.0302, accuracy: 1206/4960 (24%)\nTrain - batches : 156, average loss: 2.0298, accuracy: 1215/4992 (24%)\nTrain - batches : 157, average loss: 2.0293, accuracy: 1224/5024 (24%)\nTrain - batches : 158, average loss: 2.0279, accuracy: 1235/5056 (24%)\nTrain - batches : 159, average loss: 2.0262, accuracy: 1243/5088 (24%)\nTrain - batches : 160, average loss: 2.0254, accuracy: 1249/5120 (24%)\nTrain - batches : 161, average loss: 2.0255, accuracy: 1258/5152 (24%)\nTrain - batches : 162, average loss: 2.0250, accuracy: 1267/5184 (24%)\nTrain - batches : 163, average loss: 2.0242, accuracy: 1276/5216 (24%)\nTrain - batches : 164, average loss: 2.0226, accuracy: 1284/5248 (24%)\nTrain - batches : 165, average loss: 2.0215, accuracy: 1293/5280 (24%)\nTrain - batches : 166, average loss: 2.0209, accuracy: 1301/5312 (24%)\nTrain - batches : 167, average loss: 2.0205, accuracy: 1310/5344 (25%)\nTrain - batches : 168, average loss: 2.0194, accuracy: 1322/5376 (25%)\nTrain - batches : 169, average loss: 2.0176, accuracy: 1333/5408 (25%)\nTrain - batches : 170, average loss: 2.0166, accuracy: 1342/5440 (25%)\nTrain - batches : 171, average loss: 2.0157, accuracy: 1354/5472 (25%)\nTrain - batches : 172, average loss: 2.0147, accuracy: 1364/5504 (25%)\nTrain - batches : 173, average loss: 2.0125, accuracy: 1375/5536 (25%)\nTrain - batches : 174, average loss: 2.0109, accuracy: 1384/5568 (25%)\nTrain - batches : 175, average loss: 2.0104, accuracy: 1393/5600 (25%)\nTrain - batches : 176, average loss: 2.0109, accuracy: 1398/5632 (25%)\nTrain - batches : 177, average loss: 2.0109, accuracy: 1407/5664 (25%)\nTrain - batches : 178, average loss: 2.0110, accuracy: 1410/5696 (25%)\nTrain - batches : 179, average loss: 2.0097, accuracy: 1423/5728 (25%)\nTrain - batches : 180, average loss: 2.0084, accuracy: 1431/5760 (25%)\nTrain - batches : 181, average loss: 2.0094, accuracy: 1436/5792 (25%)\nTrain - batches : 182, average loss: 2.0087, accuracy: 1445/5824 (25%)\nTrain - batches : 183, average loss: 2.0080, accuracy: 1452/5856 (25%)\nTrain - batches : 184, average loss: 2.0070, accuracy: 1462/5888 (25%)\nTrain - batches : 185, average loss: 2.0053, accuracy: 1474/5920 (25%)\nTrain - batches : 186, average loss: 2.0035, accuracy: 1484/5952 (25%)\nTrain - batches : 187, average loss: 2.0026, accuracy: 1495/5984 (25%)\nTrain - batches : 188, average loss: 2.0021, accuracy: 1504/6016 (25%)\nTrain - batches : 189, average loss: 2.0012, accuracy: 1512/6048 (25%)\nTrain - batches : 190, average loss: 1.9998, accuracy: 1520/6080 (25%)\nTrain - batches : 191, average loss: 1.9983, accuracy: 1533/6112 (25%)\nTrain - batches : 192, average loss: 1.9974, accuracy: 1539/6144 (25%)\nTrain - batches : 193, average loss: 1.9966, accuracy: 1549/6176 (25%)\nTrain - batches : 194, average loss: 1.9965, accuracy: 1555/6208 (25%)\nTrain - batches : 195, average loss: 1.9958, accuracy: 1563/6240 (25%)\nTrain - batches : 196, average loss: 1.9943, accuracy: 1575/6272 (25%)\nTrain - batches : 197, average loss: 1.9940, accuracy: 1582/6304 (25%)\nTrain - batches : 198, average loss: 1.9918, accuracy: 1595/6336 (25%)\nTrain - batches : 199, average loss: 1.9925, accuracy: 1600/6368 (25%)\nTrain - batches : 200, average loss: 1.9924, accuracy: 1607/6400 (25%)\nTrain - batches : 201, average loss: 1.9912, accuracy: 1615/6432 (25%)\nTrain - batches : 202, average loss: 1.9913, accuracy: 1623/6464 (25%)\nTrain - batches : 203, average loss: 1.9914, accuracy: 1632/6496 (25%)\nTrain - batches : 204, average loss: 1.9908, accuracy: 1643/6528 (25%)\nTrain - batches : 205, average loss: 1.9888, accuracy: 1659/6560 (25%)\nTrain - batches : 206, average loss: 1.9877, accuracy: 1671/6592 (25%)\nTrain - batches : 207, average loss: 1.9865, accuracy: 1681/6624 (25%)\nTrain - batches : 208, average loss: 1.9850, accuracy: 1693/6656 (25%)\nTrain - batches : 209, average loss: 1.9843, accuracy: 1701/6688 (25%)\nTrain - batches : 210, average loss: 1.9835, accuracy: 1712/6720 (25%)\nTrain - batches : 211, average loss: 1.9834, accuracy: 1722/6752 (26%)\nTrain - batches : 212, average loss: 1.9829, accuracy: 1730/6784 (26%)\nTrain - batches : 213, average loss: 1.9832, accuracy: 1739/6816 (26%)\nTrain - batches : 214, average loss: 1.9823, accuracy: 1752/6848 (26%)\nTrain - batches : 215, average loss: 1.9810, accuracy: 1763/6880 (26%)\nTrain - batches : 216, average loss: 1.9805, accuracy: 1775/6912 (26%)\nTrain - batches : 217, average loss: 1.9799, accuracy: 1783/6944 (26%)\nTrain - batches : 218, average loss: 1.9790, accuracy: 1794/6976 (26%)\nTrain - batches : 219, average loss: 1.9771, accuracy: 1810/7008 (26%)\nTrain - batches : 220, average loss: 1.9763, accuracy: 1816/7040 (26%)\nTrain - batches : 221, average loss: 1.9744, accuracy: 1829/7072 (26%)\nTrain - batches : 222, average loss: 1.9731, accuracy: 1843/7104 (26%)\nTrain - batches : 223, average loss: 1.9734, accuracy: 1851/7136 (26%)\nTrain - batches : 224, average loss: 1.9730, accuracy: 1857/7168 (26%)\nTrain - batches : 225, average loss: 1.9716, accuracy: 1870/7200 (26%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 226, average loss: 1.9700, accuracy: 1884/7232 (26%)\nTrain - batches : 227, average loss: 1.9697, accuracy: 1894/7264 (26%)\nTrain - batches : 228, average loss: 1.9684, accuracy: 1907/7296 (26%)\nTrain - batches : 229, average loss: 1.9691, accuracy: 1918/7328 (26%)\nTrain - batches : 230, average loss: 1.9682, accuracy: 1927/7360 (26%)\nTrain - batches : 231, average loss: 1.9685, accuracy: 1936/7392 (26%)\nTrain - batches : 232, average loss: 1.9682, accuracy: 1947/7424 (26%)\nTrain - batches : 233, average loss: 1.9668, accuracy: 1955/7456 (26%)\nTrain - batches : 234, average loss: 1.9667, accuracy: 1960/7488 (26%)\nTrain - batches : 235, average loss: 1.9654, accuracy: 1969/7520 (26%)\nTrain - batches : 236, average loss: 1.9648, accuracy: 1979/7552 (26%)\nTrain - batches : 237, average loss: 1.9634, accuracy: 1992/7584 (26%)\nTrain - batches : 238, average loss: 1.9629, accuracy: 2001/7616 (26%)\nTrain - batches : 239, average loss: 1.9622, accuracy: 2013/7648 (26%)\nTrain - batches : 240, average loss: 1.9621, accuracy: 2025/7680 (26%)\nTrain - batches : 241, average loss: 1.9605, accuracy: 2034/7712 (26%)\nTrain - batches : 242, average loss: 1.9607, accuracy: 2042/7744 (26%)\nTrain - batches : 243, average loss: 1.9593, accuracy: 2051/7776 (26%)\nTrain - batches : 244, average loss: 1.9582, accuracy: 2065/7808 (26%)\nTrain - batches : 245, average loss: 1.9568, accuracy: 2078/7840 (27%)\nTrain - batches : 246, average loss: 1.9567, accuracy: 2086/7872 (26%)\nTrain - batches : 247, average loss: 1.9563, accuracy: 2093/7904 (26%)\nTrain - batches : 248, average loss: 1.9553, accuracy: 2103/7936 (26%)\nTrain - batches : 249, average loss: 1.9539, accuracy: 2116/7968 (27%)\nTrain - batches : 250, average loss: 1.9529, accuracy: 2130/8000 (27%)\nTrain - batches : 251, average loss: 1.9523, accuracy: 2139/8032 (27%)\nTrain - batches : 252, average loss: 1.9513, accuracy: 2148/8064 (27%)\nTrain - batches : 253, average loss: 1.9497, accuracy: 2164/8096 (27%)\nTrain - batches : 254, average loss: 1.9495, accuracy: 2175/8128 (27%)\nTrain - batches : 255, average loss: 1.9489, accuracy: 2188/8160 (27%)\nTrain - batches : 256, average loss: 1.9490, accuracy: 2195/8192 (27%)\nTrain - batches : 257, average loss: 1.9490, accuracy: 2197/8224 (27%)\nTrain - batches : 258, average loss: 1.9486, accuracy: 2206/8256 (27%)\nTrain - batches : 259, average loss: 1.9473, accuracy: 2218/8288 (27%)\nTrain - batches : 260, average loss: 1.9460, accuracy: 2235/8320 (27%)\nTrain - batches : 261, average loss: 1.9450, accuracy: 2249/8352 (27%)\nTrain - batches : 262, average loss: 1.9437, accuracy: 2262/8384 (27%)\nTrain - batches : 263, average loss: 1.9444, accuracy: 2269/8416 (27%)\nTrain - batches : 264, average loss: 1.9447, accuracy: 2276/8448 (27%)\nTrain - batches : 265, average loss: 1.9440, accuracy: 2287/8480 (27%)\nTrain - batches : 266, average loss: 1.9433, accuracy: 2297/8512 (27%)\nTrain - batches : 267, average loss: 1.9421, accuracy: 2307/8544 (27%)\nTrain - batches : 268, average loss: 1.9407, accuracy: 2322/8576 (27%)\nTrain - batches : 269, average loss: 1.9399, accuracy: 2336/8608 (27%)\nTrain - batches : 270, average loss: 1.9400, accuracy: 2346/8640 (27%)\nTrain - batches : 271, average loss: 1.9391, accuracy: 2360/8672 (27%)\nTrain - batches : 272, average loss: 1.9390, accuracy: 2371/8704 (27%)\nTrain - batches : 273, average loss: 1.9381, accuracy: 2382/8736 (27%)\nTrain - batches : 274, average loss: 1.9374, accuracy: 2392/8768 (27%)\nTrain - batches : 275, average loss: 1.9364, accuracy: 2405/8800 (27%)\nTrain - batches : 276, average loss: 1.9354, accuracy: 2419/8832 (27%)\nTrain - batches : 277, average loss: 1.9342, accuracy: 2431/8864 (27%)\nTrain - batches : 278, average loss: 1.9345, accuracy: 2441/8896 (27%)\nTrain - batches : 279, average loss: 1.9334, accuracy: 2455/8928 (27%)\nTrain - batches : 280, average loss: 1.9327, accuracy: 2463/8960 (27%)\nTrain - batches : 281, average loss: 1.9321, accuracy: 2470/8992 (27%)\nTrain - batches : 282, average loss: 1.9314, accuracy: 2482/9024 (28%)\nTrain - batches : 283, average loss: 1.9305, accuracy: 2493/9056 (28%)\nTrain - batches : 284, average loss: 1.9305, accuracy: 2501/9088 (28%)\nTrain - batches : 285, average loss: 1.9302, accuracy: 2511/9120 (28%)\nTrain - batches : 286, average loss: 1.9297, accuracy: 2523/9152 (28%)\nTrain - batches : 287, average loss: 1.9299, accuracy: 2532/9184 (28%)\nTrain - batches : 288, average loss: 1.9293, accuracy: 2543/9216 (28%)\nTrain - batches : 289, average loss: 1.9288, accuracy: 2557/9248 (28%)\nTrain - batches : 290, average loss: 1.9280, accuracy: 2570/9280 (28%)\nTrain - batches : 291, average loss: 1.9273, accuracy: 2586/9312 (28%)\nTrain - batches : 292, average loss: 1.9265, accuracy: 2598/9344 (28%)\nTrain - batches : 293, average loss: 1.9261, accuracy: 2612/9376 (28%)\nTrain - batches : 294, average loss: 1.9255, accuracy: 2622/9408 (28%)\nTrain - batches : 295, average loss: 1.9253, accuracy: 2633/9440 (28%)\nTrain - batches : 296, average loss: 1.9248, accuracy: 2641/9472 (28%)\nTrain - batches : 297, average loss: 1.9245, accuracy: 2654/9504 (28%)\nTrain - batches : 298, average loss: 1.9244, accuracy: 2666/9536 (28%)\nTrain - batches : 299, average loss: 1.9242, accuracy: 2676/9568 (28%)\nTrain - batches : 300, average loss: 1.9245, accuracy: 2686/9600 (28%)\nTrain - batches : 301, average loss: 1.9247, accuracy: 2697/9632 (28%)\nTrain - batches : 302, average loss: 1.9236, accuracy: 2707/9664 (28%)\nTrain - batches : 303, average loss: 1.9232, accuracy: 2717/9696 (28%)\nTrain - batches : 304, average loss: 1.9223, accuracy: 2729/9728 (28%)\nTrain - batches : 305, average loss: 1.9220, accuracy: 2738/9760 (28%)\nTrain - batches : 306, average loss: 1.9209, accuracy: 2750/9792 (28%)\nTrain - batches : 307, average loss: 1.9203, accuracy: 2761/9824 (28%)\nTrain - batches : 308, average loss: 1.9203, accuracy: 2769/9856 (28%)\nTrain - batches : 309, average loss: 1.9192, accuracy: 2782/9888 (28%)\nTrain - batches : 310, average loss: 1.9179, accuracy: 2796/9920 (28%)\nTrain - batches : 311, average loss: 1.9176, accuracy: 2805/9952 (28%)\nTrain - batches : 312, average loss: 1.9157, accuracy: 2825/9984 (28%)\nTrain - batches : 313, average loss: 1.9155, accuracy: 2837/10016 (28%)\nTrain - batches : 314, average loss: 1.9149, accuracy: 2850/10048 (28%)\nTrain - batches : 315, average loss: 1.9141, accuracy: 2865/10080 (28%)\nTrain - batches : 316, average loss: 1.9148, accuracy: 2874/10112 (28%)\nTrain - batches : 317, average loss: 1.9140, accuracy: 2887/10144 (28%)\nTrain - batches : 318, average loss: 1.9140, accuracy: 2897/10176 (28%)\nTrain - batches : 319, average loss: 1.9134, accuracy: 2909/10208 (28%)\nTrain - batches : 320, average loss: 1.9122, accuracy: 2919/10240 (29%)\nTrain - batches : 321, average loss: 1.9107, accuracy: 2934/10272 (29%)\nTrain - batches : 322, average loss: 1.9102, accuracy: 2944/10304 (29%)\nTrain - batches : 323, average loss: 1.9098, accuracy: 2955/10336 (29%)\nTrain - batches : 324, average loss: 1.9089, accuracy: 2965/10368 (29%)\nTrain - batches : 325, average loss: 1.9082, accuracy: 2976/10400 (29%)\nTrain - batches : 326, average loss: 1.9073, accuracy: 2987/10432 (29%)\nTrain - batches : 327, average loss: 1.9072, accuracy: 2996/10464 (29%)\nTrain - batches : 328, average loss: 1.9067, accuracy: 3003/10496 (29%)\nTrain - batches : 329, average loss: 1.9068, accuracy: 3009/10528 (29%)\nTrain - batches : 330, average loss: 1.9056, accuracy: 3021/10560 (29%)\nTrain - batches : 331, average loss: 1.9050, accuracy: 3032/10592 (29%)\nTrain - batches : 332, average loss: 1.9051, accuracy: 3039/10624 (29%)\nTrain - batches : 333, average loss: 1.9041, accuracy: 3050/10656 (29%)\nTrain - batches : 334, average loss: 1.9030, accuracy: 3063/10688 (29%)\nTrain - batches : 335, average loss: 1.9023, accuracy: 3075/10720 (29%)\nTrain - batches : 336, average loss: 1.9016, accuracy: 3087/10752 (29%)\nTrain - batches : 337, average loss: 1.9006, accuracy: 3099/10784 (29%)\nTrain - batches : 338, average loss: 1.9004, accuracy: 3107/10816 (29%)\nTrain - batches : 339, average loss: 1.9003, accuracy: 3119/10848 (29%)\nTrain - batches : 340, average loss: 1.8996, accuracy: 3130/10880 (29%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 341, average loss: 1.8997, accuracy: 3142/10912 (29%)\nTrain - batches : 342, average loss: 1.8992, accuracy: 3150/10944 (29%)\nTrain - batches : 343, average loss: 1.8986, accuracy: 3159/10976 (29%)\nTrain - batches : 344, average loss: 1.8984, accuracy: 3172/11008 (29%)\nTrain - batches : 345, average loss: 1.8981, accuracy: 3189/11040 (29%)\nTrain - batches : 346, average loss: 1.8978, accuracy: 3199/11072 (29%)\nTrain - batches : 347, average loss: 1.8973, accuracy: 3214/11104 (29%)\nTrain - batches : 348, average loss: 1.8963, accuracy: 3223/11136 (29%)\nTrain - batches : 349, average loss: 1.8957, accuracy: 3231/11168 (29%)\nTrain - batches : 350, average loss: 1.8960, accuracy: 3243/11200 (29%)\nTrain - batches : 351, average loss: 1.8956, accuracy: 3251/11232 (29%)\nTrain - batches : 352, average loss: 1.8952, accuracy: 3260/11264 (29%)\nTrain - batches : 353, average loss: 1.8951, accuracy: 3266/11296 (29%)\nTrain - batches : 354, average loss: 1.8942, accuracy: 3276/11328 (29%)\nTrain - batches : 355, average loss: 1.8937, accuracy: 3285/11360 (29%)\nTrain - batches : 356, average loss: 1.8933, accuracy: 3294/11392 (29%)\nTrain - batches : 357, average loss: 1.8923, accuracy: 3303/11424 (29%)\nTrain - batches : 358, average loss: 1.8917, accuracy: 3314/11456 (29%)\nTrain - batches : 359, average loss: 1.8908, accuracy: 3327/11488 (29%)\nTrain - batches : 360, average loss: 1.8895, accuracy: 3340/11520 (29%)\nTrain - batches : 361, average loss: 1.8888, accuracy: 3348/11552 (29%)\nTrain - batches : 362, average loss: 1.8883, accuracy: 3360/11584 (29%)\nTrain - batches : 363, average loss: 1.8880, accuracy: 3368/11616 (29%)\nTrain - batches : 364, average loss: 1.8874, accuracy: 3378/11648 (29%)\nTrain - batches : 365, average loss: 1.8873, accuracy: 3390/11680 (29%)\nTrain - batches : 366, average loss: 1.8869, accuracy: 3400/11712 (29%)\nTrain - batches : 367, average loss: 1.8866, accuracy: 3413/11744 (29%)\nTrain - batches : 368, average loss: 1.8862, accuracy: 3423/11776 (29%)\nTrain - batches : 369, average loss: 1.8852, accuracy: 3435/11808 (29%)\nTrain - batches : 370, average loss: 1.8848, accuracy: 3449/11840 (29%)\nTrain - batches : 371, average loss: 1.8844, accuracy: 3458/11872 (29%)\nTrain - batches : 372, average loss: 1.8835, accuracy: 3475/11904 (29%)\nTrain - batches : 373, average loss: 1.8830, accuracy: 3487/11936 (29%)\nTrain - batches : 374, average loss: 1.8823, accuracy: 3501/11968 (29%)\nTrain - batches : 375, average loss: 1.8812, accuracy: 3520/12000 (29%)\nTrain - batches : 376, average loss: 1.8805, accuracy: 3532/12032 (29%)\nTrain - batches : 377, average loss: 1.8800, accuracy: 3545/12064 (29%)\nTrain - batches : 378, average loss: 1.8795, accuracy: 3559/12096 (29%)\nTrain - batches : 379, average loss: 1.8799, accuracy: 3571/12128 (29%)\nTrain - batches : 380, average loss: 1.8801, accuracy: 3581/12160 (29%)\nTrain - batches : 381, average loss: 1.8799, accuracy: 3590/12192 (29%)\nTrain - batches : 382, average loss: 1.8790, accuracy: 3603/12224 (29%)\nTrain - batches : 383, average loss: 1.8788, accuracy: 3616/12256 (30%)\nTrain - batches : 384, average loss: 1.8782, accuracy: 3626/12288 (30%)\nTrain - batches : 385, average loss: 1.8776, accuracy: 3639/12320 (30%)\nTrain - batches : 386, average loss: 1.8775, accuracy: 3650/12352 (30%)\nTrain - batches : 387, average loss: 1.8768, accuracy: 3663/12384 (30%)\nTrain - batches : 388, average loss: 1.8756, accuracy: 3680/12416 (30%)\nTrain - batches : 389, average loss: 1.8751, accuracy: 3691/12448 (30%)\nTrain - batches : 390, average loss: 1.8747, accuracy: 3703/12480 (30%)\nTrain - batches : 391, average loss: 1.8736, accuracy: 3714/12512 (30%)\nTrain - batches : 392, average loss: 1.8734, accuracy: 3728/12544 (30%)\nTrain - batches : 393, average loss: 1.8731, accuracy: 3739/12576 (30%)\nTrain - batches : 394, average loss: 1.8719, accuracy: 3760/12608 (30%)\nTrain - batches : 395, average loss: 1.8710, accuracy: 3772/12640 (30%)\nTrain - batches : 396, average loss: 1.8712, accuracy: 3779/12672 (30%)\nTrain - batches : 397, average loss: 1.8711, accuracy: 3790/12704 (30%)\nTrain - batches : 398, average loss: 1.8705, accuracy: 3802/12736 (30%)\nTrain - batches : 399, average loss: 1.8695, accuracy: 3814/12768 (30%)\nTrain - batches : 400, average loss: 1.8683, accuracy: 3829/12800 (30%)\nTrain - batches : 401, average loss: 1.8675, accuracy: 3842/12832 (30%)\nTrain - batches : 402, average loss: 1.8672, accuracy: 3850/12864 (30%)\nTrain - batches : 403, average loss: 1.8671, accuracy: 3860/12896 (30%)\nTrain - batches : 404, average loss: 1.8665, accuracy: 3869/12928 (30%)\nTrain - batches : 405, average loss: 1.8653, accuracy: 3883/12960 (30%)\nTrain - batches : 406, average loss: 1.8648, accuracy: 3890/12992 (30%)\nTrain - batches : 407, average loss: 1.8645, accuracy: 3902/13024 (30%)\nTrain - batches : 408, average loss: 1.8639, accuracy: 3916/13056 (30%)\nTrain - batches : 409, average loss: 1.8640, accuracy: 3926/13088 (30%)\nTrain - batches : 410, average loss: 1.8634, accuracy: 3941/13120 (30%)\nTrain - batches : 411, average loss: 1.8628, accuracy: 3952/13152 (30%)\nTrain - batches : 412, average loss: 1.8626, accuracy: 3965/13184 (30%)\nTrain - batches : 413, average loss: 1.8619, accuracy: 3978/13216 (30%)\nTrain - batches : 414, average loss: 1.8613, accuracy: 3991/13248 (30%)\nTrain - batches : 415, average loss: 1.8611, accuracy: 3999/13280 (30%)\nTrain - batches : 416, average loss: 1.8609, accuracy: 4009/13312 (30%)\nTrain - batches : 417, average loss: 1.8602, accuracy: 4024/13344 (30%)\nTrain - batches : 418, average loss: 1.8596, accuracy: 4035/13376 (30%)\nTrain - batches : 419, average loss: 1.8591, accuracy: 4041/13408 (30%)\nTrain - batches : 420, average loss: 1.8582, accuracy: 4057/13440 (30%)\nTrain - batches : 421, average loss: 1.8581, accuracy: 4068/13472 (30%)\nTrain - batches : 422, average loss: 1.8578, accuracy: 4079/13504 (30%)\nTrain - batches : 423, average loss: 1.8576, accuracy: 4092/13536 (30%)\nTrain - batches : 424, average loss: 1.8570, accuracy: 4106/13568 (30%)\nTrain - batches : 425, average loss: 1.8563, accuracy: 4118/13600 (30%)\nTrain - batches : 426, average loss: 1.8559, accuracy: 4129/13632 (30%)\nTrain - batches : 427, average loss: 1.8560, accuracy: 4137/13664 (30%)\nTrain - batches : 428, average loss: 1.8553, accuracy: 4148/13696 (30%)\nTrain - batches : 429, average loss: 1.8548, accuracy: 4160/13728 (30%)\nTrain - batches : 430, average loss: 1.8549, accuracy: 4171/13760 (30%)\nTrain - batches : 431, average loss: 1.8547, accuracy: 4184/13792 (30%)\nTrain - batches : 432, average loss: 1.8543, accuracy: 4195/13824 (30%)\nTrain - batches : 433, average loss: 1.8542, accuracy: 4205/13856 (30%)\nTrain - batches : 434, average loss: 1.8542, accuracy: 4216/13888 (30%)\nTrain - batches : 435, average loss: 1.8535, accuracy: 4226/13920 (30%)\nTrain - batches : 436, average loss: 1.8530, accuracy: 4238/13952 (30%)\nTrain - batches : 437, average loss: 1.8531, accuracy: 4248/13984 (30%)\nTrain - batches : 438, average loss: 1.8524, accuracy: 4262/14016 (30%)\nTrain - batches : 439, average loss: 1.8516, accuracy: 4277/14048 (30%)\nTrain - batches : 440, average loss: 1.8510, accuracy: 4287/14080 (30%)\nTrain - batches : 441, average loss: 1.8507, accuracy: 4297/14112 (30%)\nTrain - batches : 442, average loss: 1.8498, accuracy: 4313/14144 (30%)\nTrain - batches : 443, average loss: 1.8487, accuracy: 4324/14176 (31%)\nTrain - batches : 444, average loss: 1.8491, accuracy: 4328/14208 (30%)\nTrain - batches : 445, average loss: 1.8489, accuracy: 4336/14240 (30%)\nTrain - batches : 446, average loss: 1.8483, accuracy: 4349/14272 (30%)\nTrain - batches : 447, average loss: 1.8478, accuracy: 4358/14304 (30%)\nTrain - batches : 448, average loss: 1.8488, accuracy: 4369/14336 (30%)\nTrain - batches : 449, average loss: 1.8478, accuracy: 4383/14368 (31%)\nTrain - batches : 450, average loss: 1.8475, accuracy: 4392/14400 (30%)\nTrain - batches : 451, average loss: 1.8468, accuracy: 4405/14432 (31%)\nTrain - batches : 452, average loss: 1.8457, accuracy: 4418/14464 (31%)\nTrain - batches : 453, average loss: 1.8457, accuracy: 4423/14496 (31%)\nTrain - batches : 454, average loss: 1.8449, accuracy: 4438/14528 (31%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 455, average loss: 1.8440, accuracy: 4452/14560 (31%)\nTrain - batches : 456, average loss: 1.8437, accuracy: 4461/14592 (31%)\nTrain - batches : 457, average loss: 1.8435, accuracy: 4472/14624 (31%)\nTrain - batches : 458, average loss: 1.8429, accuracy: 4485/14656 (31%)\nTrain - batches : 459, average loss: 1.8429, accuracy: 4494/14688 (31%)\nTrain - batches : 460, average loss: 1.8418, accuracy: 4512/14720 (31%)\nTrain - batches : 461, average loss: 1.8416, accuracy: 4525/14752 (31%)\nTrain - batches : 462, average loss: 1.8419, accuracy: 4536/14784 (31%)\nTrain - batches : 463, average loss: 1.8421, accuracy: 4542/14816 (31%)\nTrain - batches : 464, average loss: 1.8415, accuracy: 4554/14848 (31%)\nTrain - batches : 465, average loss: 1.8406, accuracy: 4567/14880 (31%)\nTrain - batches : 466, average loss: 1.8403, accuracy: 4577/14912 (31%)\nTrain - batches : 467, average loss: 1.8404, accuracy: 4587/14944 (31%)\nTrain - batches : 468, average loss: 1.8398, accuracy: 4600/14976 (31%)\nTrain - batches : 469, average loss: 1.8401, accuracy: 4613/15008 (31%)\nTrain - batches : 470, average loss: 1.8397, accuracy: 4621/15040 (31%)\nTrain - batches : 471, average loss: 1.8396, accuracy: 4630/15072 (31%)\nTrain - batches : 472, average loss: 1.8390, accuracy: 4643/15104 (31%)\nTrain - batches : 473, average loss: 1.8390, accuracy: 4652/15136 (31%)\nTrain - batches : 474, average loss: 1.8385, accuracy: 4666/15168 (31%)\nTrain - batches : 475, average loss: 1.8377, accuracy: 4684/15200 (31%)\nTrain - batches : 476, average loss: 1.8373, accuracy: 4699/15232 (31%)\nTrain - batches : 477, average loss: 1.8370, accuracy: 4708/15264 (31%)\nTrain - batches : 478, average loss: 1.8368, accuracy: 4716/15296 (31%)\nTrain - batches : 479, average loss: 1.8368, accuracy: 4726/15328 (31%)\nTrain - batches : 480, average loss: 1.8369, accuracy: 4734/15360 (31%)\nTrain - batches : 481, average loss: 1.8369, accuracy: 4743/15392 (31%)\nTrain - batches : 482, average loss: 1.8364, accuracy: 4758/15424 (31%)\nTrain - batches : 483, average loss: 1.8358, accuracy: 4770/15456 (31%)\nTrain - batches : 484, average loss: 1.8352, accuracy: 4785/15488 (31%)\nTrain - batches : 485, average loss: 1.8346, accuracy: 4798/15520 (31%)\nTrain - batches : 486, average loss: 1.8345, accuracy: 4807/15552 (31%)\nTrain - batches : 487, average loss: 1.8343, accuracy: 4820/15584 (31%)\nTrain - batches : 488, average loss: 1.8340, accuracy: 4830/15616 (31%)\nTrain - batches : 489, average loss: 1.8332, accuracy: 4842/15648 (31%)\nTrain - batches : 490, average loss: 1.8331, accuracy: 4854/15680 (31%)\nTrain - batches : 491, average loss: 1.8327, accuracy: 4867/15712 (31%)\nTrain - batches : 492, average loss: 1.8324, accuracy: 4882/15744 (31%)\nTrain - batches : 493, average loss: 1.8319, accuracy: 4894/15776 (31%)\nTrain - batches : 494, average loss: 1.8313, accuracy: 4910/15808 (31%)\nTrain - batches : 495, average loss: 1.8307, accuracy: 4926/15840 (31%)\nTrain - batches : 496, average loss: 1.8299, accuracy: 4936/15872 (31%)\nTrain - batches : 497, average loss: 1.8299, accuracy: 4947/15904 (31%)\nTrain - batches : 498, average loss: 1.8298, accuracy: 4955/15936 (31%)\nTrain - batches : 499, average loss: 1.8294, accuracy: 4965/15968 (31%)\nTrain - batches : 500, average loss: 1.8289, accuracy: 4979/16000 (31%)\nTrain - batches : 501, average loss: 1.8283, accuracy: 4993/16032 (31%)\nTrain - batches : 502, average loss: 1.8281, accuracy: 5005/16064 (31%)\nTrain - batches : 503, average loss: 1.8274, accuracy: 5021/16096 (31%)\nTrain - batches : 504, average loss: 1.8267, accuracy: 5033/16128 (31%)\nTrain - batches : 505, average loss: 1.8264, accuracy: 5045/16160 (31%)\nTrain - batches : 506, average loss: 1.8264, accuracy: 5054/16192 (31%)\nTrain - batches : 507, average loss: 1.8254, accuracy: 5070/16224 (31%)\nTrain - batches : 508, average loss: 1.8252, accuracy: 5079/16256 (31%)\nTrain - batches : 509, average loss: 1.8253, accuracy: 5089/16288 (31%)\nTrain - batches : 510, average loss: 1.8252, accuracy: 5099/16320 (31%)\nTrain - batches : 511, average loss: 1.8245, accuracy: 5112/16352 (31%)\nTrain - batches : 512, average loss: 1.8244, accuracy: 5125/16384 (31%)\nTrain - batches : 513, average loss: 1.8242, accuracy: 5135/16416 (31%)\nTrain - batches : 514, average loss: 1.8244, accuracy: 5144/16448 (31%)\nTrain - batches : 515, average loss: 1.8237, accuracy: 5161/16480 (31%)\nTrain - batches : 516, average loss: 1.8234, accuracy: 5171/16512 (31%)\nTrain - batches : 517, average loss: 1.8233, accuracy: 5186/16544 (31%)\nTrain - batches : 518, average loss: 1.8229, accuracy: 5197/16576 (31%)\nTrain - batches : 519, average loss: 1.8221, accuracy: 5213/16608 (31%)\nTrain - batches : 520, average loss: 1.8219, accuracy: 5227/16640 (31%)\nTrain - batches : 521, average loss: 1.8214, accuracy: 5239/16672 (31%)\nTrain - batches : 522, average loss: 1.8207, accuracy: 5254/16704 (31%)\nTrain - batches : 523, average loss: 1.8208, accuracy: 5263/16736 (31%)\nTrain - batches : 524, average loss: 1.8213, accuracy: 5269/16768 (31%)\nTrain - batches : 525, average loss: 1.8210, accuracy: 5279/16800 (31%)\nTrain - batches : 526, average loss: 1.8209, accuracy: 5287/16832 (31%)\nTrain - batches : 527, average loss: 1.8203, accuracy: 5301/16864 (31%)\nTrain - batches : 528, average loss: 1.8199, accuracy: 5316/16896 (31%)\nTrain - batches : 529, average loss: 1.8192, accuracy: 5331/16928 (31%)\nTrain - batches : 530, average loss: 1.8192, accuracy: 5341/16960 (31%)\nTrain - batches : 531, average loss: 1.8187, accuracy: 5352/16992 (31%)\nTrain - batches : 532, average loss: 1.8183, accuracy: 5365/17024 (32%)\nTrain - batches : 533, average loss: 1.8183, accuracy: 5379/17056 (32%)\nTrain - batches : 534, average loss: 1.8179, accuracy: 5393/17088 (32%)\nTrain - batches : 535, average loss: 1.8173, accuracy: 5408/17120 (32%)\nTrain - batches : 536, average loss: 1.8171, accuracy: 5419/17152 (32%)\nTrain - batches : 537, average loss: 1.8162, accuracy: 5435/17184 (32%)\nTrain - batches : 538, average loss: 1.8156, accuracy: 5449/17216 (32%)\nTrain - batches : 539, average loss: 1.8152, accuracy: 5461/17248 (32%)\nTrain - batches : 540, average loss: 1.8148, accuracy: 5475/17280 (32%)\nTrain - batches : 541, average loss: 1.8140, accuracy: 5492/17312 (32%)\nTrain - batches : 542, average loss: 1.8136, accuracy: 5506/17344 (32%)\nTrain - batches : 543, average loss: 1.8135, accuracy: 5518/17376 (32%)\nTrain - batches : 544, average loss: 1.8131, accuracy: 5530/17408 (32%)\nTrain - batches : 545, average loss: 1.8122, accuracy: 5545/17440 (32%)\nTrain - batches : 546, average loss: 1.8116, accuracy: 5560/17472 (32%)\nTrain - batches : 547, average loss: 1.8116, accuracy: 5569/17504 (32%)\nTrain - batches : 548, average loss: 1.8111, accuracy: 5583/17536 (32%)\nTrain - batches : 549, average loss: 1.8110, accuracy: 5595/17568 (32%)\nTrain - batches : 550, average loss: 1.8105, accuracy: 5610/17600 (32%)\nTrain - batches : 551, average loss: 1.8101, accuracy: 5622/17632 (32%)\nTrain - batches : 552, average loss: 1.8094, accuracy: 5640/17664 (32%)\nTrain - batches : 553, average loss: 1.8091, accuracy: 5651/17696 (32%)\nTrain - batches : 554, average loss: 1.8080, accuracy: 5671/17728 (32%)\nTrain - batches : 555, average loss: 1.8075, accuracy: 5685/17760 (32%)\nTrain - batches : 556, average loss: 1.8076, accuracy: 5697/17792 (32%)\nTrain - batches : 557, average loss: 1.8074, accuracy: 5704/17824 (32%)\nTrain - batches : 558, average loss: 1.8069, accuracy: 5717/17856 (32%)\nTrain - batches : 559, average loss: 1.8066, accuracy: 5728/17888 (32%)\nTrain - batches : 560, average loss: 1.8066, accuracy: 5737/17920 (32%)\nTrain - batches : 561, average loss: 1.8064, accuracy: 5748/17952 (32%)\nTrain - batches : 562, average loss: 1.8061, accuracy: 5763/17984 (32%)\nTrain - batches : 563, average loss: 1.8058, accuracy: 5774/18016 (32%)\nTrain - batches : 564, average loss: 1.8056, accuracy: 5784/18048 (32%)\nTrain - batches : 565, average loss: 1.8054, accuracy: 5795/18080 (32%)\nTrain - batches : 566, average loss: 1.8047, accuracy: 5814/18112 (32%)\nTrain - batches : 567, average loss: 1.8043, accuracy: 5827/18144 (32%)\nTrain - batches : 568, average loss: 1.8036, accuracy: 5841/18176 (32%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 569, average loss: 1.8035, accuracy: 5851/18208 (32%)\nTrain - batches : 570, average loss: 1.8030, accuracy: 5862/18240 (32%)\nTrain - batches : 571, average loss: 1.8029, accuracy: 5874/18272 (32%)\nTrain - batches : 572, average loss: 1.8023, accuracy: 5888/18304 (32%)\nTrain - batches : 573, average loss: 1.8025, accuracy: 5898/18336 (32%)\nTrain - batches : 574, average loss: 1.8017, accuracy: 5913/18368 (32%)\nTrain - batches : 575, average loss: 1.8017, accuracy: 5925/18400 (32%)\nTrain - batches : 576, average loss: 1.8011, accuracy: 5942/18432 (32%)\nTrain - batches : 577, average loss: 1.8004, accuracy: 5957/18464 (32%)\nTrain - batches : 578, average loss: 1.8003, accuracy: 5966/18496 (32%)\nTrain - batches : 579, average loss: 1.7996, accuracy: 5981/18528 (32%)\nTrain - batches : 580, average loss: 1.7991, accuracy: 5996/18560 (32%)\nTrain - batches : 581, average loss: 1.7991, accuracy: 6009/18592 (32%)\nTrain - batches : 582, average loss: 1.7985, accuracy: 6023/18624 (32%)\nTrain - batches : 583, average loss: 1.7981, accuracy: 6039/18656 (32%)\nTrain - batches : 584, average loss: 1.7978, accuracy: 6048/18688 (32%)\nTrain - batches : 585, average loss: 1.7977, accuracy: 6060/18720 (32%)\nTrain - batches : 586, average loss: 1.7971, accuracy: 6075/18752 (32%)\nTrain - batches : 587, average loss: 1.7966, accuracy: 6091/18784 (32%)\nTrain - batches : 588, average loss: 1.7960, accuracy: 6107/18816 (32%)\nTrain - batches : 589, average loss: 1.7961, accuracy: 6119/18848 (32%)\nTrain - batches : 590, average loss: 1.7959, accuracy: 6130/18880 (32%)\nTrain - batches : 591, average loss: 1.7961, accuracy: 6139/18912 (32%)\nTrain - batches : 592, average loss: 1.7954, accuracy: 6154/18944 (32%)\nTrain - batches : 593, average loss: 1.7955, accuracy: 6163/18976 (32%)\nTrain - batches : 594, average loss: 1.7948, accuracy: 6178/19008 (33%)\nTrain - batches : 595, average loss: 1.7944, accuracy: 6191/19040 (33%)\nTrain - batches : 596, average loss: 1.7940, accuracy: 6204/19072 (33%)\nTrain - batches : 597, average loss: 1.7935, accuracy: 6218/19104 (33%)\nTrain - batches : 598, average loss: 1.7931, accuracy: 6228/19136 (33%)\nTrain - batches : 599, average loss: 1.7926, accuracy: 6243/19168 (33%)\nTrain - batches : 600, average loss: 1.7918, accuracy: 6260/19200 (33%)\nTrain - batches : 601, average loss: 1.7916, accuracy: 6271/19232 (33%)\nTrain - batches : 602, average loss: 1.7910, accuracy: 6290/19264 (33%)\nTrain - batches : 603, average loss: 1.7909, accuracy: 6299/19296 (33%)\nTrain - batches : 604, average loss: 1.7902, accuracy: 6314/19328 (33%)\nTrain - batches : 605, average loss: 1.7900, accuracy: 6327/19360 (33%)\nTrain - batches : 606, average loss: 1.7895, accuracy: 6341/19392 (33%)\nTrain - batches : 607, average loss: 1.7890, accuracy: 6357/19424 (33%)\nTrain - batches : 608, average loss: 1.7885, accuracy: 6373/19456 (33%)\nTrain - batches : 609, average loss: 1.7878, accuracy: 6389/19488 (33%)\nTrain - batches : 610, average loss: 1.7873, accuracy: 6401/19520 (33%)\nTrain - batches : 611, average loss: 1.7866, accuracy: 6416/19552 (33%)\nTrain - batches : 612, average loss: 1.7864, accuracy: 6427/19584 (33%)\nTrain - batches : 613, average loss: 1.7860, accuracy: 6444/19616 (33%)\nTrain - batches : 614, average loss: 1.7854, accuracy: 6461/19648 (33%)\nTrain - batches : 615, average loss: 1.7847, accuracy: 6476/19680 (33%)\nTrain - batches : 616, average loss: 1.7844, accuracy: 6490/19712 (33%)\nTrain - batches : 617, average loss: 1.7839, accuracy: 6506/19744 (33%)\nTrain - batches : 618, average loss: 1.7839, accuracy: 6516/19776 (33%)\nTrain - batches : 619, average loss: 1.7836, accuracy: 6531/19808 (33%)\nTrain - batches : 620, average loss: 1.7833, accuracy: 6548/19840 (33%)\nTrain - batches : 621, average loss: 1.7826, accuracy: 6563/19872 (33%)\nTrain - batches : 622, average loss: 1.7822, accuracy: 6576/19904 (33%)\nTrain - batches : 623, average loss: 1.7816, accuracy: 6593/19936 (33%)\nTrain - batches : 624, average loss: 1.7819, accuracy: 6599/19968 (33%)\nTrain - batches : 625, average loss: 1.7817, accuracy: 6614/20000 (33%)\nTrain - batches : 626, average loss: 1.7811, accuracy: 6628/20032 (33%)\nTrain - batches : 627, average loss: 1.7808, accuracy: 6638/20064 (33%)\nTrain - batches : 628, average loss: 1.7806, accuracy: 6653/20096 (33%)\nTrain - batches : 629, average loss: 1.7806, accuracy: 6665/20128 (33%)\nTrain - batches : 630, average loss: 1.7806, accuracy: 6676/20160 (33%)\nTrain - batches : 631, average loss: 1.7799, accuracy: 6694/20192 (33%)\nTrain - batches : 632, average loss: 1.7794, accuracy: 6704/20224 (33%)\nTrain - batches : 633, average loss: 1.7785, accuracy: 6722/20256 (33%)\nTrain - batches : 634, average loss: 1.7785, accuracy: 6735/20288 (33%)\nTrain - batches : 635, average loss: 1.7779, accuracy: 6750/20320 (33%)\nTrain - batches : 636, average loss: 1.7781, accuracy: 6758/20352 (33%)\nTrain - batches : 637, average loss: 1.7777, accuracy: 6768/20384 (33%)\nTrain - batches : 638, average loss: 1.7771, accuracy: 6785/20416 (33%)\nTrain - batches : 639, average loss: 1.7771, accuracy: 6798/20448 (33%)\nTrain - batches : 640, average loss: 1.7765, accuracy: 6809/20480 (33%)\nTrain - batches : 641, average loss: 1.7765, accuracy: 6820/20512 (33%)\nTrain - batches : 642, average loss: 1.7758, accuracy: 6837/20544 (33%)\nTrain - batches : 643, average loss: 1.7756, accuracy: 6847/20576 (33%)\nTrain - batches : 644, average loss: 1.7754, accuracy: 6859/20608 (33%)\nTrain - batches : 645, average loss: 1.7751, accuracy: 6870/20640 (33%)\nTrain - batches : 646, average loss: 1.7749, accuracy: 6883/20672 (33%)\nTrain - batches : 647, average loss: 1.7743, accuracy: 6900/20704 (33%)\nTrain - batches : 648, average loss: 1.7742, accuracy: 6909/20736 (33%)\nTrain - batches : 649, average loss: 1.7736, accuracy: 6923/20768 (33%)\nTrain - batches : 650, average loss: 1.7732, accuracy: 6934/20800 (33%)\nTrain - batches : 651, average loss: 1.7727, accuracy: 6948/20832 (33%)\nTrain - batches : 652, average loss: 1.7722, accuracy: 6965/20864 (33%)\nTrain - batches : 653, average loss: 1.7723, accuracy: 6975/20896 (33%)\nTrain - batches : 654, average loss: 1.7718, accuracy: 6990/20928 (33%)\nTrain - batches : 655, average loss: 1.7713, accuracy: 7005/20960 (33%)\nTrain - batches : 656, average loss: 1.7706, accuracy: 7021/20992 (33%)\nTrain - batches : 657, average loss: 1.7705, accuracy: 7034/21024 (33%)\nTrain - batches : 658, average loss: 1.7699, accuracy: 7048/21056 (33%)\nTrain - batches : 659, average loss: 1.7696, accuracy: 7059/21088 (33%)\nTrain - batches : 660, average loss: 1.7689, accuracy: 7073/21120 (33%)\nTrain - batches : 661, average loss: 1.7684, accuracy: 7090/21152 (34%)\nTrain - batches : 662, average loss: 1.7678, accuracy: 7105/21184 (34%)\nTrain - batches : 663, average loss: 1.7676, accuracy: 7118/21216 (34%)\nTrain - batches : 664, average loss: 1.7675, accuracy: 7129/21248 (34%)\nTrain - batches : 665, average loss: 1.7670, accuracy: 7144/21280 (34%)\nTrain - batches : 666, average loss: 1.7666, accuracy: 7157/21312 (34%)\nTrain - batches : 667, average loss: 1.7661, accuracy: 7173/21344 (34%)\nTrain - batches : 668, average loss: 1.7664, accuracy: 7183/21376 (34%)\nTrain - batches : 669, average loss: 1.7662, accuracy: 7199/21408 (34%)\nTrain - batches : 670, average loss: 1.7658, accuracy: 7211/21440 (34%)\nTrain - batches : 671, average loss: 1.7653, accuracy: 7226/21472 (34%)\nTrain - batches : 672, average loss: 1.7647, accuracy: 7240/21504 (34%)\nTrain - batches : 673, average loss: 1.7640, accuracy: 7260/21536 (34%)\nTrain - batches : 674, average loss: 1.7634, accuracy: 7276/21568 (34%)\nTrain - batches : 675, average loss: 1.7634, accuracy: 7288/21600 (34%)\nTrain - batches : 676, average loss: 1.7631, accuracy: 7297/21632 (34%)\nTrain - batches : 677, average loss: 1.7627, accuracy: 7310/21664 (34%)\nTrain - batches : 678, average loss: 1.7624, accuracy: 7322/21696 (34%)\nTrain - batches : 679, average loss: 1.7624, accuracy: 7333/21728 (34%)\nTrain - batches : 680, average loss: 1.7618, accuracy: 7347/21760 (34%)\nTrain - batches : 681, average loss: 1.7616, accuracy: 7361/21792 (34%)\nTrain - batches : 682, average loss: 1.7612, accuracy: 7376/21824 (34%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 683, average loss: 1.7609, accuracy: 7390/21856 (34%)\nTrain - batches : 684, average loss: 1.7605, accuracy: 7401/21888 (34%)\nTrain - batches : 685, average loss: 1.7607, accuracy: 7412/21920 (34%)\nTrain - batches : 686, average loss: 1.7602, accuracy: 7429/21952 (34%)\nTrain - batches : 687, average loss: 1.7599, accuracy: 7441/21984 (34%)\nTrain - batches : 688, average loss: 1.7595, accuracy: 7457/22016 (34%)\nTrain - batches : 689, average loss: 1.7592, accuracy: 7468/22048 (34%)\nTrain - batches : 690, average loss: 1.7591, accuracy: 7480/22080 (34%)\nTrain - batches : 691, average loss: 1.7593, accuracy: 7490/22112 (34%)\nTrain - batches : 692, average loss: 1.7584, accuracy: 7506/22144 (34%)\nTrain - batches : 693, average loss: 1.7579, accuracy: 7522/22176 (34%)\nTrain - batches : 694, average loss: 1.7574, accuracy: 7537/22208 (34%)\nTrain - batches : 695, average loss: 1.7569, accuracy: 7554/22240 (34%)\nTrain - batches : 696, average loss: 1.7571, accuracy: 7565/22272 (34%)\nTrain - batches : 697, average loss: 1.7569, accuracy: 7573/22304 (34%)\nTrain - batches : 698, average loss: 1.7563, accuracy: 7593/22336 (34%)\nTrain - batches : 699, average loss: 1.7558, accuracy: 7611/22368 (34%)\nTrain - batches : 700, average loss: 1.7553, accuracy: 7629/22400 (34%)\nTrain - batches : 701, average loss: 1.7548, accuracy: 7642/22432 (34%)\nTrain - batches : 702, average loss: 1.7545, accuracy: 7654/22464 (34%)\nTrain - batches : 703, average loss: 1.7541, accuracy: 7667/22496 (34%)\nTrain - batches : 704, average loss: 1.7545, accuracy: 7676/22528 (34%)\nTrain - batches : 705, average loss: 1.7544, accuracy: 7685/22560 (34%)\nTrain - batches : 706, average loss: 1.7543, accuracy: 7697/22592 (34%)\nTrain - batches : 707, average loss: 1.7542, accuracy: 7709/22624 (34%)\nTrain - batches : 708, average loss: 1.7538, accuracy: 7725/22656 (34%)\nTrain - batches : 709, average loss: 1.7537, accuracy: 7741/22688 (34%)\nTrain - batches : 710, average loss: 1.7533, accuracy: 7755/22720 (34%)\nTrain - batches : 711, average loss: 1.7532, accuracy: 7767/22752 (34%)\nTrain - batches : 712, average loss: 1.7528, accuracy: 7781/22784 (34%)\nTrain - batches : 713, average loss: 1.7527, accuracy: 7793/22816 (34%)\nTrain - batches : 714, average loss: 1.7527, accuracy: 7802/22848 (34%)\nTrain - batches : 715, average loss: 1.7527, accuracy: 7810/22880 (34%)\nTrain - batches : 716, average loss: 1.7523, accuracy: 7824/22912 (34%)\nTrain - batches : 717, average loss: 1.7517, accuracy: 7844/22944 (34%)\nTrain - batches : 718, average loss: 1.7514, accuracy: 7859/22976 (34%)\nTrain - batches : 719, average loss: 1.7510, accuracy: 7873/23008 (34%)\nTrain - batches : 720, average loss: 1.7505, accuracy: 7891/23040 (34%)\nTrain - batches : 721, average loss: 1.7502, accuracy: 7904/23072 (34%)\nTrain - batches : 722, average loss: 1.7499, accuracy: 7918/23104 (34%)\nTrain - batches : 723, average loss: 1.7498, accuracy: 7928/23136 (34%)\nTrain - batches : 724, average loss: 1.7496, accuracy: 7940/23168 (34%)\nTrain - batches : 725, average loss: 1.7497, accuracy: 7950/23200 (34%)\nTrain - batches : 726, average loss: 1.7495, accuracy: 7962/23232 (34%)\nTrain - batches : 727, average loss: 1.7487, accuracy: 7979/23264 (34%)\nTrain - batches : 728, average loss: 1.7487, accuracy: 7989/23296 (34%)\nTrain - batches : 729, average loss: 1.7487, accuracy: 8000/23328 (34%)\nTrain - batches : 730, average loss: 1.7485, accuracy: 8009/23360 (34%)\nTrain - batches : 731, average loss: 1.7484, accuracy: 8023/23392 (34%)\nTrain - batches : 732, average loss: 1.7485, accuracy: 8035/23424 (34%)\nTrain - batches : 733, average loss: 1.7480, accuracy: 8051/23456 (34%)\nTrain - batches : 734, average loss: 1.7474, accuracy: 8067/23488 (34%)\nTrain - batches : 735, average loss: 1.7474, accuracy: 8082/23520 (34%)\nTrain - batches : 736, average loss: 1.7471, accuracy: 8095/23552 (34%)\nTrain - batches : 737, average loss: 1.7471, accuracy: 8106/23584 (34%)\nTrain - batches : 738, average loss: 1.7469, accuracy: 8122/23616 (34%)\nTrain - batches : 739, average loss: 1.7468, accuracy: 8131/23648 (34%)\nTrain - batches : 740, average loss: 1.7465, accuracy: 8148/23680 (34%)\nTrain - batches : 741, average loss: 1.7464, accuracy: 8159/23712 (34%)\nTrain - batches : 742, average loss: 1.7461, accuracy: 8174/23744 (34%)\nTrain - batches : 743, average loss: 1.7459, accuracy: 8187/23776 (34%)\nTrain - batches : 744, average loss: 1.7457, accuracy: 8204/23808 (34%)\nTrain - batches : 745, average loss: 1.7456, accuracy: 8218/23840 (34%)\nTrain - batches : 746, average loss: 1.7450, accuracy: 8233/23872 (34%)\nTrain - batches : 747, average loss: 1.7449, accuracy: 8243/23904 (34%)\nTrain - batches : 748, average loss: 1.7445, accuracy: 8260/23936 (35%)\nTrain - batches : 749, average loss: 1.7438, accuracy: 8278/23968 (35%)\nTrain - batches : 750, average loss: 1.7436, accuracy: 8287/24000 (35%)\nTrain - batches : 751, average loss: 1.7432, accuracy: 8303/24032 (35%)\nTrain - batches : 752, average loss: 1.7429, accuracy: 8317/24064 (35%)\nTrain - batches : 753, average loss: 1.7425, accuracy: 8331/24096 (35%)\nTrain - batches : 754, average loss: 1.7423, accuracy: 8342/24128 (35%)\nTrain - batches : 755, average loss: 1.7421, accuracy: 8356/24160 (35%)\nTrain - batches : 756, average loss: 1.7416, accuracy: 8372/24192 (35%)\nTrain - batches : 757, average loss: 1.7411, accuracy: 8386/24224 (35%)\nTrain - batches : 758, average loss: 1.7407, accuracy: 8400/24256 (35%)\nTrain - batches : 759, average loss: 1.7404, accuracy: 8411/24288 (35%)\nTrain - batches : 760, average loss: 1.7399, accuracy: 8426/24320 (35%)\nTrain - batches : 761, average loss: 1.7400, accuracy: 8434/24352 (35%)\nTrain - batches : 762, average loss: 1.7395, accuracy: 8450/24384 (35%)\nTrain - batches : 763, average loss: 1.7390, accuracy: 8470/24416 (35%)\nTrain - batches : 764, average loss: 1.7385, accuracy: 8487/24448 (35%)\nTrain - batches : 765, average loss: 1.7382, accuracy: 8501/24480 (35%)\nTrain - batches : 766, average loss: 1.7377, accuracy: 8515/24512 (35%)\nTrain - batches : 767, average loss: 1.7372, accuracy: 8530/24544 (35%)\nTrain - batches : 768, average loss: 1.7369, accuracy: 8548/24576 (35%)\nTrain - batches : 769, average loss: 1.7370, accuracy: 8559/24608 (35%)\nTrain - batches : 770, average loss: 1.7366, accuracy: 8574/24640 (35%)\nTrain - batches : 771, average loss: 1.7364, accuracy: 8586/24672 (35%)\nTrain - batches : 772, average loss: 1.7359, accuracy: 8599/24704 (35%)\nTrain - batches : 773, average loss: 1.7357, accuracy: 8615/24736 (35%)\nTrain - batches : 774, average loss: 1.7357, accuracy: 8628/24768 (35%)\nTrain - batches : 775, average loss: 1.7354, accuracy: 8643/24800 (35%)\nTrain - batches : 776, average loss: 1.7351, accuracy: 8656/24832 (35%)\nTrain - batches : 777, average loss: 1.7351, accuracy: 8667/24864 (35%)\nTrain - batches : 778, average loss: 1.7347, accuracy: 8681/24896 (35%)\nTrain - batches : 779, average loss: 1.7345, accuracy: 8692/24928 (35%)\nTrain - batches : 780, average loss: 1.7344, accuracy: 8704/24960 (35%)\nTrain - batches : 781, average loss: 1.7341, accuracy: 8718/24992 (35%)\nTrain - batches : 782, average loss: 1.7343, accuracy: 8730/25024 (35%)\nTrain - batches : 783, average loss: 1.7338, accuracy: 8744/25056 (35%)\nTrain - batches : 784, average loss: 1.7337, accuracy: 8759/25088 (35%)\nTrain - batches : 785, average loss: 1.7335, accuracy: 8774/25120 (35%)\nTrain - batches : 786, average loss: 1.7335, accuracy: 8787/25152 (35%)\nTrain - batches : 787, average loss: 1.7330, accuracy: 8801/25184 (35%)\nTrain - batches : 788, average loss: 1.7327, accuracy: 8814/25216 (35%)\nTrain - batches : 789, average loss: 1.7325, accuracy: 8824/25248 (35%)\nTrain - batches : 790, average loss: 1.7319, accuracy: 8839/25280 (35%)\nTrain - batches : 791, average loss: 1.7320, accuracy: 8852/25312 (35%)\nTrain - batches : 792, average loss: 1.7320, accuracy: 8864/25344 (35%)\nTrain - batches : 793, average loss: 1.7316, accuracy: 8875/25376 (35%)\nTrain - batches : 794, average loss: 1.7313, accuracy: 8886/25408 (35%)\nTrain - batches : 795, average loss: 1.7308, accuracy: 8904/25440 (35%)\nTrain - batches : 796, average loss: 1.7311, accuracy: 8914/25472 (35%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 797, average loss: 1.7314, accuracy: 8924/25504 (35%)\nTrain - batches : 798, average loss: 1.7312, accuracy: 8936/25536 (35%)\nTrain - batches : 799, average loss: 1.7313, accuracy: 8947/25568 (35%)\nTrain - batches : 800, average loss: 1.7310, accuracy: 8962/25600 (35%)\nTrain - batches : 801, average loss: 1.7308, accuracy: 8978/25632 (35%)\nTrain - batches : 802, average loss: 1.7306, accuracy: 8993/25664 (35%)\nTrain - batches : 803, average loss: 1.7305, accuracy: 9006/25696 (35%)\nTrain - batches : 804, average loss: 1.7305, accuracy: 9016/25728 (35%)\nTrain - batches : 805, average loss: 1.7301, accuracy: 9030/25760 (35%)\nTrain - batches : 806, average loss: 1.7298, accuracy: 9046/25792 (35%)\nTrain - batches : 807, average loss: 1.7296, accuracy: 9056/25824 (35%)\nTrain - batches : 808, average loss: 1.7292, accuracy: 9073/25856 (35%)\nTrain - batches : 809, average loss: 1.7291, accuracy: 9086/25888 (35%)\nTrain - batches : 810, average loss: 1.7291, accuracy: 9099/25920 (35%)\nTrain - batches : 811, average loss: 1.7289, accuracy: 9112/25952 (35%)\nTrain - batches : 812, average loss: 1.7284, accuracy: 9126/25984 (35%)\nTrain - batches : 813, average loss: 1.7283, accuracy: 9137/26016 (35%)\nTrain - batches : 814, average loss: 1.7279, accuracy: 9151/26048 (35%)\nTrain - batches : 815, average loss: 1.7281, accuracy: 9162/26080 (35%)\nTrain - batches : 816, average loss: 1.7279, accuracy: 9176/26112 (35%)\nTrain - batches : 817, average loss: 1.7271, accuracy: 9197/26144 (35%)\nTrain - batches : 818, average loss: 1.7271, accuracy: 9207/26176 (35%)\nTrain - batches : 819, average loss: 1.7268, accuracy: 9225/26208 (35%)\nTrain - batches : 820, average loss: 1.7264, accuracy: 9244/26240 (35%)\nTrain - batches : 821, average loss: 1.7263, accuracy: 9253/26272 (35%)\nTrain - batches : 822, average loss: 1.7263, accuracy: 9264/26304 (35%)\nTrain - batches : 823, average loss: 1.7260, accuracy: 9279/26336 (35%)\nTrain - batches : 824, average loss: 1.7257, accuracy: 9296/26368 (35%)\nTrain - batches : 825, average loss: 1.7253, accuracy: 9311/26400 (35%)\nTrain - batches : 826, average loss: 1.7251, accuracy: 9328/26432 (35%)\nTrain - batches : 827, average loss: 1.7247, accuracy: 9342/26464 (35%)\nTrain - batches : 828, average loss: 1.7246, accuracy: 9353/26496 (35%)\nTrain - batches : 829, average loss: 1.7243, accuracy: 9366/26528 (35%)\nTrain - batches : 830, average loss: 1.7240, accuracy: 9381/26560 (35%)\nTrain - batches : 831, average loss: 1.7237, accuracy: 9392/26592 (35%)\nTrain - batches : 832, average loss: 1.7233, accuracy: 9411/26624 (35%)\nTrain - batches : 833, average loss: 1.7231, accuracy: 9422/26656 (35%)\nTrain - batches : 834, average loss: 1.7228, accuracy: 9438/26688 (35%)\nTrain - batches : 835, average loss: 1.7223, accuracy: 9454/26720 (35%)\nTrain - batches : 836, average loss: 1.7221, accuracy: 9470/26752 (35%)\nTrain - batches : 837, average loss: 1.7218, accuracy: 9486/26784 (35%)\nTrain - batches : 838, average loss: 1.7215, accuracy: 9501/26816 (35%)\nTrain - batches : 839, average loss: 1.7214, accuracy: 9515/26848 (35%)\nTrain - batches : 840, average loss: 1.7212, accuracy: 9530/26880 (35%)\nTrain - batches : 841, average loss: 1.7208, accuracy: 9544/26912 (35%)\nTrain - batches : 842, average loss: 1.7208, accuracy: 9557/26944 (35%)\nTrain - batches : 843, average loss: 1.7205, accuracy: 9567/26976 (35%)\nTrain - batches : 844, average loss: 1.7202, accuracy: 9583/27008 (35%)\nTrain - batches : 845, average loss: 1.7200, accuracy: 9599/27040 (35%)\nTrain - batches : 846, average loss: 1.7196, accuracy: 9613/27072 (36%)\nTrain - batches : 847, average loss: 1.7192, accuracy: 9623/27104 (36%)\nTrain - batches : 848, average loss: 1.7189, accuracy: 9635/27136 (36%)\nTrain - batches : 849, average loss: 1.7184, accuracy: 9653/27168 (36%)\nTrain - batches : 850, average loss: 1.7179, accuracy: 9669/27200 (36%)\nTrain - batches : 851, average loss: 1.7177, accuracy: 9682/27232 (36%)\nTrain - batches : 852, average loss: 1.7174, accuracy: 9696/27264 (36%)\nTrain - batches : 853, average loss: 1.7171, accuracy: 9710/27296 (36%)\nTrain - batches : 854, average loss: 1.7169, accuracy: 9722/27328 (36%)\nTrain - batches : 855, average loss: 1.7170, accuracy: 9739/27360 (36%)\nTrain - batches : 856, average loss: 1.7167, accuracy: 9749/27392 (36%)\nTrain - batches : 857, average loss: 1.7166, accuracy: 9761/27424 (36%)\nTrain - batches : 858, average loss: 1.7165, accuracy: 9774/27456 (36%)\nTrain - batches : 859, average loss: 1.7159, accuracy: 9793/27488 (36%)\nTrain - batches : 860, average loss: 1.7158, accuracy: 9809/27520 (36%)\nTrain - batches : 861, average loss: 1.7155, accuracy: 9824/27552 (36%)\nTrain - batches : 862, average loss: 1.7150, accuracy: 9839/27584 (36%)\nTrain - batches : 863, average loss: 1.7147, accuracy: 9853/27616 (36%)\nTrain - batches : 864, average loss: 1.7147, accuracy: 9862/27648 (36%)\nTrain - batches : 865, average loss: 1.7146, accuracy: 9871/27680 (36%)\nTrain - batches : 866, average loss: 1.7143, accuracy: 9888/27712 (36%)\nTrain - batches : 867, average loss: 1.7138, accuracy: 9906/27744 (36%)\nTrain - batches : 868, average loss: 1.7137, accuracy: 9921/27776 (36%)\nTrain - batches : 869, average loss: 1.7133, accuracy: 9932/27808 (36%)\nTrain - batches : 870, average loss: 1.7133, accuracy: 9942/27840 (36%)\nTrain - batches : 871, average loss: 1.7133, accuracy: 9951/27872 (36%)\nTrain - batches : 872, average loss: 1.7130, accuracy: 9966/27904 (36%)\nTrain - batches : 873, average loss: 1.7126, accuracy: 9980/27936 (36%)\nTrain - batches : 874, average loss: 1.7123, accuracy: 9995/27968 (36%)\nTrain - batches : 875, average loss: 1.7120, accuracy: 10011/28000 (36%)\nTrain - batches : 876, average loss: 1.7117, accuracy: 10025/28032 (36%)\nTrain - batches : 877, average loss: 1.7119, accuracy: 10037/28064 (36%)\nTrain - batches : 878, average loss: 1.7116, accuracy: 10050/28096 (36%)\nTrain - batches : 879, average loss: 1.7111, accuracy: 10066/28128 (36%)\nTrain - batches : 880, average loss: 1.7111, accuracy: 10079/28160 (36%)\nTrain - batches : 881, average loss: 1.7108, accuracy: 10095/28192 (36%)\nTrain - batches : 882, average loss: 1.7103, accuracy: 10112/28224 (36%)\nTrain - batches : 883, average loss: 1.7102, accuracy: 10122/28256 (36%)\nTrain - batches : 884, average loss: 1.7099, accuracy: 10136/28288 (36%)\nTrain - batches : 885, average loss: 1.7096, accuracy: 10151/28320 (36%)\nTrain - batches : 886, average loss: 1.7095, accuracy: 10164/28352 (36%)\nTrain - batches : 887, average loss: 1.7093, accuracy: 10177/28384 (36%)\nTrain - batches : 888, average loss: 1.7091, accuracy: 10188/28416 (36%)\nTrain - batches : 889, average loss: 1.7087, accuracy: 10201/28448 (36%)\nTrain - batches : 890, average loss: 1.7084, accuracy: 10217/28480 (36%)\nTrain - batches : 891, average loss: 1.7079, accuracy: 10235/28512 (36%)\nTrain - batches : 892, average loss: 1.7078, accuracy: 10247/28544 (36%)\nTrain - batches : 893, average loss: 1.7078, accuracy: 10259/28576 (36%)\nTrain - batches : 894, average loss: 1.7080, accuracy: 10267/28608 (36%)\nTrain - batches : 895, average loss: 1.7078, accuracy: 10280/28640 (36%)\nTrain - batches : 896, average loss: 1.7075, accuracy: 10299/28672 (36%)\nTrain - batches : 897, average loss: 1.7071, accuracy: 10322/28704 (36%)\nTrain - batches : 898, average loss: 1.7069, accuracy: 10335/28736 (36%)\nTrain - batches : 899, average loss: 1.7066, accuracy: 10352/28768 (36%)\nTrain - batches : 900, average loss: 1.7062, accuracy: 10370/28800 (36%)\nTrain - batches : 901, average loss: 1.7060, accuracy: 10386/28832 (36%)\nTrain - batches : 902, average loss: 1.7056, accuracy: 10403/28864 (36%)\nTrain - batches : 903, average loss: 1.7056, accuracy: 10417/28896 (36%)\nTrain - batches : 904, average loss: 1.7055, accuracy: 10431/28928 (36%)\nTrain - batches : 905, average loss: 1.7057, accuracy: 10441/28960 (36%)\nTrain - batches : 906, average loss: 1.7058, accuracy: 10453/28992 (36%)\nTrain - batches : 907, average loss: 1.7057, accuracy: 10466/29024 (36%)\nTrain - batches : 908, average loss: 1.7055, accuracy: 10482/29056 (36%)\nTrain - batches : 909, average loss: 1.7054, accuracy: 10497/29088 (36%)\nTrain - batches : 910, average loss: 1.7053, accuracy: 10511/29120 (36%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 911, average loss: 1.7051, accuracy: 10524/29152 (36%)\nTrain - batches : 912, average loss: 1.7046, accuracy: 10542/29184 (36%)\nTrain - batches : 913, average loss: 1.7047, accuracy: 10551/29216 (36%)\nTrain - batches : 914, average loss: 1.7045, accuracy: 10565/29248 (36%)\nTrain - batches : 915, average loss: 1.7044, accuracy: 10574/29280 (36%)\nTrain - batches : 916, average loss: 1.7041, accuracy: 10592/29312 (36%)\nTrain - batches : 917, average loss: 1.7038, accuracy: 10609/29344 (36%)\nTrain - batches : 918, average loss: 1.7038, accuracy: 10625/29376 (36%)\nTrain - batches : 919, average loss: 1.7035, accuracy: 10641/29408 (36%)\nTrain - batches : 920, average loss: 1.7035, accuracy: 10652/29440 (36%)\nTrain - batches : 921, average loss: 1.7033, accuracy: 10668/29472 (36%)\nTrain - batches : 922, average loss: 1.7030, accuracy: 10683/29504 (36%)\nTrain - batches : 923, average loss: 1.7023, accuracy: 10702/29536 (36%)\nTrain - batches : 924, average loss: 1.7022, accuracy: 10714/29568 (36%)\nTrain - batches : 925, average loss: 1.7020, accuracy: 10727/29600 (36%)\nTrain - batches : 926, average loss: 1.7018, accuracy: 10742/29632 (36%)\nTrain - batches : 927, average loss: 1.7016, accuracy: 10757/29664 (36%)\nTrain - batches : 928, average loss: 1.7012, accuracy: 10775/29696 (36%)\nTrain - batches : 929, average loss: 1.7010, accuracy: 10793/29728 (36%)\nTrain - batches : 930, average loss: 1.7007, accuracy: 10808/29760 (36%)\nTrain - batches : 931, average loss: 1.7006, accuracy: 10820/29792 (36%)\nTrain - batches : 932, average loss: 1.7005, accuracy: 10830/29824 (36%)\nTrain - batches : 933, average loss: 1.7003, accuracy: 10842/29856 (36%)\nTrain - batches : 934, average loss: 1.7002, accuracy: 10854/29888 (36%)\nTrain - batches : 935, average loss: 1.7001, accuracy: 10864/29920 (36%)\nTrain - batches : 936, average loss: 1.7002, accuracy: 10879/29952 (36%)\nTrain - batches : 937, average loss: 1.7000, accuracy: 10895/29984 (36%)\nTrain - batches : 938, average loss: 1.6997, accuracy: 10911/30016 (36%)\nTrain - batches : 939, average loss: 1.6997, accuracy: 10922/30048 (36%)\nTrain - batches : 940, average loss: 1.7000, accuracy: 10939/30080 (36%)\nTrain - batches : 941, average loss: 1.6997, accuracy: 10950/30112 (36%)\nTrain - batches : 942, average loss: 1.6996, accuracy: 10962/30144 (36%)\nTrain - batches : 943, average loss: 1.6992, accuracy: 10975/30176 (36%)\nTrain - batches : 944, average loss: 1.6992, accuracy: 10987/30208 (36%)\nTrain - batches : 945, average loss: 1.6993, accuracy: 10998/30240 (36%)\nTrain - batches : 946, average loss: 1.6992, accuracy: 11008/30272 (36%)\nTrain - batches : 947, average loss: 1.6987, accuracy: 11026/30304 (36%)\nTrain - batches : 948, average loss: 1.6985, accuracy: 11042/30336 (36%)\nTrain - batches : 949, average loss: 1.6985, accuracy: 11052/30368 (36%)\nTrain - batches : 950, average loss: 1.6981, accuracy: 11069/30400 (36%)\nTrain - batches : 951, average loss: 1.6978, accuracy: 11082/30432 (36%)\nTrain - batches : 952, average loss: 1.6977, accuracy: 11095/30464 (36%)\nTrain - batches : 953, average loss: 1.6977, accuracy: 11106/30496 (36%)\nTrain - batches : 954, average loss: 1.6973, accuracy: 11120/30528 (36%)\nTrain - batches : 955, average loss: 1.6971, accuracy: 11135/30560 (36%)\nTrain - batches : 956, average loss: 1.6969, accuracy: 11148/30592 (36%)\nTrain - batches : 957, average loss: 1.6964, accuracy: 11170/30624 (36%)\nTrain - batches : 958, average loss: 1.6963, accuracy: 11182/30656 (36%)\nTrain - batches : 959, average loss: 1.6959, accuracy: 11201/30688 (36%)\nTrain - batches : 960, average loss: 1.6958, accuracy: 11212/30720 (36%)\nTrain - batches : 961, average loss: 1.6952, accuracy: 11231/30752 (37%)\nTrain - batches : 962, average loss: 1.6949, accuracy: 11246/30784 (37%)\nTrain - batches : 963, average loss: 1.6948, accuracy: 11261/30816 (37%)\nTrain - batches : 964, average loss: 1.6943, accuracy: 11280/30848 (37%)\nTrain - batches : 965, average loss: 1.6942, accuracy: 11289/30880 (37%)\nTrain - batches : 966, average loss: 1.6941, accuracy: 11303/30912 (37%)\nTrain - batches : 967, average loss: 1.6937, accuracy: 11316/30944 (37%)\nTrain - batches : 968, average loss: 1.6935, accuracy: 11330/30976 (37%)\nTrain - batches : 969, average loss: 1.6932, accuracy: 11346/31008 (37%)\nTrain - batches : 970, average loss: 1.6928, accuracy: 11365/31040 (37%)\nTrain - batches : 971, average loss: 1.6925, accuracy: 11382/31072 (37%)\nTrain - batches : 972, average loss: 1.6922, accuracy: 11398/31104 (37%)\nTrain - batches : 973, average loss: 1.6917, accuracy: 11416/31136 (37%)\nTrain - batches : 974, average loss: 1.6915, accuracy: 11430/31168 (37%)\nTrain - batches : 975, average loss: 1.6910, accuracy: 11448/31200 (37%)\nTrain - batches : 976, average loss: 1.6911, accuracy: 11458/31232 (37%)\nTrain - batches : 977, average loss: 1.6906, accuracy: 11473/31264 (37%)\nTrain - batches : 978, average loss: 1.6906, accuracy: 11487/31296 (37%)\nTrain - batches : 979, average loss: 1.6903, accuracy: 11502/31328 (37%)\nTrain - batches : 980, average loss: 1.6901, accuracy: 11517/31360 (37%)\nTrain - batches : 981, average loss: 1.6899, accuracy: 11529/31392 (37%)\nTrain - batches : 982, average loss: 1.6895, accuracy: 11545/31424 (37%)\nTrain - batches : 983, average loss: 1.6891, accuracy: 11563/31456 (37%)\nTrain - batches : 984, average loss: 1.6890, accuracy: 11579/31488 (37%)\nTrain - batches : 985, average loss: 1.6888, accuracy: 11596/31520 (37%)\nTrain - batches : 986, average loss: 1.6883, accuracy: 11614/31552 (37%)\nTrain - batches : 987, average loss: 1.6879, accuracy: 11632/31584 (37%)\nTrain - batches : 988, average loss: 1.6876, accuracy: 11646/31616 (37%)\nTrain - batches : 989, average loss: 1.6875, accuracy: 11658/31648 (37%)\nTrain - batches : 990, average loss: 1.6873, accuracy: 11672/31680 (37%)\nTrain - batches : 991, average loss: 1.6870, accuracy: 11688/31712 (37%)\nTrain - batches : 992, average loss: 1.6866, accuracy: 11707/31744 (37%)\nTrain - batches : 993, average loss: 1.6863, accuracy: 11721/31776 (37%)\nTrain - batches : 994, average loss: 1.6864, accuracy: 11735/31808 (37%)\nTrain - batches : 995, average loss: 1.6858, accuracy: 11753/31840 (37%)\nTrain - batches : 996, average loss: 1.6861, accuracy: 11766/31872 (37%)\nTrain - batches : 997, average loss: 1.6863, accuracy: 11773/31904 (37%)\nTrain - batches : 998, average loss: 1.6861, accuracy: 11788/31936 (37%)\nTrain - batches : 999, average loss: 1.6858, accuracy: 11802/31968 (37%)\nTrain - batches : 1000, average loss: 1.6858, accuracy: 11817/32000 (37%)\nTrain - batches : 1001, average loss: 1.6855, accuracy: 11829/32032 (37%)\nTrain - batches : 1002, average loss: 1.6850, accuracy: 11847/32064 (37%)\nTrain - batches : 1003, average loss: 1.6851, accuracy: 11860/32096 (37%)\nTrain - batches : 1004, average loss: 1.6853, accuracy: 11870/32128 (37%)\nTrain - batches : 1005, average loss: 1.6849, accuracy: 11891/32160 (37%)\nTrain - batches : 1006, average loss: 1.6849, accuracy: 11903/32192 (37%)\nTrain - batches : 1007, average loss: 1.6845, accuracy: 11917/32224 (37%)\nTrain - batches : 1008, average loss: 1.6843, accuracy: 11933/32256 (37%)\nTrain - batches : 1009, average loss: 1.6843, accuracy: 11948/32288 (37%)\nTrain - batches : 1010, average loss: 1.6839, accuracy: 11968/32320 (37%)\nTrain - batches : 1011, average loss: 1.6836, accuracy: 11986/32352 (37%)\nTrain - batches : 1012, average loss: 1.6832, accuracy: 12003/32384 (37%)\nTrain - batches : 1013, average loss: 1.6830, accuracy: 12020/32416 (37%)\nTrain - batches : 1014, average loss: 1.6829, accuracy: 12034/32448 (37%)\nTrain - batches : 1015, average loss: 1.6829, accuracy: 12047/32480 (37%)\nTrain - batches : 1016, average loss: 1.6824, accuracy: 12069/32512 (37%)\nTrain - batches : 1017, average loss: 1.6820, accuracy: 12087/32544 (37%)\nTrain - batches : 1018, average loss: 1.6818, accuracy: 12098/32576 (37%)\nTrain - batches : 1019, average loss: 1.6814, accuracy: 12112/32608 (37%)\nTrain - batches : 1020, average loss: 1.6811, accuracy: 12129/32640 (37%)\nTrain - batches : 1021, average loss: 1.6807, accuracy: 12146/32672 (37%)\nTrain - batches : 1022, average loss: 1.6804, accuracy: 12164/32704 (37%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 1023, average loss: 1.6801, accuracy: 12183/32736 (37%)\nTrain - batches : 1024, average loss: 1.6800, accuracy: 12194/32768 (37%)\nTrain - batches : 1025, average loss: 1.6796, accuracy: 12214/32800 (37%)\nTrain - batches : 1026, average loss: 1.6795, accuracy: 12226/32832 (37%)\nTrain - batches : 1027, average loss: 1.6792, accuracy: 12245/32864 (37%)\nTrain - batches : 1028, average loss: 1.6790, accuracy: 12262/32896 (37%)\nTrain - batches : 1029, average loss: 1.6791, accuracy: 12272/32928 (37%)\nTrain - batches : 1030, average loss: 1.6790, accuracy: 12283/32960 (37%)\nTrain - batches : 1031, average loss: 1.6787, accuracy: 12298/32992 (37%)\nTrain - batches : 1032, average loss: 1.6784, accuracy: 12314/33024 (37%)\nTrain - batches : 1033, average loss: 1.6780, accuracy: 12332/33056 (37%)\nTrain - batches : 1034, average loss: 1.6777, accuracy: 12349/33088 (37%)\nTrain - batches : 1035, average loss: 1.6779, accuracy: 12359/33120 (37%)\nTrain - batches : 1036, average loss: 1.6779, accuracy: 12372/33152 (37%)\nTrain - batches : 1037, average loss: 1.6776, accuracy: 12386/33184 (37%)\nTrain - batches : 1038, average loss: 1.6773, accuracy: 12400/33216 (37%)\nTrain - batches : 1039, average loss: 1.6771, accuracy: 12413/33248 (37%)\nTrain - batches : 1040, average loss: 1.6765, accuracy: 12434/33280 (37%)\nTrain - batches : 1041, average loss: 1.6762, accuracy: 12450/33312 (37%)\nTrain - batches : 1042, average loss: 1.6760, accuracy: 12463/33344 (37%)\nTrain - batches : 1043, average loss: 1.6757, accuracy: 12477/33376 (37%)\nTrain - batches : 1044, average loss: 1.6753, accuracy: 12496/33408 (37%)\nTrain - batches : 1045, average loss: 1.6755, accuracy: 12508/33440 (37%)\nTrain - batches : 1046, average loss: 1.6753, accuracy: 12524/33472 (37%)\nTrain - batches : 1047, average loss: 1.6751, accuracy: 12536/33504 (37%)\nTrain - batches : 1048, average loss: 1.6748, accuracy: 12551/33536 (37%)\nTrain - batches : 1049, average loss: 1.6746, accuracy: 12566/33568 (37%)\nTrain - batches : 1050, average loss: 1.6746, accuracy: 12582/33600 (37%)\nTrain - batches : 1051, average loss: 1.6745, accuracy: 12596/33632 (37%)\nTrain - batches : 1052, average loss: 1.6740, accuracy: 12616/33664 (37%)\nTrain - batches : 1053, average loss: 1.6739, accuracy: 12629/33696 (37%)\nTrain - batches : 1054, average loss: 1.6737, accuracy: 12641/33728 (37%)\nTrain - batches : 1055, average loss: 1.6736, accuracy: 12655/33760 (37%)\nTrain - batches : 1056, average loss: 1.6734, accuracy: 12670/33792 (37%)\nTrain - batches : 1057, average loss: 1.6732, accuracy: 12681/33824 (37%)\nTrain - batches : 1058, average loss: 1.6730, accuracy: 12696/33856 (38%)\nTrain - batches : 1059, average loss: 1.6728, accuracy: 12713/33888 (38%)\nTrain - batches : 1060, average loss: 1.6726, accuracy: 12730/33920 (38%)\nTrain - batches : 1061, average loss: 1.6722, accuracy: 12749/33952 (38%)\nTrain - batches : 1062, average loss: 1.6721, accuracy: 12763/33984 (38%)\nTrain - batches : 1063, average loss: 1.6720, accuracy: 12778/34016 (38%)\nTrain - batches : 1064, average loss: 1.6716, accuracy: 12794/34048 (38%)\nTrain - batches : 1065, average loss: 1.6712, accuracy: 12811/34080 (38%)\nTrain - batches : 1066, average loss: 1.6711, accuracy: 12823/34112 (38%)\nTrain - batches : 1067, average loss: 1.6706, accuracy: 12843/34144 (38%)\nTrain - batches : 1068, average loss: 1.6705, accuracy: 12856/34176 (38%)\nTrain - batches : 1069, average loss: 1.6704, accuracy: 12869/34208 (38%)\nTrain - batches : 1070, average loss: 1.6701, accuracy: 12888/34240 (38%)\nTrain - batches : 1071, average loss: 1.6699, accuracy: 12899/34272 (38%)\nTrain - batches : 1072, average loss: 1.6698, accuracy: 12914/34304 (38%)\nTrain - batches : 1073, average loss: 1.6699, accuracy: 12924/34336 (38%)\nTrain - batches : 1074, average loss: 1.6697, accuracy: 12942/34368 (38%)\nTrain - batches : 1075, average loss: 1.6695, accuracy: 12958/34400 (38%)\nTrain - batches : 1076, average loss: 1.6690, accuracy: 12979/34432 (38%)\nTrain - batches : 1077, average loss: 1.6689, accuracy: 12993/34464 (38%)\nTrain - batches : 1078, average loss: 1.6687, accuracy: 13008/34496 (38%)\nTrain - batches : 1079, average loss: 1.6684, accuracy: 13023/34528 (38%)\nTrain - batches : 1080, average loss: 1.6681, accuracy: 13040/34560 (38%)\nTrain - batches : 1081, average loss: 1.6678, accuracy: 13056/34592 (38%)\nTrain - batches : 1082, average loss: 1.6676, accuracy: 13071/34624 (38%)\nTrain - batches : 1083, average loss: 1.6672, accuracy: 13087/34656 (38%)\nTrain - batches : 1084, average loss: 1.6670, accuracy: 13103/34688 (38%)\nTrain - batches : 1085, average loss: 1.6669, accuracy: 13119/34720 (38%)\nTrain - batches : 1086, average loss: 1.6663, accuracy: 13137/34752 (38%)\nTrain - batches : 1087, average loss: 1.6663, accuracy: 13150/34784 (38%)\nTrain - batches : 1088, average loss: 1.6663, accuracy: 13164/34816 (38%)\nTrain - batches : 1089, average loss: 1.6659, accuracy: 13180/34848 (38%)\nTrain - batches : 1090, average loss: 1.6657, accuracy: 13198/34880 (38%)\nTrain - batches : 1091, average loss: 1.6654, accuracy: 13211/34912 (38%)\nTrain - batches : 1092, average loss: 1.6653, accuracy: 13223/34944 (38%)\nTrain - batches : 1093, average loss: 1.6649, accuracy: 13239/34976 (38%)\nTrain - batches : 1094, average loss: 1.6650, accuracy: 13250/35008 (38%)\nTrain - batches : 1095, average loss: 1.6649, accuracy: 13261/35040 (38%)\nTrain - batches : 1096, average loss: 1.6649, accuracy: 13271/35072 (38%)\nTrain - batches : 1097, average loss: 1.6645, accuracy: 13290/35104 (38%)\nTrain - batches : 1098, average loss: 1.6643, accuracy: 13305/35136 (38%)\nTrain - batches : 1099, average loss: 1.6642, accuracy: 13321/35168 (38%)\nTrain - batches : 1100, average loss: 1.6640, accuracy: 13338/35200 (38%)\nTrain - batches : 1101, average loss: 1.6638, accuracy: 13354/35232 (38%)\nTrain - batches : 1102, average loss: 1.6636, accuracy: 13368/35264 (38%)\nTrain - batches : 1103, average loss: 1.6633, accuracy: 13382/35296 (38%)\nTrain - batches : 1104, average loss: 1.6632, accuracy: 13392/35328 (38%)\nTrain - batches : 1105, average loss: 1.6630, accuracy: 13407/35360 (38%)\nTrain - batches : 1106, average loss: 1.6627, accuracy: 13424/35392 (38%)\nTrain - batches : 1107, average loss: 1.6626, accuracy: 13436/35424 (38%)\nTrain - batches : 1108, average loss: 1.6625, accuracy: 13451/35456 (38%)\nTrain - batches : 1109, average loss: 1.6621, accuracy: 13468/35488 (38%)\nTrain - batches : 1110, average loss: 1.6618, accuracy: 13485/35520 (38%)\nTrain - batches : 1111, average loss: 1.6615, accuracy: 13503/35552 (38%)\nTrain - batches : 1112, average loss: 1.6612, accuracy: 13522/35584 (38%)\nTrain - batches : 1113, average loss: 1.6608, accuracy: 13542/35616 (38%)\nTrain - batches : 1114, average loss: 1.6606, accuracy: 13560/35648 (38%)\nTrain - batches : 1115, average loss: 1.6604, accuracy: 13573/35680 (38%)\nTrain - batches : 1116, average loss: 1.6602, accuracy: 13585/35712 (38%)\nTrain - batches : 1117, average loss: 1.6604, accuracy: 13597/35744 (38%)\nTrain - batches : 1118, average loss: 1.6603, accuracy: 13609/35776 (38%)\nTrain - batches : 1119, average loss: 1.6601, accuracy: 13622/35808 (38%)\nTrain - batches : 1120, average loss: 1.6601, accuracy: 13632/35840 (38%)\nTrain - batches : 1121, average loss: 1.6597, accuracy: 13649/35872 (38%)\nTrain - batches : 1122, average loss: 1.6595, accuracy: 13667/35904 (38%)\nTrain - batches : 1123, average loss: 1.6595, accuracy: 13679/35936 (38%)\nTrain - batches : 1124, average loss: 1.6594, accuracy: 13694/35968 (38%)\nTrain - batches : 1125, average loss: 1.6593, accuracy: 13702/36000 (38%)\nTrain - batches : 1126, average loss: 1.6591, accuracy: 13717/36032 (38%)\nTrain - batches : 1127, average loss: 1.6587, accuracy: 13737/36064 (38%)\nTrain - batches : 1128, average loss: 1.6590, accuracy: 13746/36096 (38%)\nTrain - batches : 1129, average loss: 1.6587, accuracy: 13760/36128 (38%)\nTrain - batches : 1130, average loss: 1.6582, accuracy: 13778/36160 (38%)\nTrain - batches : 1131, average loss: 1.6580, accuracy: 13795/36192 (38%)\nTrain - batches : 1132, average loss: 1.6578, accuracy: 13807/36224 (38%)\nTrain - batches : 1133, average loss: 1.6579, accuracy: 13818/36256 (38%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 1134, average loss: 1.6577, accuracy: 13832/36288 (38%)\nTrain - batches : 1135, average loss: 1.6575, accuracy: 13847/36320 (38%)\nTrain - batches : 1136, average loss: 1.6571, accuracy: 13864/36352 (38%)\nTrain - batches : 1137, average loss: 1.6571, accuracy: 13881/36384 (38%)\nTrain - batches : 1138, average loss: 1.6571, accuracy: 13892/36416 (38%)\nTrain - batches : 1139, average loss: 1.6569, accuracy: 13909/36448 (38%)\nTrain - batches : 1140, average loss: 1.6565, accuracy: 13928/36480 (38%)\nTrain - batches : 1141, average loss: 1.6562, accuracy: 13944/36512 (38%)\nTrain - batches : 1142, average loss: 1.6559, accuracy: 13962/36544 (38%)\nTrain - batches : 1143, average loss: 1.6555, accuracy: 13979/36576 (38%)\nTrain - batches : 1144, average loss: 1.6556, accuracy: 13995/36608 (38%)\nTrain - batches : 1145, average loss: 1.6553, accuracy: 14010/36640 (38%)\nTrain - batches : 1146, average loss: 1.6551, accuracy: 14025/36672 (38%)\nTrain - batches : 1147, average loss: 1.6549, accuracy: 14041/36704 (38%)\nTrain - batches : 1148, average loss: 1.6545, accuracy: 14057/36736 (38%)\nTrain - batches : 1149, average loss: 1.6543, accuracy: 14071/36768 (38%)\nTrain - batches : 1150, average loss: 1.6541, accuracy: 14088/36800 (38%)\nTrain - batches : 1151, average loss: 1.6540, accuracy: 14098/36832 (38%)\nTrain - batches : 1152, average loss: 1.6537, accuracy: 14110/36864 (38%)\nTrain - batches : 1153, average loss: 1.6534, accuracy: 14128/36896 (38%)\nTrain - batches : 1154, average loss: 1.6532, accuracy: 14140/36928 (38%)\nTrain - batches : 1155, average loss: 1.6532, accuracy: 14151/36960 (38%)\nTrain - batches : 1156, average loss: 1.6528, accuracy: 14169/36992 (38%)\nTrain - batches : 1157, average loss: 1.6524, accuracy: 14189/37024 (38%)\nTrain - batches : 1158, average loss: 1.6522, accuracy: 14207/37056 (38%)\nTrain - batches : 1159, average loss: 1.6521, accuracy: 14220/37088 (38%)\nTrain - batches : 1160, average loss: 1.6519, accuracy: 14231/37120 (38%)\nTrain - batches : 1161, average loss: 1.6518, accuracy: 14244/37152 (38%)\nTrain - batches : 1162, average loss: 1.6515, accuracy: 14260/37184 (38%)\nTrain - batches : 1163, average loss: 1.6514, accuracy: 14276/37216 (38%)\nTrain - batches : 1164, average loss: 1.6513, accuracy: 14287/37248 (38%)\nTrain - batches : 1165, average loss: 1.6510, accuracy: 14303/37280 (38%)\nTrain - batches : 1166, average loss: 1.6509, accuracy: 14320/37312 (38%)\nTrain - batches : 1167, average loss: 1.6507, accuracy: 14334/37344 (38%)\nTrain - batches : 1168, average loss: 1.6505, accuracy: 14351/37376 (38%)\nTrain - batches : 1169, average loss: 1.6506, accuracy: 14363/37408 (38%)\nTrain - batches : 1170, average loss: 1.6504, accuracy: 14380/37440 (38%)\nTrain - batches : 1171, average loss: 1.6502, accuracy: 14396/37472 (38%)\nTrain - batches : 1172, average loss: 1.6500, accuracy: 14410/37504 (38%)\nTrain - batches : 1173, average loss: 1.6496, accuracy: 14429/37536 (38%)\nTrain - batches : 1174, average loss: 1.6495, accuracy: 14442/37568 (38%)\nTrain - batches : 1175, average loss: 1.6492, accuracy: 14457/37600 (38%)\nTrain - batches : 1176, average loss: 1.6490, accuracy: 14469/37632 (38%)\nTrain - batches : 1177, average loss: 1.6488, accuracy: 14485/37664 (38%)\nTrain - batches : 1178, average loss: 1.6485, accuracy: 14503/37696 (38%)\nTrain - batches : 1179, average loss: 1.6482, accuracy: 14516/37728 (38%)\nTrain - batches : 1180, average loss: 1.6479, accuracy: 14532/37760 (38%)\nTrain - batches : 1181, average loss: 1.6478, accuracy: 14546/37792 (38%)\nTrain - batches : 1182, average loss: 1.6476, accuracy: 14563/37824 (39%)\nTrain - batches : 1183, average loss: 1.6475, accuracy: 14577/37856 (39%)\nTrain - batches : 1184, average loss: 1.6471, accuracy: 14600/37888 (39%)\nTrain - batches : 1185, average loss: 1.6470, accuracy: 14616/37920 (39%)\nTrain - batches : 1186, average loss: 1.6465, accuracy: 14635/37952 (39%)\nTrain - batches : 1187, average loss: 1.6464, accuracy: 14653/37984 (39%)\nTrain - batches : 1188, average loss: 1.6464, accuracy: 14663/38016 (39%)\nTrain - batches : 1189, average loss: 1.6460, accuracy: 14683/38048 (39%)\nTrain - batches : 1190, average loss: 1.6458, accuracy: 14699/38080 (39%)\nTrain - batches : 1191, average loss: 1.6457, accuracy: 14713/38112 (39%)\nTrain - batches : 1192, average loss: 1.6454, accuracy: 14729/38144 (39%)\nTrain - batches : 1193, average loss: 1.6454, accuracy: 14742/38176 (39%)\nTrain - batches : 1194, average loss: 1.6454, accuracy: 14754/38208 (39%)\nTrain - batches : 1195, average loss: 1.6453, accuracy: 14770/38240 (39%)\nTrain - batches : 1196, average loss: 1.6452, accuracy: 14780/38272 (39%)\nTrain - batches : 1197, average loss: 1.6449, accuracy: 14797/38304 (39%)\nTrain - batches : 1198, average loss: 1.6445, accuracy: 14819/38336 (39%)\nTrain - batches : 1199, average loss: 1.6441, accuracy: 14834/38368 (39%)\nTrain - batches : 1200, average loss: 1.6438, accuracy: 14853/38400 (39%)\nTrain - batches : 1201, average loss: 1.6438, accuracy: 14866/38432 (39%)\nTrain - batches : 1202, average loss: 1.6440, accuracy: 14876/38464 (39%)\nTrain - batches : 1203, average loss: 1.6438, accuracy: 14888/38496 (39%)\nTrain - batches : 1204, average loss: 1.6436, accuracy: 14906/38528 (39%)\nTrain - batches : 1205, average loss: 1.6433, accuracy: 14923/38560 (39%)\nTrain - batches : 1206, average loss: 1.6430, accuracy: 14938/38592 (39%)\nTrain - batches : 1207, average loss: 1.6428, accuracy: 14953/38624 (39%)\nTrain - batches : 1208, average loss: 1.6427, accuracy: 14962/38656 (39%)\nTrain - batches : 1209, average loss: 1.6426, accuracy: 14980/38688 (39%)\nTrain - batches : 1210, average loss: 1.6422, accuracy: 15000/38720 (39%)\nTrain - batches : 1211, average loss: 1.6421, accuracy: 15015/38752 (39%)\nTrain - batches : 1212, average loss: 1.6417, accuracy: 15032/38784 (39%)\nTrain - batches : 1213, average loss: 1.6415, accuracy: 15046/38816 (39%)\nTrain - batches : 1214, average loss: 1.6415, accuracy: 15061/38848 (39%)\nTrain - batches : 1215, average loss: 1.6415, accuracy: 15075/38880 (39%)\nTrain - batches : 1216, average loss: 1.6414, accuracy: 15087/38912 (39%)\nTrain - batches : 1217, average loss: 1.6412, accuracy: 15103/38944 (39%)\nTrain - batches : 1218, average loss: 1.6411, accuracy: 15118/38976 (39%)\nTrain - batches : 1219, average loss: 1.6406, accuracy: 15137/39008 (39%)\nTrain - batches : 1220, average loss: 1.6404, accuracy: 15152/39040 (39%)\nTrain - batches : 1221, average loss: 1.6402, accuracy: 15167/39072 (39%)\nTrain - batches : 1222, average loss: 1.6399, accuracy: 15184/39104 (39%)\nTrain - batches : 1223, average loss: 1.6397, accuracy: 15198/39136 (39%)\nTrain - batches : 1224, average loss: 1.6396, accuracy: 15210/39168 (39%)\nTrain - batches : 1225, average loss: 1.6392, accuracy: 15226/39200 (39%)\nTrain - batches : 1226, average loss: 1.6391, accuracy: 15240/39232 (39%)\nTrain - batches : 1227, average loss: 1.6388, accuracy: 15257/39264 (39%)\nTrain - batches : 1228, average loss: 1.6387, accuracy: 15269/39296 (39%)\nTrain - batches : 1229, average loss: 1.6385, accuracy: 15283/39328 (39%)\nTrain - batches : 1230, average loss: 1.6385, accuracy: 15301/39360 (39%)\nTrain - batches : 1231, average loss: 1.6382, accuracy: 15316/39392 (39%)\nTrain - batches : 1232, average loss: 1.6382, accuracy: 15332/39424 (39%)\nTrain - batches : 1233, average loss: 1.6381, accuracy: 15348/39456 (39%)\nTrain - batches : 1234, average loss: 1.6380, accuracy: 15360/39488 (39%)\nTrain - batches : 1235, average loss: 1.6378, accuracy: 15376/39520 (39%)\nTrain - batches : 1236, average loss: 1.6376, accuracy: 15392/39552 (39%)\nTrain - batches : 1237, average loss: 1.6377, accuracy: 15404/39584 (39%)\nTrain - batches : 1238, average loss: 1.6376, accuracy: 15413/39616 (39%)\nTrain - batches : 1239, average loss: 1.6375, accuracy: 15429/39648 (39%)\nTrain - batches : 1240, average loss: 1.6372, accuracy: 15444/39680 (39%)\nTrain - batches : 1241, average loss: 1.6369, accuracy: 15461/39712 (39%)\nTrain - batches : 1242, average loss: 1.6367, accuracy: 15476/39744 (39%)\nTrain - batches : 1243, average loss: 1.6364, accuracy: 15493/39776 (39%)\nTrain - batches : 1244, average loss: 1.6365, accuracy: 15504/39808 (39%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 1245, average loss: 1.6363, accuracy: 15518/39840 (39%)\nTrain - batches : 1246, average loss: 1.6362, accuracy: 15535/39872 (39%)\nTrain - batches : 1247, average loss: 1.6358, accuracy: 15558/39904 (39%)\nTrain - batches : 1248, average loss: 1.6357, accuracy: 15572/39936 (39%)\nTrain - batches : 1249, average loss: 1.6357, accuracy: 15585/39968 (39%)\nTrain - batches : 1250, average loss: 1.6354, accuracy: 15602/40000 (39%)\nTrain - batches : 1251, average loss: 1.6351, accuracy: 15620/40032 (39%)\nTrain - batches : 1252, average loss: 1.6351, accuracy: 15631/40064 (39%)\nTrain - batches : 1253, average loss: 1.6350, accuracy: 15646/40096 (39%)\nTrain - batches : 1254, average loss: 1.6348, accuracy: 15660/40128 (39%)\nTrain - batches : 1255, average loss: 1.6347, accuracy: 15677/40160 (39%)\nTrain - batches : 1256, average loss: 1.6346, accuracy: 15693/40192 (39%)\nTrain - batches : 1257, average loss: 1.6349, accuracy: 15706/40224 (39%)\nTrain - batches : 1258, average loss: 1.6346, accuracy: 15724/40256 (39%)\nTrain - batches : 1259, average loss: 1.6344, accuracy: 15742/40288 (39%)\nTrain - batches : 1260, average loss: 1.6342, accuracy: 15762/40320 (39%)\nTrain - batches : 1261, average loss: 1.6342, accuracy: 15777/40352 (39%)\nTrain - batches : 1262, average loss: 1.6341, accuracy: 15790/40384 (39%)\nTrain - batches : 1263, average loss: 1.6340, accuracy: 15802/40416 (39%)\nTrain - batches : 1264, average loss: 1.6339, accuracy: 15818/40448 (39%)\nTrain - batches : 1265, average loss: 1.6338, accuracy: 15831/40480 (39%)\nTrain - batches : 1266, average loss: 1.6336, accuracy: 15843/40512 (39%)\nTrain - batches : 1267, average loss: 1.6335, accuracy: 15855/40544 (39%)\nTrain - batches : 1268, average loss: 1.6335, accuracy: 15872/40576 (39%)\nTrain - batches : 1269, average loss: 1.6332, accuracy: 15893/40608 (39%)\nTrain - batches : 1270, average loss: 1.6329, accuracy: 15910/40640 (39%)\nTrain - batches : 1271, average loss: 1.6326, accuracy: 15924/40672 (39%)\nTrain - batches : 1272, average loss: 1.6323, accuracy: 15939/40704 (39%)\nTrain - batches : 1273, average loss: 1.6321, accuracy: 15957/40736 (39%)\nTrain - batches : 1274, average loss: 1.6318, accuracy: 15976/40768 (39%)\nTrain - batches : 1275, average loss: 1.6316, accuracy: 15987/40800 (39%)\nTrain - batches : 1276, average loss: 1.6313, accuracy: 16005/40832 (39%)\nTrain - batches : 1277, average loss: 1.6313, accuracy: 16023/40864 (39%)\nTrain - batches : 1278, average loss: 1.6311, accuracy: 16041/40896 (39%)\nTrain - batches : 1279, average loss: 1.6305, accuracy: 16064/40928 (39%)\nTrain - batches : 1280, average loss: 1.6304, accuracy: 16079/40960 (39%)\nTrain - batches : 1281, average loss: 1.6303, accuracy: 16093/40992 (39%)\nTrain - batches : 1282, average loss: 1.6300, accuracy: 16109/41024 (39%)\nTrain - batches : 1283, average loss: 1.6299, accuracy: 16124/41056 (39%)\nTrain - batches : 1284, average loss: 1.6299, accuracy: 16136/41088 (39%)\nTrain - batches : 1285, average loss: 1.6296, accuracy: 16152/41120 (39%)\nTrain - batches : 1286, average loss: 1.6295, accuracy: 16168/41152 (39%)\nTrain - batches : 1287, average loss: 1.6293, accuracy: 16181/41184 (39%)\nTrain - batches : 1288, average loss: 1.6291, accuracy: 16198/41216 (39%)\nTrain - batches : 1289, average loss: 1.6290, accuracy: 16214/41248 (39%)\nTrain - batches : 1290, average loss: 1.6288, accuracy: 16232/41280 (39%)\nTrain - batches : 1291, average loss: 1.6285, accuracy: 16248/41312 (39%)\nTrain - batches : 1292, average loss: 1.6282, accuracy: 16265/41344 (39%)\nTrain - batches : 1293, average loss: 1.6280, accuracy: 16280/41376 (39%)\nTrain - batches : 1294, average loss: 1.6281, accuracy: 16292/41408 (39%)\nTrain - batches : 1295, average loss: 1.6279, accuracy: 16306/41440 (39%)\nTrain - batches : 1296, average loss: 1.6277, accuracy: 16324/41472 (39%)\nTrain - batches : 1297, average loss: 1.6273, accuracy: 16342/41504 (39%)\nTrain - batches : 1298, average loss: 1.6271, accuracy: 16362/41536 (39%)\nTrain - batches : 1299, average loss: 1.6269, accuracy: 16378/41568 (39%)\nTrain - batches : 1300, average loss: 1.6267, accuracy: 16391/41600 (39%)\nTrain - batches : 1301, average loss: 1.6263, accuracy: 16408/41632 (39%)\nTrain - batches : 1302, average loss: 1.6260, accuracy: 16427/41664 (39%)\nTrain - batches : 1303, average loss: 1.6257, accuracy: 16445/41696 (39%)\nTrain - batches : 1304, average loss: 1.6256, accuracy: 16460/41728 (39%)\nTrain - batches : 1305, average loss: 1.6256, accuracy: 16471/41760 (39%)\nTrain - batches : 1306, average loss: 1.6257, accuracy: 16482/41792 (39%)\nTrain - batches : 1307, average loss: 1.6257, accuracy: 16495/41824 (39%)\nTrain - batches : 1308, average loss: 1.6255, accuracy: 16509/41856 (39%)\nTrain - batches : 1309, average loss: 1.6253, accuracy: 16525/41888 (39%)\nTrain - batches : 1310, average loss: 1.6251, accuracy: 16539/41920 (39%)\nTrain - batches : 1311, average loss: 1.6249, accuracy: 16554/41952 (39%)\nTrain - batches : 1312, average loss: 1.6245, accuracy: 16573/41984 (39%)\nTrain - batches : 1313, average loss: 1.6243, accuracy: 16590/42016 (39%)\nTrain - batches : 1314, average loss: 1.6241, accuracy: 16608/42048 (39%)\nTrain - batches : 1315, average loss: 1.6238, accuracy: 16620/42080 (39%)\nTrain - batches : 1316, average loss: 1.6237, accuracy: 16634/42112 (39%)\nTrain - batches : 1317, average loss: 1.6233, accuracy: 16652/42144 (40%)\nTrain - batches : 1318, average loss: 1.6232, accuracy: 16666/42176 (40%)\nTrain - batches : 1319, average loss: 1.6229, accuracy: 16686/42208 (40%)\nTrain - batches : 1320, average loss: 1.6226, accuracy: 16700/42240 (40%)\nTrain - batches : 1321, average loss: 1.6224, accuracy: 16719/42272 (40%)\nTrain - batches : 1322, average loss: 1.6223, accuracy: 16731/42304 (40%)\nTrain - batches : 1323, average loss: 1.6223, accuracy: 16749/42336 (40%)\nTrain - batches : 1324, average loss: 1.6224, accuracy: 16764/42368 (40%)\nTrain - batches : 1325, average loss: 1.6225, accuracy: 16777/42400 (40%)\nTrain - batches : 1326, average loss: 1.6225, accuracy: 16792/42432 (40%)\nTrain - batches : 1327, average loss: 1.6223, accuracy: 16808/42464 (40%)\nTrain - batches : 1328, average loss: 1.6223, accuracy: 16824/42496 (40%)\nTrain - batches : 1329, average loss: 1.6221, accuracy: 16837/42528 (40%)\nTrain - batches : 1330, average loss: 1.6221, accuracy: 16849/42560 (40%)\nTrain - batches : 1331, average loss: 1.6219, accuracy: 16864/42592 (40%)\nTrain - batches : 1332, average loss: 1.6218, accuracy: 16878/42624 (40%)\nTrain - batches : 1333, average loss: 1.6215, accuracy: 16894/42656 (40%)\nTrain - batches : 1334, average loss: 1.6214, accuracy: 16908/42688 (40%)\nTrain - batches : 1335, average loss: 1.6212, accuracy: 16925/42720 (40%)\nTrain - batches : 1336, average loss: 1.6210, accuracy: 16941/42752 (40%)\nTrain - batches : 1337, average loss: 1.6205, accuracy: 16961/42784 (40%)\nTrain - batches : 1338, average loss: 1.6203, accuracy: 16980/42816 (40%)\nTrain - batches : 1339, average loss: 1.6201, accuracy: 16994/42848 (40%)\nTrain - batches : 1340, average loss: 1.6200, accuracy: 17008/42880 (40%)\nTrain - batches : 1341, average loss: 1.6199, accuracy: 17022/42912 (40%)\nTrain - batches : 1342, average loss: 1.6200, accuracy: 17031/42944 (40%)\nTrain - batches : 1343, average loss: 1.6199, accuracy: 17047/42976 (40%)\nTrain - batches : 1344, average loss: 1.6197, accuracy: 17064/43008 (40%)\nTrain - batches : 1345, average loss: 1.6194, accuracy: 17084/43040 (40%)\nTrain - batches : 1346, average loss: 1.6191, accuracy: 17100/43072 (40%)\nTrain - batches : 1347, average loss: 1.6189, accuracy: 17116/43104 (40%)\nTrain - batches : 1348, average loss: 1.6189, accuracy: 17131/43136 (40%)\nTrain - batches : 1349, average loss: 1.6186, accuracy: 17149/43168 (40%)\nTrain - batches : 1350, average loss: 1.6183, accuracy: 17166/43200 (40%)\nTrain - batches : 1351, average loss: 1.6179, accuracy: 17184/43232 (40%)\nTrain - batches : 1352, average loss: 1.6177, accuracy: 17201/43264 (40%)\nTrain - batches : 1353, average loss: 1.6174, accuracy: 17216/43296 (40%)\nTrain - batches : 1354, average loss: 1.6172, accuracy: 17235/43328 (40%)\nTrain - batches : 1355, average loss: 1.6172, accuracy: 17249/43360 (40%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 1356, average loss: 1.6170, accuracy: 17269/43392 (40%)\nTrain - batches : 1357, average loss: 1.6168, accuracy: 17282/43424 (40%)\nTrain - batches : 1358, average loss: 1.6167, accuracy: 17298/43456 (40%)\nTrain - batches : 1359, average loss: 1.6162, accuracy: 17319/43488 (40%)\nTrain - batches : 1360, average loss: 1.6161, accuracy: 17336/43520 (40%)\nTrain - batches : 1361, average loss: 1.6158, accuracy: 17355/43552 (40%)\nTrain - batches : 1362, average loss: 1.6155, accuracy: 17373/43584 (40%)\nTrain - batches : 1363, average loss: 1.6154, accuracy: 17389/43616 (40%)\nTrain - batches : 1364, average loss: 1.6152, accuracy: 17407/43648 (40%)\nTrain - batches : 1365, average loss: 1.6151, accuracy: 17420/43680 (40%)\nTrain - batches : 1366, average loss: 1.6149, accuracy: 17433/43712 (40%)\nTrain - batches : 1367, average loss: 1.6147, accuracy: 17452/43744 (40%)\nTrain - batches : 1368, average loss: 1.6146, accuracy: 17469/43776 (40%)\nTrain - batches : 1369, average loss: 1.6145, accuracy: 17482/43808 (40%)\nTrain - batches : 1370, average loss: 1.6142, accuracy: 17495/43840 (40%)\nTrain - batches : 1371, average loss: 1.6141, accuracy: 17508/43872 (40%)\nTrain - batches : 1372, average loss: 1.6139, accuracy: 17523/43904 (40%)\nTrain - batches : 1373, average loss: 1.6137, accuracy: 17542/43936 (40%)\nTrain - batches : 1374, average loss: 1.6135, accuracy: 17556/43968 (40%)\nTrain - batches : 1375, average loss: 1.6133, accuracy: 17571/44000 (40%)\nTrain - batches : 1376, average loss: 1.6130, accuracy: 17586/44032 (40%)\nTrain - batches : 1377, average loss: 1.6127, accuracy: 17608/44064 (40%)\nTrain - batches : 1378, average loss: 1.6127, accuracy: 17620/44096 (40%)\nTrain - batches : 1379, average loss: 1.6125, accuracy: 17634/44128 (40%)\nTrain - batches : 1380, average loss: 1.6124, accuracy: 17649/44160 (40%)\nTrain - batches : 1381, average loss: 1.6122, accuracy: 17665/44192 (40%)\nTrain - batches : 1382, average loss: 1.6117, accuracy: 17685/44224 (40%)\nTrain - batches : 1383, average loss: 1.6115, accuracy: 17703/44256 (40%)\nTrain - batches : 1384, average loss: 1.6112, accuracy: 17719/44288 (40%)\nTrain - batches : 1385, average loss: 1.6110, accuracy: 17736/44320 (40%)\nTrain - batches : 1386, average loss: 1.6111, accuracy: 17749/44352 (40%)\nTrain - batches : 1387, average loss: 1.6109, accuracy: 17765/44384 (40%)\nTrain - batches : 1388, average loss: 1.6109, accuracy: 17778/44416 (40%)\nTrain - batches : 1389, average loss: 1.6109, accuracy: 17795/44448 (40%)\nTrain - batches : 1390, average loss: 1.6105, accuracy: 17815/44480 (40%)\nTrain - batches : 1391, average loss: 1.6103, accuracy: 17832/44512 (40%)\nTrain - batches : 1392, average loss: 1.6100, accuracy: 17854/44544 (40%)\nTrain - batches : 1393, average loss: 1.6099, accuracy: 17866/44576 (40%)\nTrain - batches : 1394, average loss: 1.6098, accuracy: 17881/44608 (40%)\nTrain - batches : 1395, average loss: 1.6097, accuracy: 17892/44640 (40%)\nTrain - batches : 1396, average loss: 1.6096, accuracy: 17907/44672 (40%)\nTrain - batches : 1397, average loss: 1.6095, accuracy: 17919/44704 (40%)\nTrain - batches : 1398, average loss: 1.6093, accuracy: 17938/44736 (40%)\nTrain - batches : 1399, average loss: 1.6090, accuracy: 17958/44768 (40%)\nTrain - batches : 1400, average loss: 1.6089, accuracy: 17975/44800 (40%)\nTrain - batches : 1401, average loss: 1.6091, accuracy: 17985/44832 (40%)\nTrain - batches : 1402, average loss: 1.6088, accuracy: 18000/44864 (40%)\nTrain - batches : 1403, average loss: 1.6086, accuracy: 18015/44896 (40%)\nTrain - batches : 1404, average loss: 1.6085, accuracy: 18032/44928 (40%)\nTrain - batches : 1405, average loss: 1.6084, accuracy: 18049/44960 (40%)\nTrain - batches : 1406, average loss: 1.6081, accuracy: 18065/44992 (40%)\nTrain - batches : 1407, average loss: 1.6078, accuracy: 18085/45024 (40%)\nTrain - batches : 1408, average loss: 1.6076, accuracy: 18103/45056 (40%)\nTrain - batches : 1409, average loss: 1.6075, accuracy: 18117/45088 (40%)\nTrain - batches : 1410, average loss: 1.6074, accuracy: 18135/45120 (40%)\nTrain - batches : 1411, average loss: 1.6072, accuracy: 18152/45152 (40%)\nTrain - batches : 1412, average loss: 1.6069, accuracy: 18168/45184 (40%)\nTrain - batches : 1413, average loss: 1.6069, accuracy: 18177/45216 (40%)\nTrain - batches : 1414, average loss: 1.6066, accuracy: 18195/45248 (40%)\nTrain - batches : 1415, average loss: 1.6064, accuracy: 18211/45280 (40%)\nTrain - batches : 1416, average loss: 1.6060, accuracy: 18232/45312 (40%)\nTrain - batches : 1417, average loss: 1.6058, accuracy: 18249/45344 (40%)\nTrain - batches : 1418, average loss: 1.6058, accuracy: 18265/45376 (40%)\nTrain - batches : 1419, average loss: 1.6060, accuracy: 18277/45408 (40%)\nTrain - batches : 1420, average loss: 1.6058, accuracy: 18294/45440 (40%)\nTrain - batches : 1421, average loss: 1.6055, accuracy: 18314/45472 (40%)\nTrain - batches : 1422, average loss: 1.6054, accuracy: 18328/45504 (40%)\nTrain - batches : 1423, average loss: 1.6051, accuracy: 18345/45536 (40%)\nTrain - batches : 1424, average loss: 1.6049, accuracy: 18362/45568 (40%)\nTrain - batches : 1425, average loss: 1.6047, accuracy: 18374/45600 (40%)\nTrain - batches : 1426, average loss: 1.6045, accuracy: 18389/45632 (40%)\nTrain - batches : 1427, average loss: 1.6045, accuracy: 18405/45664 (40%)\nTrain - batches : 1428, average loss: 1.6041, accuracy: 18424/45696 (40%)\nTrain - batches : 1429, average loss: 1.6037, accuracy: 18445/45728 (40%)\nTrain - batches : 1430, average loss: 1.6036, accuracy: 18463/45760 (40%)\nTrain - batches : 1431, average loss: 1.6035, accuracy: 18476/45792 (40%)\nTrain - batches : 1432, average loss: 1.6033, accuracy: 18491/45824 (40%)\nTrain - batches : 1433, average loss: 1.6032, accuracy: 18508/45856 (40%)\nTrain - batches : 1434, average loss: 1.6030, accuracy: 18524/45888 (40%)\nTrain - batches : 1435, average loss: 1.6028, accuracy: 18540/45920 (40%)\nTrain - batches : 1436, average loss: 1.6024, accuracy: 18563/45952 (40%)\nTrain - batches : 1437, average loss: 1.6021, accuracy: 18580/45984 (40%)\nTrain - batches : 1438, average loss: 1.6018, accuracy: 18599/46016 (40%)\nTrain - batches : 1439, average loss: 1.6016, accuracy: 18618/46048 (40%)\nTrain - batches : 1440, average loss: 1.6015, accuracy: 18635/46080 (40%)\nTrain - batches : 1441, average loss: 1.6012, accuracy: 18655/46112 (40%)\nTrain - batches : 1442, average loss: 1.6012, accuracy: 18667/46144 (40%)\nTrain - batches : 1443, average loss: 1.6010, accuracy: 18681/46176 (40%)\nTrain - batches : 1444, average loss: 1.6006, accuracy: 18702/46208 (40%)\nTrain - batches : 1445, average loss: 1.6005, accuracy: 18716/46240 (40%)\nTrain - batches : 1446, average loss: 1.6001, accuracy: 18738/46272 (40%)\nTrain - batches : 1447, average loss: 1.6000, accuracy: 18752/46304 (40%)\nTrain - batches : 1448, average loss: 1.5997, accuracy: 18773/46336 (41%)\nTrain - batches : 1449, average loss: 1.5995, accuracy: 18787/46368 (41%)\nTrain - batches : 1450, average loss: 1.5993, accuracy: 18803/46400 (41%)\nTrain - batches : 1451, average loss: 1.5990, accuracy: 18821/46432 (41%)\nTrain - batches : 1452, average loss: 1.5990, accuracy: 18835/46464 (41%)\nTrain - batches : 1453, average loss: 1.5988, accuracy: 18852/46496 (41%)\nTrain - batches : 1454, average loss: 1.5987, accuracy: 18869/46528 (41%)\nTrain - batches : 1455, average loss: 1.5986, accuracy: 18880/46560 (41%)\nTrain - batches : 1456, average loss: 1.5986, accuracy: 18894/46592 (41%)\nTrain - batches : 1457, average loss: 1.5982, accuracy: 18916/46624 (41%)\nTrain - batches : 1458, average loss: 1.5982, accuracy: 18932/46656 (41%)\nTrain - batches : 1459, average loss: 1.5979, accuracy: 18950/46688 (41%)\nTrain - batches : 1460, average loss: 1.5977, accuracy: 18964/46720 (41%)\nTrain - batches : 1461, average loss: 1.5975, accuracy: 18982/46752 (41%)\nTrain - batches : 1462, average loss: 1.5972, accuracy: 18998/46784 (41%)\nTrain - batches : 1463, average loss: 1.5970, accuracy: 19019/46816 (41%)\nTrain - batches : 1464, average loss: 1.5967, accuracy: 19034/46848 (41%)\nTrain - batches : 1465, average loss: 1.5965, accuracy: 19053/46880 (41%)\nTrain - batches : 1466, average loss: 1.5962, accuracy: 19066/46912 (41%)\n"}, {"name": "stdout", "output_type": "stream", "text": "Train - batches : 1467, average loss: 1.5961, accuracy: 19081/46944 (41%)\nTrain - batches : 1468, average loss: 1.5962, accuracy: 19093/46976 (41%)\nTrain - batches : 1469, average loss: 1.5961, accuracy: 19106/47008 (41%)\nTrain - batches : 1470, average loss: 1.5959, accuracy: 19121/47040 (41%)\nTrain - batches : 1471, average loss: 1.5958, accuracy: 19138/47072 (41%)\nTrain - batches : 1472, average loss: 1.5957, accuracy: 19159/47104 (41%)\nTrain - batches : 1473, average loss: 1.5953, accuracy: 19180/47136 (41%)\nTrain - batches : 1474, average loss: 1.5951, accuracy: 19196/47168 (41%)\nTrain - batches : 1475, average loss: 1.5948, accuracy: 19216/47200 (41%)\nTrain - batches : 1476, average loss: 1.5946, accuracy: 19234/47232 (41%)\nTrain - batches : 1477, average loss: 1.5947, accuracy: 19246/47264 (41%)\nTrain - batches : 1478, average loss: 1.5944, accuracy: 19263/47296 (41%)\nTrain - batches : 1479, average loss: 1.5941, accuracy: 19281/47328 (41%)\nTrain - batches : 1480, average loss: 1.5940, accuracy: 19298/47360 (41%)\nTrain - batches : 1481, average loss: 1.5938, accuracy: 19313/47392 (41%)\nTrain - batches : 1482, average loss: 1.5936, accuracy: 19326/47424 (41%)\nTrain - batches : 1483, average loss: 1.5933, accuracy: 19347/47456 (41%)\nTrain - batches : 1484, average loss: 1.5935, accuracy: 19355/47488 (41%)\nTrain - batches : 1485, average loss: 1.5932, accuracy: 19372/47520 (41%)\nTrain - batches : 1486, average loss: 1.5932, accuracy: 19385/47552 (41%)\nTrain - batches : 1487, average loss: 1.5930, accuracy: 19403/47584 (41%)\nTrain - batches : 1488, average loss: 1.5927, accuracy: 19421/47616 (41%)\nTrain - batches : 1489, average loss: 1.5927, accuracy: 19434/47648 (41%)\nTrain - batches : 1490, average loss: 1.5926, accuracy: 19453/47680 (41%)\nTrain - batches : 1491, average loss: 1.5925, accuracy: 19464/47712 (41%)\nTrain - batches : 1492, average loss: 1.5921, accuracy: 19482/47744 (41%)\nTrain - batches : 1493, average loss: 1.5923, accuracy: 19492/47776 (41%)\nTrain - batches : 1494, average loss: 1.5919, accuracy: 19511/47808 (41%)\nTrain - batches : 1495, average loss: 1.5917, accuracy: 19529/47840 (41%)\nTrain - batches : 1496, average loss: 1.5916, accuracy: 19546/47872 (41%)\nTrain - batches : 1497, average loss: 1.5915, accuracy: 19561/47904 (41%)\nTrain - batches : 1498, average loss: 1.5914, accuracy: 19576/47936 (41%)\nTrain - batches : 1499, average loss: 1.5912, accuracy: 19594/47968 (41%)\nTrain - batches : 1500, average loss: 1.5912, accuracy: 19606/48000 (41%)\nTrain - batches : 1501, average loss: 1.5912, accuracy: 19620/48032 (41%)\nTrain - batches : 1502, average loss: 1.5910, accuracy: 19636/48064 (41%)\nTrain - batches : 1503, average loss: 1.5907, accuracy: 19656/48096 (41%)\nTrain - batches : 1504, average loss: 1.5905, accuracy: 19674/48128 (41%)\nTrain - batches : 1505, average loss: 1.5904, accuracy: 19688/48160 (41%)\nTrain - batches : 1506, average loss: 1.5903, accuracy: 19704/48192 (41%)\nTrain - batches : 1507, average loss: 1.5902, accuracy: 19721/48224 (41%)\nTrain - batches : 1508, average loss: 1.5901, accuracy: 19734/48256 (41%)\nTrain - batches : 1509, average loss: 1.5898, accuracy: 19750/48288 (41%)\nTrain - batches : 1510, average loss: 1.5897, accuracy: 19765/48320 (41%)\nTrain - batches : 1511, average loss: 1.5895, accuracy: 19784/48352 (41%)\nTrain - batches : 1512, average loss: 1.5894, accuracy: 19796/48384 (41%)\nTrain - batches : 1513, average loss: 1.5892, accuracy: 19813/48416 (41%)\nTrain - batches : 1514, average loss: 1.5890, accuracy: 19826/48448 (41%)\nTrain - batches : 1515, average loss: 1.5888, accuracy: 19842/48480 (41%)\nTrain - batches : 1516, average loss: 1.5887, accuracy: 19862/48512 (41%)\nTrain - batches : 1517, average loss: 1.5886, accuracy: 19877/48544 (41%)\nTrain - batches : 1518, average loss: 1.5883, accuracy: 19896/48576 (41%)\nTrain - batches : 1519, average loss: 1.5881, accuracy: 19912/48608 (41%)\nTrain - batches : 1520, average loss: 1.5879, accuracy: 19929/48640 (41%)\nTrain - batches : 1521, average loss: 1.5880, accuracy: 19942/48672 (41%)\nTrain - batches : 1522, average loss: 1.5878, accuracy: 19955/48704 (41%)\nTrain - batches : 1523, average loss: 1.5875, accuracy: 19973/48736 (41%)\nTrain - batches : 1524, average loss: 1.5875, accuracy: 19986/48768 (41%)\nTrain - batches : 1525, average loss: 1.5872, accuracy: 20005/48800 (41%)\nTrain - batches : 1526, average loss: 1.5870, accuracy: 20024/48832 (41%)\nTrain - batches : 1527, average loss: 1.5867, accuracy: 20043/48864 (41%)\nTrain - batches : 1528, average loss: 1.5863, accuracy: 20061/48896 (41%)\nTrain - batches : 1529, average loss: 1.5863, accuracy: 20076/48928 (41%)\nTrain - batches : 1530, average loss: 1.5860, accuracy: 20094/48960 (41%)\nTrain - batches : 1531, average loss: 1.5859, accuracy: 20111/48992 (41%)\nTrain - batches : 1532, average loss: 1.5857, accuracy: 20129/49024 (41%)\nTrain - batches : 1533, average loss: 1.5857, accuracy: 20137/49056 (41%)\nTrain - batches : 1534, average loss: 1.5855, accuracy: 20155/49088 (41%)\nTrain - batches : 1535, average loss: 1.5852, accuracy: 20168/49120 (41%)\nTrain - batches : 1536, average loss: 1.5851, accuracy: 20186/49152 (41%)\nTrain - batches : 1537, average loss: 1.5848, accuracy: 20204/49184 (41%)\nTrain - batches : 1538, average loss: 1.5845, accuracy: 20226/49216 (41%)\nTrain - batches : 1539, average loss: 1.5842, accuracy: 20246/49248 (41%)\nTrain - batches : 1540, average loss: 1.5840, accuracy: 20259/49280 (41%)\nTrain - batches : 1541, average loss: 1.5839, accuracy: 20276/49312 (41%)\nTrain - batches : 1542, average loss: 1.5837, accuracy: 20293/49344 (41%)\nTrain - batches : 1543, average loss: 1.5838, accuracy: 20308/49376 (41%)\nTrain - batches : 1544, average loss: 1.5838, accuracy: 20321/49408 (41%)\nTrain - batches : 1545, average loss: 1.5836, accuracy: 20338/49440 (41%)\nTrain - batches : 1546, average loss: 1.5836, accuracy: 20353/49472 (41%)\nTrain - batches : 1547, average loss: 1.5832, accuracy: 20372/49504 (41%)\nTrain - batches : 1548, average loss: 1.5829, accuracy: 20389/49536 (41%)\nTrain - batches : 1549, average loss: 1.5829, accuracy: 20401/49568 (41%)\nTrain - batches : 1550, average loss: 1.5827, accuracy: 20416/49600 (41%)\nTrain - batches : 1551, average loss: 1.5826, accuracy: 20431/49632 (41%)\nTrain - batches : 1552, average loss: 1.5822, accuracy: 20449/49664 (41%)\nTrain - batches : 1553, average loss: 1.5821, accuracy: 20466/49696 (41%)\nTrain - batches : 1554, average loss: 1.5818, accuracy: 20483/49728 (41%)\nTrain - batches : 1555, average loss: 1.5816, accuracy: 20501/49760 (41%)\nTrain - batches : 1556, average loss: 1.5813, accuracy: 20521/49792 (41%)\nTrain - batches : 1557, average loss: 1.5814, accuracy: 20535/49824 (41%)\nTrain - batches : 1558, average loss: 1.5811, accuracy: 20553/49856 (41%)\nTrain - batches : 1559, average loss: 1.5810, accuracy: 20569/49888 (41%)\nTrain - batches : 1560, average loss: 1.5808, accuracy: 20586/49920 (41%)\nTrain - batches : 1561, average loss: 1.5806, accuracy: 20603/49952 (41%)\nTrain - batches : 1562, average loss: 1.5804, accuracy: 20616/49984 (41%)\nTrain - batches : 1563, average loss: 1.5803, accuracy: 20623/50000 (41%)\nTest - batches: 1563, average loss: 0.0428, accuracy: 4988/10000 (50%)\n\nTraining cost:  1398  seconds.\n"}]}, {"metadata": {"id": "bd858908a5714a46881266091e1442a1"}, "cell_type": "markdown", "source": "<a id = \"gpu\"></a>\n## Step 4: Training the model on GPU with Watson Machine Learning Accelerator\n\n#### Prepare the model files for running on GPU:"}, {"metadata": {"id": "b7cf774caa0c4902a2bd0719b5b955d0"}, "cell_type": "code", "source": "import os\nmodel_dir = f'/project_data/data_asset/pytorch-resnet/resnet-wmla' \nmodel_main = f'main.py'\n\nos.makedirs(model_dir, exist_ok=True)", "execution_count": 6, "outputs": []}, {"metadata": {"id": "64a1d4d199a9464aa0c589f9791eb66e"}, "cell_type": "code", "source": "%%writefile {model_dir}/{model_main}\n#!/usr/bin/env python\n# coding: utf-8\n\n# # Image Classification Using PyTorch Resnet with Watson Machine Learning Accelerator Notebook\n# This asset details the process of performing a basic computer vision image classification example using the notebook functionality within Watson Machine Learning Accelerator. In this asset, you will learn how to accelerate your training with pytorch resnet model upon the cifar10 dataset.\n#\n# Please refer to [Resnet Introduction](https://arxiv.org/abs/1512.03385) for more details.\n\n\n\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torchvision.models as models\nimport time\n\nimport sys\nimport os\nimport glob\nimport argparse\n\nlog_interval = 10\n\nseed = 1\nuse_cuda = False\ncompleted_batch =0\ncompleted_test_batch =0\ncriterion = nn.CrossEntropyLoss()\n\n\nparser = argparse.ArgumentParser(description='Tensorflow MNIST Example')\nparser.add_argument('--batch-size', type=int, default=128, metavar='N',\n                    help='input batch size for training (default: 128)')\nparser.add_argument('--epochs', type=int, default=1, metavar='N',\n                    help='number of epochs to train (default: 1)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('cuda', action='store_true', default=True,\n                    help='disables CUDA training')\nargs = parser.parse_args()\nprint(args)\n\n\n# ## Create the Resnet18 model\nuse_cuda = args.cuda\nprint(\"Use cuda: \", use_cuda)\n\n# ## Download the Cifar10 dataset\n# Below code will download the cifar10 dataset automatically to $DATA_DIR/cifar10.\n# You could also download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and upload it manually.\n\nprint(\"DATA_DIR: \" + os.getenv(\"DATA_DIR\"))\nDATA_DIR = os.getenv(\"DATA_DIR\")\n\ndef getDatasets():\n    train_data_dir = DATA_DIR + \"/cifar10\"\n    test_data_dir = DATA_DIR + \"/cifar10\"\n\n    transform_train = transforms.Compose([\n        transforms.Resize(224),\n        #transforms.RandomCrop(self.resolution, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    return (torchvision.datasets.CIFAR10(root=train_data_dir, train=True, download=True, transform = transform_train),\n            torchvision.datasets.CIFAR10(root=test_data_dir, train=False, download=True, transform = transform_test)\n            )\n\ntorch.manual_seed(seed)\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n\ntrain_dataset, test_dataset = getDatasets()\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n\n# ## Implement the customized train and test loop\n\n\ndef train(model, device, train_loader, optimizer, epoch):\n    global completed_batch\n    train_loss = 0\n    correct = 0\n    total = 0\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n\n        completed_batch += 1\n\n        print ('Train - batches : {}, average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n           completed_batch, train_loss/(batch_idx+1), correct, total, 100.*correct/total))\n\n\ndef test(model, device, test_loader, epoch):\n    global completed_test_batch\n    global completed_batch\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    completed_test_batch = completed_batch -  len(test_loader)\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(test_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            loss = criterion(output, target)\n\n            test_loss += loss.item() # sum up batch loss\n            _, pred = output.max(1) # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            total += target.size(0)\n\n            completed_test_batch += 1\n\n    test_loss /= len(test_loader.dataset)\n    test_acc = 100. * correct / len(test_loader.dataset)\n    # Output test info for per epoch\n    print('Test - batches: {}, average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)\\n'.format(\n        completed_batch, test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\n# ## Create the Resnet18 model\n\nmodel_type = \"resnet18\"\n#model_type = \"resnet50\"\nprint(\"=> using pytorch build-in model '{}'\".format(model_type))\n\nmodel = models.resnet18()\n\n\n# Using pytorch build-in resnet18 model, the model is pre-trained on the ImageNet dataset,\n# which has 1000 classifications. To transfer it to cifar10 dataset, we can modify the last fully-connected layer output size to 10\n\nfor param in model.parameters():\n    param.requires_grad = True  # set False if you only want to train the last layer using pretrained model\n    # Replace the last fully-connected layer\n    # Parameters of newly constructed modules have requires_grad=True by default\n    model.fc = nn.Linear(512, 10)\n\n\n# (Optional) To use wmla pretrained resnet18 model for cifar10, load the model weight file. The pretrained model weight file can be downloaded [here](https://?).\n\nweightfile = DATA_DIR + \"/checkpoint/model_epoch_final.pth\"\nif os.path.exists(weightfile):\n    print (\"Initial weight file is \" + weightfile)\n    model.load_state_dict(torch.load(weightfile, map_location=lambda storage, loc: storage))\n\n\n# ## Run the model trainings\nmodel.to(device)\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)\nepochs = args.epochs\nscheduler = optim.lr_scheduler.StepLR(optimizer, 30, 0.1, last_epoch=-1)\n\n# Output total iterations info for deep learning insights\nprint(\"Total iterations: %s\" % (len(train_loader) * epochs))\n\nprint(\"RESULT_DIR: \" + os.getenv(\"RESULT_DIR\"))\nRESULT_DIR = os.getenv(\"RESULT_DIR\")\nos.makedirs(RESULT_DIR, exist_ok=True)\n\nfor epoch in range(1, epochs+1):\n    print(\"\\nRunning epoch %s ... It might take several minutes for each epoch to run.\" % epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader, epoch)\n    scheduler.step()\n\n    torch.save(model.state_dict(),  RESULT_DIR + \"/model/model_epoch_%d.pth\"%(epoch))\n\ntorch.save(model.state_dict(), RESULT_DIR + \"/model/model_epoch_final.pth\")", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "Overwriting /project_data/data_asset/pytorch-resnet/resnet-wmla/main.py\n", "name": "stdout"}]}, {"metadata": {"id": "512529be85064a589196e893c24cc1e8"}, "cell_type": "markdown", "source": "## Training results on GPU\n\n#### Training was run from a Cloud Pak for Data Notebook utilizing a GPU kernel. \n\n\nIn the custom environment that was created with **16vCPU** and **32GB**, it took **147seconds** (or approximately **2.5 minutes**) to complete 1 EPOCH training.\n"}, {"metadata": {"id": "9b428dc3dcef44bf94f7afe59e68f5d8"}, "cell_type": "code", "source": "files = {'file': open('/project_data/data_asset/pytorch-resnet/resnet-wmla/main.py', 'rb')}\n\nargs = '--exec-start PyTorch --cs-datastore-meta type=fs \\\n                     --workerDeviceNum 1 \\\n                     --model-main main.py --epochs 1'\n", "execution_count": 17, "outputs": []}, {"metadata": {"id": "12744932929149de91df0af4d20bfd50"}, "cell_type": "code", "source": "starttime = datetime.datetime.now()\n\nr = requests.post(dl_rest_url+'/execs?args='+args, files=files,\n                  headers=commonHeaders, verify=False)\nif not r.ok:\n    print('submit job failed: code=%s, %s'%(r.status_code, r.content))\n    \njob_status = query_job_status(r.json(),refresh_rate=5)\n\nendtime = datetime.datetime.now()\n\nprint(\"\\nTraining cost: \", (endtime - starttime).seconds, \" seconds.\")", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Refreshing every 5 seconds\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "           id  \\\n0  xwmla-1882   \n\n                                                                                                                      args  \\\n0  --exec-start PyTorch --cs-datastore-meta type=fs                      --workerDeviceNum 1                      --mod...   \n\n  submissionId creator     state       appId           schedulerUrl  \\\n0   xwmla-1882   admin  FINISHED  xwmla-1882  https://wmla-mss:9080   \n\n  modelFileOwnerName  \\\n0               wmla   \n\n                                                          workDir  \\\n0  /gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code   \n\n                  appName            createTime  elastic nameSpace  numWorker  \\\n0  SingleNodePytorchTrain  2021-02-08T21:17:27Z    False     xwmla          1   \n\n  framework  \n0   PyTorch  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>args</th>\n      <th>submissionId</th>\n      <th>creator</th>\n      <th>state</th>\n      <th>appId</th>\n      <th>schedulerUrl</th>\n      <th>modelFileOwnerName</th>\n      <th>workDir</th>\n      <th>appName</th>\n      <th>createTime</th>\n      <th>elastic</th>\n      <th>nameSpace</th>\n      <th>numWorker</th>\n      <th>framework</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xwmla-1882</td>\n      <td>--exec-start PyTorch --cs-datastore-meta type=fs                      --workerDeviceNum 1                      --mod...</td>\n      <td>xwmla-1882</td>\n      <td>admin</td>\n      <td>FINISHED</td>\n      <td>xwmla-1882</td>\n      <td>https://wmla-mss:9080</td>\n      <td>wmla</td>\n      <td>/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code</td>\n      <td>SingleNodePytorchTrain</td>\n      <td>2021-02-08T21:17:27Z</td>\n      <td>False</td>\n      <td>xwmla</td>\n      <td>1</td>\n      <td>PyTorch</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}, {"output_type": "stream", "text": "{ 'appId': 'xwmla-1882',\n  'appName': 'SingleNodePytorchTrain',\n  'args': '--exec-start PyTorch --cs-datastore-meta '\n          'type=fs                      --workerDeviceNum '\n          '1                      --model-main main.py --epochs 1 ',\n  'createTime': '2021-02-08T21:17:27Z',\n  'creator': 'admin',\n  'elastic': False,\n  'framework': 'PyTorch',\n  'id': 'xwmla-1882',\n  'modelFileOwnerName': 'wmla',\n  'nameSpace': 'xwmla',\n  'numWorker': 1,\n  'schedulerUrl': 'https://wmla-mss:9080',\n  'state': 'FINISHED',\n  'submissionId': 'xwmla-1882',\n  'workDir': '/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code'}\n\nTraining cost:  142  seconds.\n", "name": "stdout"}]}, {"metadata": {"id": "4cdf692a2d9740209a8484619f409a0a"}, "cell_type": "markdown", "source": "## Training metrics and logs\n\n#### Retrieve and display the model training metrics:"}, {"metadata": {"id": "28bd037ad580460aa3545c65c19c47dc"}, "cell_type": "code", "source": "query_train_metric(r.json())", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "https://wmla-console-xwmla.apps.wml1x180.ma.platformlab.ibm.com/platform/rest/deeplearning/v1/execs/xwmla-1882/log\nNamespace(batch_size=128, cuda=True, epochs=1, lr=0.01)\nUse cuda:  True\nDATA_DIR: /gpfs/mydatafs\nFiles already downloaded and verified\nFiles already downloaded and verified\n=> using pytorch build-in model 'resnet18'\nTotal iterations: 391\nRESULT_DIR: /gpfs/myresultfs/admin/batchworkdir/xwmla-1882\n\nRunning epoch 1 ... It might take several minutes for each epoch to run.\nTrain - batches : 1, average loss: 2.4147, accuracy: 15/128 (12%)\nTrain - batches : 2, average loss: 2.3836, accuracy: 27/256 (11%)\nTrain - batches : 3, average loss: 2.3745, accuracy: 40/384 (10%)\nTrain - batches : 4, average loss: 2.3524, accuracy: 58/512 (11%)\nTrain - batches : 5, average loss: 2.3373, accuracy: 77/640 (12%)\nTrain - batches : 6, average loss: 2.3258, accuracy: 105/768 (14%)\nTrain - batches : 7, average loss: 2.3177, accuracy: 121/896 (14%)\nTrain - batches : 8, average loss: 2.3115, accuracy: 137/1024 (13%)\nTrain - batches : 9, average loss: 2.3087, accuracy: 151/1152 (13%)\nTrain - batches : 10, average loss: 2.3004, accuracy: 176/1280 (14%)\nTrain - batches : 11, average loss: 2.2997, accuracy: 188/1408 (13%)\nTrain - batches : 12, average loss: 2.2921, accuracy: 214/1536 (14%)\nTrain - batches : 13, average loss: 2.2876, accuracy: 230/1664 (14%)\nTrain - batches : 14, average loss: 2.2829, accuracy: 250/1792 (14%)\nTrain - batches : 15, average loss: 2.2766, accuracy: 279/1920 (15%)\nTrain - batches : 16, average loss: 2.2727, accuracy: 301/2048 (15%)\nTrain - batches : 17, average loss: 2.2689, accuracy: 320/2176 (15%)\nTrain - batches : 18, average loss: 2.2613, accuracy: 349/2304 (15%)\nTrain - batches : 19, average loss: 2.2545, accuracy: 381/2432 (16%)\nTrain - batches : 20, average loss: 2.2506, accuracy: 413/2560 (16%)\nTrain - batches : 21, average loss: 2.2448, accuracy: 445/2688 (17%)\nTrain - batches : 22, average loss: 2.2411, accuracy: 472/2816 (17%)\nTrain - batches : 23, average loss: 2.2350, accuracy: 508/2944 (17%)\nTrain - batches : 24, average loss: 2.2317, accuracy: 535/3072 (17%)\nTrain - batches : 25, average loss: 2.2289, accuracy: 562/3200 (18%)\nTrain - batches : 26, average loss: 2.2241, accuracy: 595/3328 (18%)\nTrain - batches : 27, average loss: 2.2188, accuracy: 627/3456 (18%)\nTrain - batches : 28, average loss: 2.2145, accuracy: 660/3584 (18%)\nTrain - batches : 29, average loss: 2.2124, accuracy: 685/3712 (18%)\nTrain - batches : 30, average loss: 2.2077, accuracy: 715/3840 (19%)\nTrain - batches : 31, average loss: 2.2041, accuracy: 746/3968 (19%)\nTrain - batches : 32, average loss: 2.1999, accuracy: 776/4096 (19%)\nTrain - batches : 33, average loss: 2.1937, accuracy: 821/4224 (19%)\nTrain - batches : 34, average loss: 2.1902, accuracy: 858/4352 (20%)\nTrain - batches : 35, average loss: 2.1870, accuracy: 881/4480 (20%)\nTrain - batches : 36, average loss: 2.1841, accuracy: 907/4608 (20%)\nTrain - batches : 37, average loss: 2.1804, accuracy: 945/4736 (20%)\nTrain - batches : 38, average loss: 2.1768, accuracy: 982/4864 (20%)\nTrain - batches : 39, average loss: 2.1737, accuracy: 1024/4992 (21%)\nTrain - batches : 40, average loss: 2.1704, accuracy: 1050/5120 (21%)\nTrain - batches : 41, average loss: 2.1667, accuracy: 1080/5248 (21%)\nTrain - batches : 42, average loss: 2.1650, accuracy: 1123/5376 (21%)\nTrain - batches : 43, average loss: 2.1629, accuracy: 1147/5504 (21%)\nTrain - batches : 44, average loss: 2.1577, accuracy: 1197/5632 (21%)\nTrain - batches : 45, average loss: 2.1537, accuracy: 1230/5760 (21%)\nTrain - batches : 46, average loss: 2.1491, accuracy: 1272/5888 (22%)\nTrain - batches : 47, average loss: 2.1468, accuracy: 1303/6016 (22%)\nTrain - batches : 48, average loss: 2.1422, accuracy: 1342/6144 (22%)\nTrain - batches : 49, average loss: 2.1397, accuracy: 1375/6272 (22%)\nTrain - batches : 50, average loss: 2.1388, accuracy: 1405/6400 (22%)\nTrain - batches : 51, average loss: 2.1355, accuracy: 1443/6528 (22%)\nTrain - batches : 52, average loss: 2.1327, accuracy: 1481/6656 (22%)\nTrain - batches : 53, average loss: 2.1288, accuracy: 1518/6784 (22%)\nTrain - batches : 54, average loss: 2.1245, accuracy: 1556/6912 (23%)\nTrain - batches : 55, average loss: 2.1231, accuracy: 1588/7040 (23%)\nTrain - batches : 56, average loss: 2.1211, accuracy: 1617/7168 (23%)\nTrain - batches : 57, average loss: 2.1192, accuracy: 1649/7296 (23%)\nTrain - batches : 58, average loss: 2.1168, accuracy: 1689/7424 (23%)\nTrain - batches : 59, average loss: 2.1139, accuracy: 1721/7552 (23%)\nTrain - batches : 60, average loss: 2.1112, accuracy: 1753/7680 (23%)\nTrain - batches : 61, average loss: 2.1088, accuracy: 1795/7808 (23%)\nTrain - batches : 62, average loss: 2.1078, accuracy: 1824/7936 (23%)\nTrain - batches : 63, average loss: 2.1044, accuracy: 1867/8064 (23%)\nTrain - batches : 64, average loss: 2.1019, accuracy: 1901/8192 (23%)\nTrain - batches : 65, average loss: 2.0988, accuracy: 1946/8320 (23%)\nTrain - batches : 66, average loss: 2.0965, accuracy: 1982/8448 (23%)\nTrain - batches : 67, average loss: 2.0955, accuracy: 2012/8576 (23%)\nTrain - batches : 68, average loss: 2.0926, accuracy: 2048/8704 (24%)\nTrain - batches : 69, average loss: 2.0893, accuracy: 2088/8832 (24%)\nTrain - batches : 70, average loss: 2.0865, accuracy: 2126/8960 (24%)\nTrain - batches : 71, average loss: 2.0845, accuracy: 2165/9088 (24%)\nTrain - batches : 72, average loss: 2.0815, accuracy: 2211/9216 (24%)\nTrain - batches : 73, average loss: 2.0795, accuracy: 2241/9344 (24%)\nTrain - batches : 74, average loss: 2.0782, accuracy: 2276/9472 (24%)\nTrain - batches : 75, average loss: 2.0754, accuracy: 2324/9600 (24%)\nTrain - batches : 76, average loss: 2.0729, accuracy: 2363/9728 (24%)\nTrain - batches : 77, average loss: 2.0702, accuracy: 2405/9856 (24%)\nTrain - batches : 78, average loss: 2.0695, accuracy: 2433/9984 (24%)\nTrain - batches : 79, average loss: 2.0669, accuracy: 2469/10112 (24%)\nTrain - batches : 80, average loss: 2.0646, accuracy: 2502/10240 (24%)\nTrain - batches : 81, average loss: 2.0615, accuracy: 2541/10368 (25%)\nTrain - batches : 82, average loss: 2.0591, accuracy: 2579/10496 (25%)\nTrain - batches : 83, average loss: 2.0559, accuracy: 2617/10624 (25%)\nTrain - batches : 84, average loss: 2.0533, accuracy: 2662/10752 (25%)\nTrain - batches : 85, average loss: 2.0514, accuracy: 2705/10880 (25%)\nTrain - batches : 86, average loss: 2.0488, accuracy: 2748/11008 (25%)\nTrain - batches : 87, average loss: 2.0468, accuracy: 2783/11136 (25%)\nTrain - batches : 88, average loss: 2.0454, accuracy: 2819/11264 (25%)\nTrain - batches : 89, average loss: 2.0438, accuracy: 2849/11392 (25%)\nTrain - batches : 90, average loss: 2.0416, accuracy: 2889/11520 (25%)\nTrain - batches : 91, average loss: 2.0393, accuracy: 2929/11648 (25%)\nTrain - batches : 92, average loss: 2.0362, accuracy: 2971/11776 (25%)\nTrain - batches : 93, average loss: 2.0343, accuracy: 3005/11904 (25%)\nTrain - batches : 94, average loss: 2.0316, accuracy: 3048/12032 (25%)\nTrain - batches : 95, average loss: 2.0307, accuracy: 3080/12160 (25%)\nTrain - batches : 96, average loss: 2.0280, accuracy: 3120/12288 (25%)\nTrain - batches : 97, average loss: 2.0263, accuracy: 3157/12416 (25%)\nTrain - batches : 98, average loss: 2.0247, accuracy: 3200/12544 (26%)\nTrain - batches : 99, average loss: 2.0224, accuracy: 3248/12672 (26%)\nTrain - batches : 100, average loss: 2.0208, accuracy: 3297/12800 (26%)\nTrain - batches : 101, average loss: 2.0191, accuracy: 3340/12928 (26%)\nTrain - batches : 102, average loss: 2.0172, accuracy: 3377/13056 (26%)\nTrain - batches : 103, average loss: 2.0154, accuracy: 3416/13184 (26%)\nTrain - batches : 104, average loss: 2.0131, accuracy: 3458/13312 (26%)\nTrain - batches : 105, average loss: 2.0122, accuracy: 3491/13440 (26%)\nTrain - batches : 106, average loss: 2.0097, accuracy: 3535/13568 (26%)\nTrain - batches : 107, average loss: 2.0082, accuracy: 3572/13696 (26%)\nTrain - batches : 108, average loss: 2.0070, accuracy: 3613/13824 (26%)\nTrain - batches : 109, average loss: 2.0047, accuracy: 3660/13952 (26%)\nTrain - batches : 110, average loss: 2.0035, accuracy: 3703/14080 (26%)\nTrain - batches : 111, average loss: 2.0018, accuracy: 3756/14208 (26%)\nTrain - batches : 112, average loss: 2.0000, accuracy: 3798/14336 (26%)\nTrain - batches : 113, average loss: 1.9988, accuracy: 3838/14464 (27%)\nTrain - batches : 114, average loss: 1.9970, accuracy: 3882/14592 (27%)\nTrain - batches : 115, average loss: 1.9953, accuracy: 3927/14720 (27%)\nTrain - batches : 116, average loss: 1.9941, accuracy: 3966/14848 (27%)\nTrain - batches : 117, average loss: 1.9925, accuracy: 4014/14976 (27%)\nTrain - batches : 118, average loss: 1.9910, accuracy: 4055/15104 (27%)\nTrain - batches : 119, average loss: 1.9893, accuracy: 4107/15232 (27%)\nTrain - batches : 120, average loss: 1.9884, accuracy: 4148/15360 (27%)\nTrain - batches : 121, average loss: 1.9874, accuracy: 4195/15488 (27%)\nTrain - batches : 122, average loss: 1.9857, accuracy: 4239/15616 (27%)\nTrain - batches : 123, average loss: 1.9850, accuracy: 4285/15744 (27%)\nTrain - batches : 124, average loss: 1.9846, accuracy: 4324/15872 (27%)\nTrain - batches : 125, average loss: 1.9833, accuracy: 4361/16000 (27%)\nTrain - batches : 126, average loss: 1.9820, accuracy: 4395/16128 (27%)\nTrain - batches : 127, average loss: 1.9800, accuracy: 4437/16256 (27%)\nTrain - batches : 128, average loss: 1.9779, accuracy: 4492/16384 (27%)\nTrain - batches : 129, average loss: 1.9763, accuracy: 4542/16512 (28%)\nTrain - batches : 130, average loss: 1.9743, accuracy: 4597/16640 (28%)\nTrain - batches : 131, average loss: 1.9730, accuracy: 4650/16768 (28%)\nTrain - batches : 132, average loss: 1.9708, accuracy: 4698/16896 (28%)\nTrain - batches : 133, average loss: 1.9696, accuracy: 4733/17024 (28%)\nTrain - batches : 134, average loss: 1.9675, accuracy: 4778/17152 (28%)\nTrain - batches : 135, average loss: 1.9666, accuracy: 4821/17280 (28%)\nTrain - batches : 136, average loss: 1.9653, accuracy: 4868/17408 (28%)\nTrain - batches : 137, average loss: 1.9633, accuracy: 4916/17536 (28%)\nTrain - batches : 138, average loss: 1.9627, accuracy: 4958/17664 (28%)\nTrain - batches : 139, average loss: 1.9607, accuracy: 5007/17792 (28%)\nTrain - batches : 140, average loss: 1.9591, accuracy: 5053/17920 (28%)\nTrain - batches : 141, average loss: 1.9572, accuracy: 5103/18048 (28%)\nTrain - batches : 142, average loss: 1.9557, accuracy: 5155/18176 (28%)\nTrain - batches : 143, average loss: 1.9547, accuracy: 5205/18304 (28%)\nTrain - batches : 144, average loss: 1.9535, accuracy: 5239/18432 (28%)\nTrain - batches : 145, average loss: 1.9524, accuracy: 5277/18560 (28%)\nTrain - batches : 146, average loss: 1.9517, accuracy: 5315/18688 (28%)\nTrain - batches : 147, average loss: 1.9507, accuracy: 5359/18816 (28%)\nTrain - batches : 148, average loss: 1.9491, accuracy: 5405/18944 (29%)\nTrain - batches : 149, average loss: 1.9479, accuracy: 5447/19072 (29%)\nTrain - batches : 150, average loss: 1.9459, accuracy: 5485/19200 (29%)\nTrain - batches : 151, average loss: 1.9446, accuracy: 5527/19328 (29%)\nTrain - batches : 152, average loss: 1.9437, accuracy: 5571/19456 (29%)\nTrain - batches : 153, average loss: 1.9430, accuracy: 5610/19584 (29%)\nTrain - batches : 154, average loss: 1.9428, accuracy: 5648/19712 (29%)\nTrain - batches : 155, average loss: 1.9419, accuracy: 5693/19840 (29%)\nTrain - batches : 156, average loss: 1.9417, accuracy: 5730/19968 (29%)\nTrain - batches : 157, average loss: 1.9410, accuracy: 5780/20096 (29%)\nTrain - batches : 158, average loss: 1.9401, accuracy: 5818/20224 (29%)\nTrain - batches : 159, average loss: 1.9390, accuracy: 5862/20352 (29%)\nTrain - batches : 160, average loss: 1.9379, accuracy: 5909/20480 (29%)\nTrain - batches : 161, average loss: 1.9370, accuracy: 5956/20608 (29%)\nTrain - batches : 162, average loss: 1.9359, accuracy: 6003/20736 (29%)\nTrain - batches : 163, average loss: 1.9349, accuracy: 6051/20864 (29%)\nTrain - batches : 164, average loss: 1.9339, accuracy: 6095/20992 (29%)\nTrain - batches : 165, average loss: 1.9325, accuracy: 6144/21120 (29%)\nTrain - batches : 166, average loss: 1.9315, accuracy: 6189/21248 (29%)\nTrain - batches : 167, average loss: 1.9302, accuracy: 6237/21376 (29%)\nTrain - batches : 168, average loss: 1.9298, accuracy: 6279/21504 (29%)\nTrain - batches : 169, average loss: 1.9290, accuracy: 6324/21632 (29%)\nTrain - batches : 170, average loss: 1.9274, accuracy: 6370/21760 (29%)\nTrain - batches : 171, average loss: 1.9262, accuracy: 6418/21888 (29%)\nTrain - batches : 172, average loss: 1.9252, accuracy: 6459/22016 (29%)\nTrain - batches : 173, average loss: 1.9248, accuracy: 6497/22144 (29%)\nTrain - batches : 174, average loss: 1.9239, accuracy: 6545/22272 (29%)\nTrain - batches : 175, average loss: 1.9226, accuracy: 6591/22400 (29%)\nTrain - batches : 176, average loss: 1.9216, accuracy: 6639/22528 (29%)\nTrain - batches : 177, average loss: 1.9209, accuracy: 6677/22656 (29%)\nTrain - batches : 178, average loss: 1.9203, accuracy: 6729/22784 (30%)\nTrain - batches : 179, average loss: 1.9194, accuracy: 6771/22912 (30%)\nTrain - batches : 180, average loss: 1.9180, accuracy: 6825/23040 (30%)\nTrain - batches : 181, average loss: 1.9169, accuracy: 6871/23168 (30%)\nTrain - batches : 182, average loss: 1.9157, accuracy: 6921/23296 (30%)\nTrain - batches : 183, average loss: 1.9151, accuracy: 6960/23424 (30%)\nTrain - batches : 184, average loss: 1.9145, accuracy: 6999/23552 (30%)\nTrain - batches : 185, average loss: 1.9134, accuracy: 7044/23680 (30%)\nTrain - batches : 186, average loss: 1.9121, accuracy: 7091/23808 (30%)\nTrain - batches : 187, average loss: 1.9112, accuracy: 7134/23936 (30%)\nTrain - batches : 188, average loss: 1.9101, accuracy: 7191/24064 (30%)\nTrain - batches : 189, average loss: 1.9090, accuracy: 7240/24192 (30%)\nTrain - batches : 190, average loss: 1.9084, accuracy: 7290/24320 (30%)\nTrain - batches : 191, average loss: 1.9066, accuracy: 7347/24448 (30%)\nTrain - batches : 192, average loss: 1.9058, accuracy: 7390/24576 (30%)\nTrain - batches : 193, average loss: 1.9044, accuracy: 7439/24704 (30%)\nTrain - batches : 194, average loss: 1.9034, accuracy: 7488/24832 (30%)\nTrain - batches : 195, average loss: 1.9024, accuracy: 7541/24960 (30%)\nTrain - batches : 196, average loss: 1.9014, accuracy: 7592/25088 (30%)\nTrain - batches : 197, average loss: 1.9002, accuracy: 7652/25216 (30%)\nTrain - batches : 198, average loss: 1.8996, accuracy: 7704/25344 (30%)\nTrain - batches : 199, average loss: 1.8989, accuracy: 7760/25472 (30%)\nTrain - batches : 200, average loss: 1.8978, accuracy: 7809/25600 (31%)\nTrain - batches : 201, average loss: 1.8965, accuracy: 7861/25728 (31%)\nTrain - batches : 202, average loss: 1.8953, accuracy: 7905/25856 (31%)\nTrain - batches : 203, average loss: 1.8944, accuracy: 7957/25984 (31%)\nTrain - batches : 204, average loss: 1.8935, accuracy: 8009/26112 (31%)\nTrain - batches : 205, average loss: 1.8925, accuracy: 8050/26240 (31%)\nTrain - batches : 206, average loss: 1.8917, accuracy: 8100/26368 (31%)\nTrain - batches : 207, average loss: 1.8908, accuracy: 8148/26496 (31%)\nTrain - batches : 208, average loss: 1.8911, accuracy: 8186/26624 (31%)\nTrain - batches : 209, average loss: 1.8900, accuracy: 8240/26752 (31%)\nTrain - batches : 210, average loss: 1.8894, accuracy: 8283/26880 (31%)\nTrain - batches : 211, average loss: 1.8877, accuracy: 8339/27008 (31%)\nTrain - batches : 212, average loss: 1.8866, accuracy: 8389/27136 (31%)\nTrain - batches : 213, average loss: 1.8860, accuracy: 8437/27264 (31%)\nTrain - batches : 214, average loss: 1.8844, accuracy: 8497/27392 (31%)\nTrain - batches : 215, average loss: 1.8832, accuracy: 8560/27520 (31%)\nTrain - batches : 216, average loss: 1.8820, accuracy: 8616/27648 (31%)\nTrain - batches : 217, average loss: 1.8809, accuracy: 8663/27776 (31%)\nTrain - batches : 218, average loss: 1.8797, accuracy: 8713/27904 (31%)\nTrain - batches : 219, average loss: 1.8794, accuracy: 8756/28032 (31%)\nTrain - batches : 220, average loss: 1.8783, accuracy: 8806/28160 (31%)\nTrain - batches : 221, average loss: 1.8774, accuracy: 8860/28288 (31%)\nTrain - batches : 222, average loss: 1.8762, accuracy: 8908/28416 (31%)\nTrain - batches : 223, average loss: 1.8755, accuracy: 8951/28544 (31%)\nTrain - batches : 224, average loss: 1.8753, accuracy: 8995/28672 (31%)\nTrain - batches : 225, average loss: 1.8748, accuracy: 9040/28800 (31%)\nTrain - batches : 226, average loss: 1.8742, accuracy: 9089/28928 (31%)\nTrain - batches : 227, average loss: 1.8739, accuracy: 9127/29056 (31%)\nTrain - batches : 228, average loss: 1.8728, accuracy: 9185/29184 (31%)\nTrain - batches : 229, average loss: 1.8720, accuracy: 9229/29312 (31%)\nTrain - batches : 230, average loss: 1.8712, accuracy: 9279/29440 (32%)\nTrain - batches : 231, average loss: 1.8705, accuracy: 9321/29568 (32%)\nTrain - batches : 232, average loss: 1.8695, accuracy: 9382/29696 (32%)\nTrain - batches : 233, average loss: 1.8690, accuracy: 9427/29824 (32%)\nTrain - batches : 234, average loss: 1.8683, accuracy: 9476/29952 (32%)\nTrain - batches : 235, average loss: 1.8674, accuracy: 9520/30080 (32%)\nTrain - batches : 236, average loss: 1.8664, accuracy: 9565/30208 (32%)\nTrain - batches : 237, average loss: 1.8651, accuracy: 9620/30336 (32%)\nTrain - batches : 238, average loss: 1.8641, accuracy: 9676/30464 (32%)\nTrain - batches : 239, average loss: 1.8633, accuracy: 9724/30592 (32%)\nTrain - batches : 240, average loss: 1.8628, accuracy: 9765/30720 (32%)\nTrain - batches : 241, average loss: 1.8624, accuracy: 9815/30848 (32%)\nTrain - batches : 242, average loss: 1.8619, accuracy: 9862/30976 (32%)\nTrain - batches : 243, average loss: 1.8609, accuracy: 9908/31104 (32%)\nTrain - batches : 244, average loss: 1.8598, accuracy: 9956/31232 (32%)\nTrain - batches : 245, average loss: 1.8589, accuracy: 10005/31360 (32%)\nTrain - batches : 246, average loss: 1.8582, accuracy: 10052/31488 (32%)\nTrain - batches : 247, average loss: 1.8577, accuracy: 10102/31616 (32%)\nTrain - batches : 248, average loss: 1.8572, accuracy: 10150/31744 (32%)\nTrain - batches : 249, average loss: 1.8563, accuracy: 10198/31872 (32%)\nTrain - batches : 250, average loss: 1.8557, accuracy: 10237/32000 (32%)\nTrain - batches : 251, average loss: 1.8548, accuracy: 10286/32128 (32%)\nTrain - batches : 252, average loss: 1.8540, accuracy: 10334/32256 (32%)\nTrain - batches : 253, average loss: 1.8528, accuracy: 10387/32384 (32%)\nTrain - batches : 254, average loss: 1.8518, accuracy: 10434/32512 (32%)\nTrain - batches : 255, average loss: 1.8509, accuracy: 10486/32640 (32%)\nTrain - batches : 256, average loss: 1.8503, accuracy: 10537/32768 (32%)\nTrain - batches : 257, average loss: 1.8490, accuracy: 10594/32896 (32%)\nTrain - batches : 258, average loss: 1.8487, accuracy: 10640/33024 (32%)\nTrain - batches : 259, average loss: 1.8480, accuracy: 10694/33152 (32%)\nTrain - batches : 260, average loss: 1.8473, accuracy: 10741/33280 (32%)\nTrain - batches : 261, average loss: 1.8463, accuracy: 10807/33408 (32%)\nTrain - batches : 262, average loss: 1.8455, accuracy: 10858/33536 (32%)\nTrain - batches : 263, average loss: 1.8449, accuracy: 10910/33664 (32%)\nTrain - batches : 264, average loss: 1.8442, accuracy: 10956/33792 (32%)\nTrain - batches : 265, average loss: 1.8434, accuracy: 11000/33920 (32%)\nTrain - batches : 266, average loss: 1.8427, accuracy: 11050/34048 (32%)\nTrain - batches : 267, average loss: 1.8421, accuracy: 11099/34176 (32%)\nTrain - batches : 268, average loss: 1.8412, accuracy: 11155/34304 (33%)\nTrain - batches : 269, average loss: 1.8402, accuracy: 11207/34432 (33%)\nTrain - batches : 270, average loss: 1.8393, accuracy: 11260/34560 (33%)\nTrain - batches : 271, average loss: 1.8392, accuracy: 11306/34688 (33%)\nTrain - batches : 272, average loss: 1.8382, accuracy: 11369/34816 (33%)\nTrain - batches : 273, average loss: 1.8375, accuracy: 11410/34944 (33%)\nTrain - batches : 274, average loss: 1.8365, accuracy: 11463/35072 (33%)\nTrain - batches : 275, average loss: 1.8360, accuracy: 11510/35200 (33%)\nTrain - batches : 276, average loss: 1.8356, accuracy: 11555/35328 (33%)\nTrain - batches : 277, average loss: 1.8348, accuracy: 11608/35456 (33%)\nTrain - batches : 278, average loss: 1.8342, accuracy: 11662/35584 (33%)\nTrain - batches : 279, average loss: 1.8333, accuracy: 11714/35712 (33%)\nTrain - batches : 280, average loss: 1.8328, accuracy: 11755/35840 (33%)\nTrain - batches : 281, average loss: 1.8320, accuracy: 11802/35968 (33%)\nTrain - batches : 282, average loss: 1.8312, accuracy: 11857/36096 (33%)\nTrain - batches : 283, average loss: 1.8306, accuracy: 11914/36224 (33%)\nTrain - batches : 284, average loss: 1.8300, accuracy: 11964/36352 (33%)\nTrain - batches : 285, average loss: 1.8290, accuracy: 12019/36480 (33%)\nTrain - batches : 286, average loss: 1.8284, accuracy: 12069/36608 (33%)\nTrain - batches : 287, average loss: 1.8275, accuracy: 12123/36736 (33%)\nTrain - batches : 288, average loss: 1.8268, accuracy: 12172/36864 (33%)\nTrain - batches : 289, average loss: 1.8265, accuracy: 12212/36992 (33%)\nTrain - batches : 290, average loss: 1.8255, accuracy: 12270/37120 (33%)\nTrain - batches : 291, average loss: 1.8249, accuracy: 12325/37248 (33%)\nTrain - batches : 292, average loss: 1.8236, accuracy: 12387/37376 (33%)\nTrain - batches : 293, average loss: 1.8227, accuracy: 12446/37504 (33%)\nTrain - batches : 294, average loss: 1.8223, accuracy: 12492/37632 (33%)\nTrain - batches : 295, average loss: 1.8214, accuracy: 12547/37760 (33%)\nTrain - batches : 296, average loss: 1.8205, accuracy: 12605/37888 (33%)\nTrain - batches : 297, average loss: 1.8204, accuracy: 12639/38016 (33%)\nTrain - batches : 298, average loss: 1.8201, accuracy: 12683/38144 (33%)\nTrain - batches : 299, average loss: 1.8195, accuracy: 12736/38272 (33%)\nTrain - batches : 300, average loss: 1.8187, accuracy: 12787/38400 (33%)\nTrain - batches : 301, average loss: 1.8183, accuracy: 12835/38528 (33%)\nTrain - batches : 302, average loss: 1.8176, accuracy: 12884/38656 (33%)\nTrain - batches : 303, average loss: 1.8168, accuracy: 12945/38784 (33%)\nTrain - batches : 304, average loss: 1.8163, accuracy: 12990/38912 (33%)\nTrain - batches : 305, average loss: 1.8158, accuracy: 13033/39040 (33%)\nTrain - batches : 306, average loss: 1.8153, accuracy: 13075/39168 (33%)\nTrain - batches : 307, average loss: 1.8146, accuracy: 13128/39296 (33%)\nTrain - batches : 308, average loss: 1.8136, accuracy: 13189/39424 (33%)\nTrain - batches : 309, average loss: 1.8127, accuracy: 13245/39552 (33%)\nTrain - batches : 310, average loss: 1.8120, accuracy: 13299/39680 (34%)\nTrain - batches : 311, average loss: 1.8114, accuracy: 13352/39808 (34%)\nTrain - batches : 312, average loss: 1.8106, accuracy: 13407/39936 (34%)\nTrain - batches : 313, average loss: 1.8097, accuracy: 13452/40064 (34%)\nTrain - batches : 314, average loss: 1.8087, accuracy: 13513/40192 (34%)\nTrain - batches : 315, average loss: 1.8077, accuracy: 13568/40320 (34%)\nTrain - batches : 316, average loss: 1.8069, accuracy: 13624/40448 (34%)\nTrain - batches : 317, average loss: 1.8065, accuracy: 13668/40576 (34%)\nTrain - batches : 318, average loss: 1.8062, accuracy: 13723/40704 (34%)\nTrain - batches : 319, average loss: 1.8056, accuracy: 13778/40832 (34%)\nTrain - batches : 320, average loss: 1.8050, accuracy: 13826/40960 (34%)\nTrain - batches : 321, average loss: 1.8047, accuracy: 13869/41088 (34%)\nTrain - batches : 322, average loss: 1.8042, accuracy: 13914/41216 (34%)\nTrain - batches : 323, average loss: 1.8038, accuracy: 13957/41344 (34%)\nTrain - batches : 324, average loss: 1.8031, accuracy: 14010/41472 (34%)\nTrain - batches : 325, average loss: 1.8027, accuracy: 14065/41600 (34%)\nTrain - batches : 326, average loss: 1.8020, accuracy: 14120/41728 (34%)\nTrain - batches : 327, average loss: 1.8016, accuracy: 14164/41856 (34%)\nTrain - batches : 328, average loss: 1.8013, accuracy: 14215/41984 (34%)\nTrain - batches : 329, average loss: 1.8006, accuracy: 14276/42112 (34%)\nTrain - batches : 330, average loss: 1.7997, accuracy: 14340/42240 (34%)\nTrain - batches : 331, average loss: 1.7991, accuracy: 14389/42368 (34%)\nTrain - batches : 332, average loss: 1.7981, accuracy: 14443/42496 (34%)\nTrain - batches : 333, average loss: 1.7976, accuracy: 14492/42624 (34%)\nTrain - batches : 334, average loss: 1.7969, accuracy: 14545/42752 (34%)\nTrain - batches : 335, average loss: 1.7957, accuracy: 14602/42880 (34%)\nTrain - batches : 336, average loss: 1.7948, accuracy: 14661/43008 (34%)\nTrain - batches : 337, average loss: 1.7940, accuracy: 14722/43136 (34%)\nTrain - batches : 338, average loss: 1.7939, accuracy: 14769/43264 (34%)\nTrain - batches : 339, average loss: 1.7933, accuracy: 14825/43392 (34%)\nTrain - batches : 340, average loss: 1.7928, accuracy: 14876/43520 (34%)\nTrain - batches : 341, average loss: 1.7922, accuracy: 14928/43648 (34%)\nTrain - batches : 342, average loss: 1.7913, accuracy: 14988/43776 (34%)\nTrain - batches : 343, average loss: 1.7912, accuracy: 15033/43904 (34%)\nTrain - batches : 344, average loss: 1.7909, accuracy: 15075/44032 (34%)\nTrain - batches : 345, average loss: 1.7904, accuracy: 15123/44160 (34%)\nTrain - batches : 346, average loss: 1.7899, accuracy: 15175/44288 (34%)\nTrain - batches : 347, average loss: 1.7895, accuracy: 15225/44416 (34%)\nTrain - batches : 348, average loss: 1.7890, accuracy: 15280/44544 (34%)\nTrain - batches : 349, average loss: 1.7887, accuracy: 15326/44672 (34%)\nTrain - batches : 350, average loss: 1.7882, accuracy: 15383/44800 (34%)\nTrain - batches : 351, average loss: 1.7876, accuracy: 15444/44928 (34%)\nTrain - batches : 352, average loss: 1.7867, accuracy: 15505/45056 (34%)\nTrain - batches : 353, average loss: 1.7861, accuracy: 15550/45184 (34%)\nTrain - batches : 354, average loss: 1.7854, accuracy: 15600/45312 (34%)\nTrain - batches : 355, average loss: 1.7850, accuracy: 15645/45440 (34%)\nTrain - batches : 356, average loss: 1.7840, accuracy: 15701/45568 (34%)\nTrain - batches : 357, average loss: 1.7832, accuracy: 15753/45696 (34%)\nTrain - batches : 358, average loss: 1.7824, accuracy: 15819/45824 (35%)\nTrain - batches : 359, average loss: 1.7817, accuracy: 15873/45952 (35%)\nTrain - batches : 360, average loss: 1.7810, accuracy: 15925/46080 (35%)\nTrain - batches : 361, average loss: 1.7804, accuracy: 15981/46208 (35%)\nTrain - batches : 362, average loss: 1.7800, accuracy: 16037/46336 (35%)\nTrain - batches : 363, average loss: 1.7791, accuracy: 16095/46464 (35%)\nTrain - batches : 364, average loss: 1.7784, accuracy: 16148/46592 (35%)\nTrain - batches : 365, average loss: 1.7780, accuracy: 16200/46720 (35%)\nTrain - batches : 366, average loss: 1.7776, accuracy: 16245/46848 (35%)\nTrain - batches : 367, average loss: 1.7769, accuracy: 16302/46976 (35%)\nTrain - batches : 368, average loss: 1.7763, accuracy: 16359/47104 (35%)\nTrain - batches : 369, average loss: 1.7756, accuracy: 16423/47232 (35%)\nTrain - batches : 370, average loss: 1.7749, accuracy: 16477/47360 (35%)\nTrain - batches : 371, average loss: 1.7741, accuracy: 16538/47488 (35%)\nTrain - batches : 372, average loss: 1.7733, accuracy: 16595/47616 (35%)\nTrain - batches : 373, average loss: 1.7730, accuracy: 16643/47744 (35%)\nTrain - batches : 374, average loss: 1.7725, accuracy: 16698/47872 (35%)\nTrain - batches : 375, average loss: 1.7720, accuracy: 16752/48000 (35%)\nTrain - batches : 376, average loss: 1.7714, accuracy: 16806/48128 (35%)\nTrain - batches : 377, average loss: 1.7711, accuracy: 16861/48256 (35%)\nTrain - batches : 378, average loss: 1.7708, accuracy: 16906/48384 (35%)\nTrain - batches : 379, average loss: 1.7702, accuracy: 16957/48512 (35%)\nTrain - batches : 380, average loss: 1.7699, accuracy: 17007/48640 (35%)\nTrain - batches : 381, average loss: 1.7693, accuracy: 17062/48768 (35%)\nTrain - batches : 382, average loss: 1.7691, accuracy: 17113/48896 (35%)\nTrain - batches : 383, average loss: 1.7685, accuracy: 17170/49024 (35%)\nTrain - batches : 384, average loss: 1.7680, accuracy: 17223/49152 (35%)\nTrain - batches : 385, average loss: 1.7677, accuracy: 17275/49280 (35%)\nTrain - batches : 386, average loss: 1.7672, accuracy: 17329/49408 (35%)\nTrain - batches : 387, average loss: 1.7666, accuracy: 17388/49536 (35%)\nTrain - batches : 388, average loss: 1.7660, accuracy: 17440/49664 (35%)\nTrain - batches : 389, average loss: 1.7655, accuracy: 17496/49792 (35%)\nTrain - batches : 390, average loss: 1.7651, accuracy: 17544/49920 (35%)\nTrain - batches : 391, average loss: 1.7644, accuracy: 17583/50000 (35%)\nTest - batches: 391, average loss: 0.0138, accuracy: 3796/10000 (38%)\n\n\n", "name": "stdout"}]}, {"metadata": {"id": "132ffd64d79c4b81837a565291dc67da"}, "cell_type": "markdown", "source": "#### Retrieve and display the model training logs:"}, {"metadata": {"id": "87c7cd6ac5d64afa8947783fb5f854e5"}, "cell_type": "code", "source": "query_executor_stdout_log(r.json())", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "https://wmla-console-xwmla.apps.wml1x180.ma.platformlab.ibm.com/platform/rest/deeplearning/v1/scheduler/applications/xwmla-1882/executor/1/logs/stdout?lastlines=1000\n*Task <1> SubProcess*: 2021-02-08 21:17:37.197660 37 INFO Create log direcotry /wmla-logging/dli/xwmla-1882/dli/./app.xwmla-1882-task12n-nxl8r\n*Task <1> SubProcess*: 2021-02-08 21:17:37.202192 37 INFO Running on kubernetes.\n*Task <1> SubProcess*: 2021-02-08 21:17:37.208179 37 INFO List GPUs\n*Task <1> SubProcess*: Mon Feb  8 21:17:37 2021       \n*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n*Task <1> SubProcess*: | NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\n*Task <1> SubProcess*: |-------------------------------+----------------------+----------------------+\n*Task <1> SubProcess*: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n*Task <1> SubProcess*: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n*Task <1> SubProcess*: |===============================+======================+======================|\n*Task <1> SubProcess*: |   0  Tesla V100-PCIE...  On   | 00000000:06:00.0 Off |                  Off |\n*Task <1> SubProcess*: | N/A   40C    P0    27W / 250W |      0MiB / 16160MiB |      0%      Default |\n*Task <1> SubProcess*: +-------------------------------+----------------------+----------------------+\n*Task <1> SubProcess*:                                                                                \n*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n*Task <1> SubProcess*: | Processes:                                                       GPU Memory |\n*Task <1> SubProcess*: |  GPU       PID   Type   Process name                             Usage      |\n*Task <1> SubProcess*: |=============================================================================|\n*Task <1> SubProcess*: |  No running processes found                                                 |\n*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n*Task <1> SubProcess*: 2021-02-08 21:17:37.270456 37 INFO NVIDIA_VISIBLE_DEVICES=GPU-2c33df03-e129-bd55-5ff2-1288461e4b44\n*Task <1> SubProcess*: 2021-02-08 21:17:37.274846 37 INFO NVIDIA_DRIVER_CAPABILITIES=compute,utility\n*Task <1> SubProcess*: 2021-02-08 21:17:37.288584 37 INFO Init enviroment with /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/spark-env.sh\n*Task <1> SubProcess*: 2021-02-08 21:17:37.292942 37 INFO Enter into spark-env.sh\n*Task <1> SubProcess*: 2021-02-08 21:17:37.301246 37 INFO Path variables: WMLA_TOP=/opt/ibm/spectrumcomputing, FABRIC_HOME=/opt/ibm/spectrumcomputing/dli/2.2.0/fabric, DLI_DATA_FS=/gpfs/mydatafs, DLI_RESULT_FS=/gpfs/myresultfs, DLI_DEFAULT_CONDA_HOME=/opt/anaconda3, DLI_DEFAULT_CONDA_ENV_NAME=dlipy3\n*Task <1> SubProcess*: 2021-02-08 21:17:37.316275 37 INFO Setup environment on node xwmla-1882-task12n-nxl8r\n*Task <1> SubProcess*: 2021-02-08 21:17:37.748078 37 INFO Succeed to activate conda environment dlipy3\n*Task <1> SubProcess*: 2021-02-08 21:17:37.759599 37 INFO Conda variables: conda_home=/opt/anaconda3, conda_env=dlipy3, gpu_enabled=Y, python=/opt/anaconda3/envs/dlipy3/bin/python\n*Task <1> SubProcess*: 2021-02-08 21:17:37.763974 37 INFO Check python version\n*Task <1> SubProcess*: 3.7.9 (default, Aug 31 2020, 12:42:55) \n*Task <1> SubProcess*: [GCC 7.3.0]\n*Task <1> SubProcess*: 2021-02-08 21:17:37.865779 37 INFO DLI_LOGGER_LEVEL=info\n*Task <1> SubProcess*: 2021-02-08 21:17:37.872677 37 INFO Trace envrionment\n*Task <1> SubProcess*: CUDA_VISIBLE_DEVICES=GPU-2c33df03-e129-bd55-5ff2-1288461e4b44\n*Task <1> SubProcess*: PATH=/opt/anaconda3/envs/dlipy3/bin:/opt/anaconda3/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/anaconda3/bin\n*Task <1> SubProcess*: PYTHONPATH=/opt/wmla-mk/sdk/python::/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dli_utils/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dataset/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/compression/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/py4j-0.10.4-src.zip:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/pyspark.zip:/gpfs/myresultfs/admin:/gpfs/myresultfs/admin/lib/python3.7/site-packages\n*Task <1> SubProcess*: LD_LIBRARY_PATH=/opt/wmla-mk/sdk/python:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/anaconda3/envs/dlipy3/cuda/lib::/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core:/opt/wmla-mk/sdk/python:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/anaconda3/envs/dlipy3/cuda/lib::/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core:/opt/anaconda3/envs/dlipy3/lib\n*Task <1> SubProcess*: WMLA_TOP=/opt/ibm/spectrumcomputing\n*Task <1> SubProcess*: DLI_DATA_FS=/gpfs/mydatafs\n*Task <1> SubProcess*: DLI_RESULT_FS=/gpfs/myresultfs\n*Task <1> SubProcess*: IS_K8S=ON\n*Task <1> SubProcess*: IS_TASK0=\n*Task <1> SubProcess*: DLI_EXECID=xwmla-1882\n*Task <1> SubProcess*: DLI_WORK_DIR=/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code\n*Task <1> SubProcess*: DLI_CS_DATASTORE_META=type=fs\n*Task <1> SubProcess*: FRAME_WORK=PyTorch\n*Task <1> SubProcess*: EXEC_TYPE=\n*Task <1> SubProcess*: DLI_IS_ELASTIC=false\n*Task <1> SubProcess*: FABRIC_HOME=/opt/ibm/spectrumcomputing/dli/2.2.0/fabric\n*Task <1> SubProcess*: NCCL_SOCKET_IFNAME=\n*Task <1> SubProcess*: NCCL_P2P_DISABLE=\n*Task <1> SubProcess*: DLIM_MK_COND_HOME=\n*Task <1> SubProcess*: HPO_PLUGIN_CONDA_HOME=\n*Task <1> SubProcess*: DLI_CONDA_HOME=\n*Task <1> SubProcess*: DLI_DEFAULT_CONDA_HOME=/opt/anaconda3\n*Task <1> SubProcess*: DLIM_MK_COND_ENV_NAME=\n*Task <1> SubProcess*: HPO_PLUGIN_CONDA_ENV=\n*Task <1> SubProcess*: DLI_CONDA_ENV_NAME=\n*Task <1> SubProcess*: DLI_DEFAULT_CONDA_ENV_NAME=dlipy3\n*Task <1> SubProcess*: DLI_SC_RUNASUSER=1000620000\n*Task <1> SubProcess*: DLI_SC_RUNASGROUP=1000620000\n*Task <1> SubProcess*: DLI_SC_FSGROUP=1000620000\n*Task <1> SubProcess*: DLI_SUBMIT_USERNAME=admin\n*Task <1> SubProcess*: CURRENT_RUNASUSER=1000620000\n*Task <1> SubProcess*: CURRENT_RUNASGROUP=1000620000\n*Task <1> SubProcess*: DATA_DIR=\n*Task <1> SubProcess*: RESULT_DIR=\n*Task <1> SubProcess*: LOG_DIR=\n*Task <1> SubProcess*: SAVED_MODEL_DIR=\n*Task <1> SubProcess*: CHECKPOINT_DIR=\n*Task <1> SubProcess*: DLI_LOGGER_LEVEL=info\n*Task <1> SubProcess*: APP_ID=xwmla-1882\n*Task <1> SubProcess*: SPACE_ID=\n*Task <1> SubProcess*: 2021-02-08 21:17:37.879339 37 INFO Finish environment setup.\n*Task <1> SubProcess*: 2021-02-08 21:17:37.884120 37 INFO Entering working directory /gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code\n*Task <1> SubProcess*: 2021-02-08 21:17:37.889363 37 INFO Execute command: /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/launcher/launcher.py --exec_mode=single --work_dir=/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code --app_type=executable --model=/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dl_plugins/pytorch_wrapper.sh -- /gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code/main.py --epochs 1\n*Task <1> SubProcess*: 2021-02-08 21:17:37.893912 37 INFO Job log files under /wmla-logging/dli/xwmla-1882/dli/./app.xwmla-1882-task12n-nxl8r\n*Task <1> SubProcess*: /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/launcher/launcher.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n*Task <1> SubProcess*:   from imp import find_module\n*Task <1> SubProcess*: DLI_LOGGER_LEVEL=info\n*Task <1> SubProcess*: 2021-02-08 21:17:38.003536 308 {'options': <Values at 0x7f28d617a9d0: {'app': '/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dl_plugins/pytorch_wrapper.sh', 'num_of_worker_hosts': 1, 'num_of_ps_hosts': 1, 'customized_port': 0, 'app_type': 'executable', 'work_dir': '/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code', 'exec_engine': 'msd', 'exec_mode': 'single'}>, 'args': ['/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code/main.py', '--epochs', '1']}\n*Task <1> SubProcess*: 2021-02-08 21:17:38.003606 308 wrap_app is:  /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dl_plugins/pytorch_wrapper.sh\n*Task <1> SubProcess*: 2021-02-08 21:17:38.003699 308 calling \"on_task_invoke\"\n*Task <1> SubProcess*: 2021-02-08 21:17:38.003729 308 callback on_task_invoke_begin is started\n*Task <1> SubProcess*: 2021-02-08 21:17:38.134055 308 callback on_task_invoke_begin is finished\n*Task <1> SubProcess*: DLI_LOGGER_LEVEL=info\n*Task <1> SubProcess*: 2021-02-08 21:17:38.141044 308 load file: executable, /opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/launcher/plugins/executable.py\n*Task <1> SubProcess*: 2021-02-08 21:17:38.141574 308 find class: executable.Executable\n*Task <1> SubProcess*: 2021-02-08 21:17:38.145368 308 Original id is 1, cluster detail is 10.129.2.147\n*Task <1> SubProcess*: 2021-02-08 21:17:38.145403 308 The current task id is 0\n*Task <1> SubProcess*: 2021-02-08 21:17:38.145420 308 partition index: 0, host: 10.129.2.147 \n*Task <1> SubProcess*: 2021-02-08 21:17:38.145485 308 f=/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dl_plugins/pytorch_wrapper.sh &&                   if [ ! -x $f ] && head -n1 $f | grep -sq \"^#!\"; then chmod a+x $f || true; fi &&                   if [ $(ls -l $f | awk '{print $1}'|grep x |wc -l) -gt 0 ] && [ -x $f ]; then p=env; else p=python; fi && $p $f  /gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code/main.py --epochs 1\n*Task <1> SubProcess*: 2021-02-08 21:17:38.145501 308 calling \"buildCmd\"\n*Task <1> SubProcess*: WMLA_EDI_PORT_9000_TCP_PORT=9000\n*Task <1> SubProcess*: WMLA_AUTH_REST_SERVICE_HOST=172.30.145.203\n*Task <1> SubProcess*: WMLA_AUTH_REST_SERVICE_PORT_HTTPS=3000\n*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT_9090_TCP=tcp://172.30.1.98:9090\n*Task <1> SubProcess*: EMETRICS_STREAMING=ON\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT_8001_TCP_ADDR=172.30.39.248\n*Task <1> SubProcess*: WMLA_DLPD_PORT_9243_TCP_PORT=9243\n*Task <1> SubProcess*: HOSTNAME=xwmla-1882-task12n-nxl8r\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_SERVICE_HOST=172.30.39.248\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_89_TCP_PORT=89\n*Task <1> SubProcess*: WMLA_INGRESS_PORT_30746_TCP_PORT=30746\n*Task <1> SubProcess*: WMLA_EDI_PORT_9010_TCP_ADDR=172.30.50.232\n*Task <1> SubProcess*: KUBERNETES_PORT_443_TCP_PORT=443\n*Task <1> SubProcess*: WMLA_EDI_PORT_9010_TCP_PROTO=tcp\n*Task <1> SubProcess*: KUBERNETES_PORT=tcp://172.30.0.1:443\n*Task <1> SubProcess*: TERM=xterm-256color\n*Task <1> SubProcess*: WMLA_JUPYTER_HUB_SERVICE_PORT=8081\n*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_SERVICE_PORT=5043\n*Task <1> SubProcess*: WMLA_MSS_PORT_9080_TCP=tcp://172.30.106.213:9080\n*Task <1> SubProcess*: CONDA_SHLVL=1\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_89_TCP=tcp://172.30.214.57:89\n*Task <1> SubProcess*: WMLA_INFERENCE_SERVICE_PORT_REST=9000\n*Task <1> SubProcess*: WMLA_INGRESS_PORT_30746_TCP=tcp://172.30.182.101:30746\n*Task <1> SubProcess*: WMLA_EDI_SERVICE_PORT_ADMIN=8889\n*Task <1> SubProcess*: KUBERNETES_SERVICE_PORT=443\n*Task <1> SubProcess*: PYTHONUNBUFFERED=x\n*Task <1> SubProcess*: CONDA_PROMPT_MODIFIER=(dlipy3) \n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT_8001_TCP=tcp://172.30.39.248:8001\n*Task <1> SubProcess*: WMLA_GUI_PORT_8443_TCP_ADDR=172.30.119.116\n*Task <1> SubProcess*: LOG_FILE_PATH=/wmla-logging/dli/xwmla-1882/xwmla-1882-task12n-nxl8r\n*Task <1> SubProcess*: TF_INCLUDE_DIR=/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core/include\n*Task <1> SubProcess*: OLDPWD=/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code\n*Task <1> SubProcess*: WMLA_GUI_SERVICE_PORT_HTTP=8443\n*Task <1> SubProcess*: WMLA_ETCD_PORT_2379_TCP_ADDR=172.30.127.95\n*Task <1> SubProcess*: WMLA_ETCD_SERVICE_PORT=2379\n*Task <1> SubProcess*: WMLA_INFERENCE_PORT_9000_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT_8888_TCP=tcp://172.30.205.170:8888\n*Task <1> SubProcess*: WMLA_MSS_PORT_9080_TCP_PORT=9080\n*Task <1> SubProcess*: KUBERNETES_SERVICE_HOST=172.30.0.1\n*Task <1> SubProcess*: WMLA_INGRESS_SERVICE_PORT=30746\n*Task <1> SubProcess*: DLI_OBJECT_NAME=xwmla-1882\n*Task <1> SubProcess*: WMLA_ETCD_SERVICE_PORT_CLIENT=2379\n*Task <1> SubProcess*: WMLA_GUI_SERVICE_HOST=172.30.119.116\n*Task <1> SubProcess*: WMLA_DLPD_PORT_27017_TCP_PORT=27017\n*Task <1> SubProcess*: WMLA_DLPD_PORT_9243_TCP_ADDR=172.30.2.247\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_SERVICE_HOST=172.30.214.57\n*Task <1> SubProcess*: DLI_LOGGER_LEVEL=info\n*Task <1> SubProcess*: CAFFE_BIN=/opt/anaconda3/envs/dlipy3/bin\n*Task <1> SubProcess*: DLI_VERSION=2.2.0\n*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT_8081_TCP_PROTO=tcp\n*Task <1> SubProcess*: DLI_EXECID=xwmla-1882\n*Task <1> SubProcess*: MSD_MK_CONN_TIMEOUT=15\n*Task <1> SubProcess*: MSD_POD_IP=10.129.2.147\n*Task <1> SubProcess*: TASK12N_DEVICE_TYPE=ngpus\n*Task <1> SubProcess*: PYTHONUSERBASE=/gpfs/myresultfs/admin\n*Task <1> SubProcess*: TASK_ID=1\n*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT=tcp://172.30.104.57:8892\n*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT_8888_TCP_PORT=8888\n*Task <1> SubProcess*: OPAL_PREFIX=/opt/anaconda3/envs/dlipy3\n*Task <1> SubProcess*: LD_LIBRARY_PATH=/opt/wmla-mk/sdk/python:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/anaconda3/envs/dlipy3/cuda/lib::/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core:/opt/wmla-mk/sdk/python:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/anaconda3/envs/dlipy3/cuda/lib::/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core:/opt/anaconda3/envs/dlipy3/lib\n*Task <1> SubProcess*: WMLA_INFERENCE_SERVICE_HOST=172.30.188.165\n*Task <1> SubProcess*: WMLA_EDI_PORT_8889_TCP_PORT=8889\n*Task <1> SubProcess*: WMLA_ETCD_PORT_2379_TCP=tcp://172.30.127.95:2379\n*Task <1> SubProcess*: WMLA_INGRESS_SERVICE_PORT_HTTPS_PROXY=30746\n*Task <1> SubProcess*: NVIDIA_VISIBLE_DEVICES=GPU-2c33df03-e129-bd55-5ff2-1288461e4b44\n*Task <1> SubProcess*: MK_SERVER_ADDRESS=127.0.0.1:42789\n*Task <1> SubProcess*: WMLA_MSS_SERVICE_PORT=9080\n*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT_8081_TCP_ADDR=172.30.95.180\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_SERVICE_PORT_HTTP=89\n*Task <1> SubProcess*: DLI_WORK_DIR=/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code\n*Task <1> SubProcess*: WMLA_MSD_PORT_10000_TCP_PORT=10000\n*Task <1> SubProcess*: WMLA_ETCD_PORT=tcp://172.30.127.95:2379\n*Task <1> SubProcess*: WMLA_DLPD_PORT_27017_TCP=tcp://172.30.2.247:27017\n*Task <1> SubProcess*: CONDA_EXE=/opt/anaconda3/bin/conda\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_493_TCP_ADDR=172.30.214.57\n*Task <1> SubProcess*: WMLA_EDI_SERVICE_PORT_STREAM=9010\n*Task <1> SubProcess*: WMLA_INFERENCE_SERVICE_PORT=9000\n*Task <1> SubProcess*: WMLA_MSD_PORT_10000_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_PROMETHEUS_SERVICE_PORT_PROMUI=9090\n*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_SERVICE_HOST=172.30.205.170\n*Task <1> SubProcess*: WMLA_EDI_PORT_9000_TCP=tcp://172.30.50.232:9000\n*Task <1> SubProcess*: WMLA_MSS_PORT_9080_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_EDI_PORT_8889_TCP_ADDR=172.30.50.232\n*Task <1> SubProcess*: MSD_POD_NAME=xwmla-1882-task12n-nxl8r\n*Task <1> SubProcess*: DLI_DATA_FS=/gpfs/mydatafs\n*Task <1> SubProcess*: WMLA_INFERENCE_PORT_9000_TCP=tcp://172.30.188.165:9000\n*Task <1> SubProcess*: WMLA_DLPD_PORT_27017_TCP_PROTO=tcp\n*Task <1> SubProcess*: NVIDIA_DRIVER_CAPABILITIES=compute,utility\n*Task <1> SubProcess*: DLI_SUBMIT_USERNAME=admin\n*Task <1> SubProcess*: WMLA_GRAFANA_PORT_3000_TCP=tcp://172.30.164.114:3000\n*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT_8888_TCP_ADDR=172.30.205.170\n*Task <1> SubProcess*: WMLA_MSD_PORT_9080_TCP_PROTO=tcp\n*Task <1> SubProcess*: POWERAI_SAVE_OMP_NUM_THREADS=16\n*Task <1> SubProcess*: WMLA_INFOSERVICE_SERVICE_HOST=172.30.104.57\n*Task <1> SubProcess*: CLUSTERADMIN=wmla\n*Task <1> SubProcess*: WMLA_AUTH_REST_PORT=tcp://172.30.145.203:3000\n*Task <1> SubProcess*: WMLA_GRAFANA_SERVICE_PORT_GRAFANA=3000\n*Task <1> SubProcess*: WMLA_GRAFANA_PORT_3000_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_AUTH_REST_PORT_3000_TCP_ADDR=172.30.145.203\n*Task <1> SubProcess*: DLI_DEFAULT_CONDA_HOME=/opt/anaconda3\n*Task <1> SubProcess*: _CE_CONDA=\n*Task <1> SubProcess*: TASK_JOB_ID=xwmla-1882\n*Task <1> SubProcess*: WMLA_ETCD_PORT_2379_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_DLPD_SERVICE_HOST=172.30.2.247\n*Task <1> SubProcess*: WMLA_EDI_PORT_9010_TCP_PORT=9010\n*Task <1> SubProcess*: PATH=/opt/anaconda3/envs/dlipy3/bin:/opt/anaconda3/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/anaconda3/bin\n*Task <1> SubProcess*: WMLA_PROMETHEUS_SERVICE_PORT=9090\n*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT_9090_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_GUI_PORT_8443_TCP_PORT=8443\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT_8001_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_EDI_PORT_9000_TCP_ADDR=172.30.50.232\n*Task <1> SubProcess*: MSD_POD_AFFINITY_RULE=preferred\n*Task <1> SubProcess*: DLIMSD_LOG_DIR=/wmla-logging/dli/xwmla-1882/dli/./app.xwmla-1882-task12n-nxl8r\n*Task <1> SubProcess*: CONDA_PREFIX=/opt/anaconda3/envs/dlipy3\n*Task <1> SubProcess*: PWD=/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code\n*Task <1> SubProcess*: WMLA_TOP=/opt/ibm/spectrumcomputing\n*Task <1> SubProcess*: WMLA_GRAFANA_SERVICE_HOST=172.30.164.114\n*Task <1> SubProcess*: WMLA_MSD_SERVICE_HOST=172.30.243.42\n*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT_8892_TCP_PORT=8892\n*Task <1> SubProcess*: WMLA_EDI_SERVICE_HOST=172.30.50.232\n*Task <1> SubProcess*: POWERAI_SAVE_CAFFE_BIN=unset\n*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT_5043_TCP_PORT=5043\n*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_SERVICE_PORT=8888\n*Task <1> SubProcess*: SPACE_ID=\n*Task <1> SubProcess*: CUDA_VISIBLE_DEVICES=GPU-2c33df03-e129-bd55-5ff2-1288461e4b44\n*Task <1> SubProcess*: WMLA_INFERENCE_PORT_9000_TCP_PORT=9000\n*Task <1> SubProcess*: WMLA_AUTH_REST_PORT_3000_TCP=tcp://172.30.145.203:3000\n*Task <1> SubProcess*: LANG=en_US.UTF-8\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_SERVICE_PORT=89\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_89_TCP_ADDR=172.30.214.57\n*Task <1> SubProcess*: WMLA_INGRESS_PORT_30746_TCP_ADDR=172.30.182.101\n*Task <1> SubProcess*: WMLA_INGRESS_PORT_30746_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_INFOSERVICE_SERVICE_PORT_HISTORY_REST=8892\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_89_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT_5043_TCP_ADDR=172.30.58.183\n*Task <1> SubProcess*: MSD_CONTAINER_USER=1000620000\n*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT=tcp://172.30.95.180:8081\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT_8001_TCP_PORT=8001\n*Task <1> SubProcess*: WMLA_ETCD_SERVICE_HOST=172.30.127.95\n*Task <1> SubProcess*: WMLA_MSD_PORT_9080_TCP_PORT=9080\n*Task <1> SubProcess*: WMLA_MSS_SERVICE_HOST=172.30.106.213\n*Task <1> SubProcess*: WMLA_AUTH_REST_PORT_3000_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT_5043_TCP=tcp://172.30.58.183:5043\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_493_TCP=tcp://172.30.214.57:493\n*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT_8892_TCP_ADDR=172.30.104.57\n*Task <1> SubProcess*: MSD_POD_AFFINITY_TOPOLOGY_KEY=kubernetes.io/hostname\n*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT=tcp://172.30.1.98:9090\n*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT_8892_TCP_PROTO=tcp\n*Task <1> SubProcess*: SIMPLIFIEDWEM=N\n*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT_5043_TCP_PROTO=tcp\n*Task <1> SubProcess*: APP_ID=xwmla-1882\n*Task <1> SubProcess*: DLI_RESULT_FS=/gpfs/myresultfs\n*Task <1> SubProcess*: DLI_OBJECT_ID=xwmla-1882\n*Task <1> SubProcess*: _CE_M=\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT=tcp://172.30.214.57:89\n*Task <1> SubProcess*: WMLA_DLPD_PORT_9243_TCP_PROTO=tcp\n*Task <1> SubProcess*: REDHARE_LD_LIBRARY_PATH=/opt/wmla-mk/sdk/python\n*Task <1> SubProcess*: SHLVL=4\n*Task <1> SubProcess*: HOME=/\n*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_PORT=tcp://172.30.58.183:5043\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_493_TCP_PORT=493\n*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT_9090_TCP_ADDR=172.30.1.98\n*Task <1> SubProcess*: WMLA_INFOSERVICE_SERVICE_PORT=8892\n*Task <1> SubProcess*: LANGUAGE=en_US:en\n*Task <1> SubProcess*: WMLA_MSD_SERVICE_PORT=10000\n*Task <1> SubProcess*: WMLA_GRAFANA_SERVICE_PORT=3000\n*Task <1> SubProcess*: WMLA_MSD_PORT_9080_TCP=tcp://172.30.243.42:9080\n*Task <1> SubProcess*: WMLA_EDI_SERVICE_PORT=9000\n*Task <1> SubProcess*: CURRENT_RUNASGROUP=1000620000\n*Task <1> SubProcess*: TF_LIBRARY_DIR=/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages/tensorflow_core\n*Task <1> SubProcess*: WMLA_INFERENCE_PORT_9000_TCP_ADDR=172.30.188.165\n*Task <1> SubProcess*: KUBERNETES_PORT_443_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_AUTH_REST_PORT_3000_TCP_PORT=3000\n*Task <1> SubProcess*: WMLA_MSS_SERVICE_PORT_ENTRANCE_HTTP=9080\n*Task <1> SubProcess*: REDHARE_MK_MSD_ADDR=172.30.243.42:10000\n*Task <1> SubProcess*: FABRIC_HOME=/opt/ibm/spectrumcomputing/dli/2.2.0/fabric\n*Task <1> SubProcess*: KUBERNETES_SERVICE_PORT_HTTPS=443\n*Task <1> SubProcess*: DLIM_MK_LOG_PATH=/wmla-logging/dli\n*Task <1> SubProcess*: DLI_CS_DATASTORE_META=type=fs\n*Task <1> SubProcess*: CURRENT_RUNASUSER=1000620000\n*Task <1> SubProcess*: WMLA_INGRESS_PORT=tcp://172.30.182.101:30746\n*Task <1> SubProcess*: WMLA_GUI_SERVICE_PORT=8443\n*Task <1> SubProcess*: WMLA_EDI_PORT_9000_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_PROMETHEUS_SERVICE_HOST=172.30.1.98\n*Task <1> SubProcess*: mss_cluster_spec=10.129.2.147\n*Task <1> SubProcess*: WMLA_GRAFANA_PORT_3000_TCP_PORT=3000\n*Task <1> SubProcess*: WMLA_DLPD_PORT_27017_TCP_ADDR=172.30.2.247\n*Task <1> SubProcess*: DLI_RESULT_FS_SUBDIR_PERMISSION_ENFORCE=true\n*Task <1> SubProcess*: PYTHONPATH=/opt/wmla-mk/sdk/python::/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dli_utils/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dataset/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/compression/:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/py4j-0.10.4-src.zip:/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/pyspark.zip:/gpfs/myresultfs/admin:/gpfs/myresultfs/admin/lib/python3.7/site-packages\n*Task <1> SubProcess*: WMLA_MSS_PORT=tcp://172.30.106.213:9080\n*Task <1> SubProcess*: WMLA_GUI_PORT_8443_TCP=tcp://172.30.119.116:8443\n*Task <1> SubProcess*: JOB_ID=xwmla-1882\n*Task <1> SubProcess*: NSS_SDB_USE_CACHE=no\n*Task <1> SubProcess*: CONDA_PYTHON_EXE=/opt/anaconda3/bin/python\n*Task <1> SubProcess*: MPI_ROOT=/opt/anaconda3/envs/dlipy3\n*Task <1> SubProcess*: WMLA_MSD_PORT_10000_TCP_ADDR=172.30.243.42\n*Task <1> SubProcess*: REDHARE_UNLIMITED_RETRY=false\n*Task <1> SubProcess*: JOB_LOG_PATH=/wmla-logging/dli/xwmla-1882\n*Task <1> SubProcess*: WMLA_MSS_PORT_9080_TCP_ADDR=172.30.106.213\n*Task <1> SubProcess*: WMLA_PROMETHEUS_PORT_9090_TCP_PORT=9090\n*Task <1> SubProcess*: WMLA_GUI_PORT=tcp://172.30.119.116:8443\n*Task <1> SubProcess*: JOB_UUID=7d819f4c-ae1a-415c-b700-741d82f15557\n*Task <1> SubProcess*: IS_K8S=ON\n*Task <1> SubProcess*: MSD_MK_ID=7d819f4c-ae1a-415c-b700-741d82f15557\n*Task <1> SubProcess*: REDHARE_PYTHONPATH=/opt/wmla-mk/sdk/python\n*Task <1> SubProcess*: WMLA_LOGSTASH_SERVICE_SERVICE_HOST=172.30.58.183\n*Task <1> SubProcess*: WMLA_DLPD_PORT=tcp://172.30.2.247:27017\n*Task <1> SubProcess*: WMLA_MSD_SERVICE_PORT_RPC=10000\n*Task <1> SubProcess*: WMLA_GRAFANA_PORT_3000_TCP_ADDR=172.30.164.114\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_PORT_493_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_DLPD_PORT_9243_TCP=tcp://172.30.2.247:9243\n*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT_8081_TCP_PORT=8081\n*Task <1> SubProcess*: LOGSTASH_INSTANCE_URL=wmla-logstash-service:5043\n*Task <1> SubProcess*: MSD_POD_HOST_IP=9.21.55.16\n*Task <1> SubProcess*: CONDA_DEFAULT_ENV=dlipy3\n*Task <1> SubProcess*: OMP_NUM_THREADS=16\n*Task <1> SubProcess*: WMLA_EDI_PORT_8889_TCP=tcp://172.30.50.232:8889\n*Task <1> SubProcess*: WMLA_MSD_PORT_9080_TCP_ADDR=172.30.243.42\n*Task <1> SubProcess*: WMLA_GUI_PORT_8443_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_DLPD_SERVICE_PORT_DL_REST_PORT=9243\n*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_SERVICE_PORT_HTTP=8888\n*Task <1> SubProcess*: WMLA_EDI_PORT_9010_TCP=tcp://172.30.50.232:9010\n*Task <1> SubProcess*: WMLA_DLPD_SERVICE_PORT_MONGODB_PORT=27017\n*Task <1> SubProcess*: WMLA_INFERENCE_PORT=tcp://172.30.188.165:9000\n*Task <1> SubProcess*: FRAME_WORK=PyTorch\n*Task <1> SubProcess*: DLI_SC_RUNASGROUP=1000620000\n*Task <1> SubProcess*: DLI_SC_RUNASUSER=1000620000\n*Task <1> SubProcess*: DLI_SC_FSGROUP=1000620000\n*Task <1> SubProcess*: KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1\n*Task <1> SubProcess*: WMLA_MSD_PORT_10000_TCP=tcp://172.30.243.42:10000\n*Task <1> SubProcess*: WMLA_INGRESS_SERVICE_HOST=172.30.182.101\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_PUBLIC_SERVICE_PORT_HTTPS=493\n*Task <1> SubProcess*: WMLA_DLPD_SERVICE_PORT=27017\n*Task <1> SubProcess*: WMLA_MSD_PORT=tcp://172.30.243.42:10000\n*Task <1> SubProcess*: WMLA_GRAFANA_PORT=tcp://172.30.164.114:3000\n*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT_8888_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_EDI_PORT=tcp://172.30.50.232:9000\n*Task <1> SubProcess*: WMLA_EDI_PORT_8889_TCP_PROTO=tcp\n*Task <1> SubProcess*: WMLA_AUTH_REST_SERVICE_PORT=3000\n*Task <1> SubProcess*: WMLA_JUPYTER_HUB_PORT_8081_TCP=tcp://172.30.95.180:8081\n*Task <1> SubProcess*: WMLA_MSD_SERVICE_PORT_REST=9080\n*Task <1> SubProcess*: WMLA_EDI_SERVICE_PORT_REST=9000\n*Task <1> SubProcess*: DLI_DEFAULT_CONDA_ENV_NAME=dlipy3\n*Task <1> SubProcess*: WMLA_ETCD_PORT_2379_TCP_PORT=2379\n*Task <1> SubProcess*: WMLA_JUPYTER_HUB_SERVICE_HOST=172.30.95.180\n*Task <1> SubProcess*: KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443\n*Task <1> SubProcess*: DLI_IS_ELASTIC=false\n*Task <1> SubProcess*: container=oci\n*Task <1> SubProcess*: WMLA_INFOSERVICE_PORT_8892_TCP=tcp://172.30.104.57:8892\n*Task <1> SubProcess*: WMLA_JUPYTER_ENTERPRISE_GATEWAY_PORT=tcp://172.30.205.170:8888\n*Task <1> SubProcess*: DLI_LAUNCHER_CALLBACK=/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/msd/msdcallback\n*Task <1> SubProcess*: BASE_LOG_PATH=/wmla-logging/dli\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_SERVICE_PORT=8001\n*Task <1> SubProcess*: WMLA_JUPYTER_PROXY_API_PORT=tcp://172.30.39.248:8001\n*Task <1> SubProcess*: MSD_POD_GPU_DEVICES=GPU-2c33df03-e129-bd55-5ff2-1288461e4b44\n*Task <1> SubProcess*: _=/usr/bin/env\n*Task <1> SubProcess*: DLI_TASK_ID=0\n*Task <1> SubProcess*: 2021-02-08 21:17:38.172667 439 INFO Enter into pytorch_wrapper.sh\n*Task <1> SubProcess*: 2021-02-08 21:17:38.176800 439 INFO Check pytorch version\n*Task <1> SubProcess*: 1.3.1\n*Task <1> SubProcess*: 2021-02-08 21:17:39.017592 439 INFO Initialize\n*Task <1> SubProcess*: 2021-02-08 21:17:39.023850 439 INFO Setup file system datastore\n*Task <1> SubProcess*: 2021-02-08 21:17:39.053917 439 INFO RESULT_DIR=/gpfs/myresultfs/admin/batchworkdir/xwmla-1882, DATA_DIR=/gpfs/mydatafs, LOG_DIR=/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/log/app.xwmla-1882-task12n-nxl8r, MODEL_FILE_LOCATION=/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code\n*Task <1> SubProcess*: 2021-02-08 21:17:39.058334 439 INFO List files under DATA_DIR\n*Task <1> SubProcess*: $ls -l /gpfs/mydatafs\n*Task <1> SubProcess*: total 211632\n*Task <1> SubProcess*: drwxrwxrwx. 4 root       root            44 Jan 26 23:32 cifar10\n*Task <1> SubProcess*: -rw-r--r--. 1 root       root     170498071 Jun  4  2009 cifar-10-python.tar.gz\n*Task <1> SubProcess*: drwxr-xr-x. 5 root       root            80 Nov 25 16:24 higgs\n*Task <1> SubProcess*: drwxrwxr-x. 3 root       root            50 Oct 24  2019 __MACOSX\n*Task <1> SubProcess*: drwxr-xr-x. 5 1000620000 10006200        47 Nov 23 22:49 MNIST\n*Task <1> SubProcess*: drwxrwxrwx. 3 1000620000 10006200        36 Jan 11 03:47 msdtool\n*Task <1> SubProcess*: drwxr-xr-x. 3 1000620000 10006200        19 Nov 30 04:05 pytorch_mnist\n*Task <1> SubProcess*: drwxr-xr-x. 3 root       root            36 Oct 24  2019 pytorch-mnist\n*Task <1> SubProcess*: -rw-r--r--. 1 root       root      23006288 Nov 12 00:19 pytorch-mnist-dataset.zip\n*Task <1> SubProcess*: drwxr-xr-x. 2 root       root            43 Dec  1 21:45 samaya\n*Task <1> SubProcess*: drwxr-xr-x. 2 1000620000 10006200        42 Nov 25 18:19 tf2x_mnist\n*Task <1> SubProcess*: drwxrwxrwx. 2      20002    20002       140 Feb 13  2019 tf-mnist\n*Task <1> SubProcess*: -rwxr-xr-x. 1 root       root      11601920 Nov 12 00:21 tf-mnist-dataset.tar\n*Task <1> SubProcess*: -rw-r--r--. 1 root       root      11597448 Feb  4 04:33 tf-mnist.zip\n*Task <1> SubProcess*: drwxr-xr-x. 2 1000620000 10006200        49 Nov 25 18:46 xgb-model\n*Task <1> SubProcess*: 2021-02-08 21:17:39.068570 439 INFO Save log files under /gpfs/myresultfs/admin/batchworkdir/xwmla-1882/log/app.xwmla-1882-task12n-nxl8r\n*Task <1> SubProcess*: 2021-02-08 21:17:39.116303 439 INFO Check python modules path\n*Task <1> SubProcess*: ['', '/opt/wmla-mk/sdk/python', '/gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code', '/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dli_utils', '/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/dataset', '/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/compression', '/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/py4j-0.10.4-src.zip', '/opt/ibm/spectrumcomputing/dli/2.2.0/dlpd/tools/3rdparty/pyspark.zip', '/gpfs/myresultfs/admin', '/gpfs/myresultfs/admin/lib/python3.7/site-packages', '/opt/anaconda3/envs/dlipy3/lib/python37.zip', '/opt/anaconda3/envs/dlipy3/lib/python3.7', '/opt/anaconda3/envs/dlipy3/lib/python3.7/lib-dynload', '/opt/anaconda3/envs/dlipy3/lib/python3.7/site-packages']\n*Task <1> SubProcess*: 2021-02-08 21:17:39.166291 439 INFO Start running user model\n*Task <1> SubProcess*: /gpfs/myresultfs/admin/batchworkdir/xwmla-1882/_submitted_code/main.py --epochs 1\n*Task <1> SubProcess*: Namespace(batch_size=128, cuda=True, epochs=1, lr=0.01)\n*Task <1> SubProcess*: Use cuda:  True\n*Task <1> SubProcess*: DATA_DIR: /gpfs/mydatafs\n*Task <1> SubProcess*: Files already downloaded and verified\n*Task <1> SubProcess*: Files already downloaded and verified\n*Task <1> SubProcess*: => using pytorch build-in model 'resnet18'\n*Task <1> SubProcess*: Total iterations: 391\n*Task <1> SubProcess*: RESULT_DIR: /gpfs/myresultfs/admin/batchworkdir/xwmla-1882\n*Task <1> SubProcess*: Running epoch 1 ... It might take several minutes for each epoch to run.\n*Task <1> SubProcess*: Train - batches : 1, average loss: 2.4147, accuracy: 15/128 (12%)\n*Task <1> SubProcess*: Train - batches : 2, average loss: 2.3836, accuracy: 27/256 (11%)\n*Task <1> SubProcess*: Train - batches : 3, average loss: 2.3745, accuracy: 40/384 (10%)\n*Task <1> SubProcess*: Train - batches : 4, average loss: 2.3524, accuracy: 58/512 (11%)\n*Task <1> SubProcess*: Train - batches : 5, average loss: 2.3373, accuracy: 77/640 (12%)\n*Task <1> SubProcess*: Train - batches : 6, average loss: 2.3258, accuracy: 105/768 (14%)\n*Task <1> SubProcess*: Train - batches : 7, average loss: 2.3177, accuracy: 121/896 (14%)\n*Task <1> SubProcess*: Train - batches : 8, average loss: 2.3115, accuracy: 137/1024 (13%)\n*Task <1> SubProcess*: Train - batches : 9, average loss: 2.3087, accuracy: 151/1152 (13%)\n*Task <1> SubProcess*: Train - batches : 10, average loss: 2.3004, accuracy: 176/1280 (14%)\n*Task <1> SubProcess*: Train - batches : 11, average loss: 2.2997, accuracy: 188/1408 (13%)\n*Task <1> SubProcess*: Train - batches : 12, average loss: 2.2921, accuracy: 214/1536 (14%)\n*Task <1> SubProcess*: Train - batches : 13, average loss: 2.2876, accuracy: 230/1664 (14%)\n*Task <1> SubProcess*: Train - batches : 14, average loss: 2.2829, accuracy: 250/1792 (14%)\n*Task <1> SubProcess*: Train - batches : 15, average loss: 2.2766, accuracy: 279/1920 (15%)\n*Task <1> SubProcess*: Train - batches : 16, average loss: 2.2727, accuracy: 301/2048 (15%)\n*Task <1> SubProcess*: Train - batches : 17, average loss: 2.2689, accuracy: 320/2176 (15%)\n*Task <1> SubProcess*: Train - batches : 18, average loss: 2.2613, accuracy: 349/2304 (15%)\n*Task <1> SubProcess*: Train - batches : 19, average loss: 2.2545, accuracy: 381/2432 (16%)\n*Task <1> SubProcess*: Train - batches : 20, average loss: 2.2506, accuracy: 413/2560 (16%)\n*Task <1> SubProcess*: Train - batches : 21, average loss: 2.2448, accuracy: 445/2688 (17%)\n*Task <1> SubProcess*: Train - batches : 22, average loss: 2.2411, accuracy: 472/2816 (17%)\n*Task <1> SubProcess*: Train - batches : 23, average loss: 2.2350, accuracy: 508/2944 (17%)\n*Task <1> SubProcess*: Train - batches : 24, average loss: 2.2317, accuracy: 535/3072 (17%)\n*Task <1> SubProcess*: Train - batches : 25, average loss: 2.2289, accuracy: 562/3200 (18%)\n*Task <1> SubProcess*: Train - batches : 26, average loss: 2.2241, accuracy: 595/3328 (18%)\n*Task <1> SubProcess*: Train - batches : 27, average loss: 2.2188, accuracy: 627/3456 (18%)\n*Task <1> SubProcess*: Train - batches : 28, average loss: 2.2145, accuracy: 660/3584 (18%)\n*Task <1> SubProcess*: Train - batches : 29, average loss: 2.2124, accuracy: 685/3712 (18%)\n*Task <1> SubProcess*: Train - batches : 30, average loss: 2.2077, accuracy: 715/3840 (19%)\n*Task <1> SubProcess*: Train - batches : 31, average loss: 2.2041, accuracy: 746/3968 (19%)\n*Task <1> SubProcess*: Train - batches : 32, average loss: 2.1999, accuracy: 776/4096 (19%)\n*Task <1> SubProcess*: Train - batches : 33, average loss: 2.1937, accuracy: 821/4224 (19%)\n*Task <1> SubProcess*: Train - batches : 34, average loss: 2.1902, accuracy: 858/4352 (20%)\n*Task <1> SubProcess*: Train - batches : 35, average loss: 2.1870, accuracy: 881/4480 (20%)\n*Task <1> SubProcess*: Train - batches : 36, average loss: 2.1841, accuracy: 907/4608 (20%)\n*Task <1> SubProcess*: Train - batches : 37, average loss: 2.1804, accuracy: 945/4736 (20%)\n*Task <1> SubProcess*: Train - batches : 38, average loss: 2.1768, accuracy: 982/4864 (20%)\n*Task <1> SubProcess*: Train - batches : 39, average loss: 2.1737, accuracy: 1024/4992 (21%)\n*Task <1> SubProcess*: Train - batches : 40, average loss: 2.1704, accuracy: 1050/5120 (21%)\n*Task <1> SubProcess*: Train - batches : 41, average loss: 2.1667, accuracy: 1080/5248 (21%)\n*Task <1> SubProcess*: Train - batches : 42, average loss: 2.1650, accuracy: 1123/5376 (21%)\n*Task <1> SubProcess*: Train - batches : 43, average loss: 2.1629, accuracy: 1147/5504 (21%)\n*Task <1> SubProcess*: Train - batches : 44, average loss: 2.1577, accuracy: 1197/5632 (21%)\n*Task <1> SubProcess*: Train - batches : 45, average loss: 2.1537, accuracy: 1230/5760 (21%)\n*Task <1> SubProcess*: Train - batches : 46, average loss: 2.1491, accuracy: 1272/5888 (22%)\n*Task <1> SubProcess*: Train - batches : 47, average loss: 2.1468, accuracy: 1303/6016 (22%)\n*Task <1> SubProcess*: Train - batches : 48, average loss: 2.1422, accuracy: 1342/6144 (22%)\n*Task <1> SubProcess*: Train - batches : 49, average loss: 2.1397, accuracy: 1375/6272 (22%)\n*Task <1> SubProcess*: Train - batches : 50, average loss: 2.1388, accuracy: 1405/6400 (22%)\n*Task <1> SubProcess*: Train - batches : 51, average loss: 2.1355, accuracy: 1443/6528 (22%)\n*Task <1> SubProcess*: Train - batches : 52, average loss: 2.1327, accuracy: 1481/6656 (22%)\n*Task <1> SubProcess*: Train - batches : 53, average loss: 2.1288, accuracy: 1518/6784 (22%)\n*Task <1> SubProcess*: Train - batches : 54, average loss: 2.1245, accuracy: 1556/6912 (23%)\n*Task <1> SubProcess*: Train - batches : 55, average loss: 2.1231, accuracy: 1588/7040 (23%)\n*Task <1> SubProcess*: Train - batches : 56, average loss: 2.1211, accuracy: 1617/7168 (23%)\n*Task <1> SubProcess*: Train - batches : 57, average loss: 2.1192, accuracy: 1649/7296 (23%)\n*Task <1> SubProcess*: Train - batches : 58, average loss: 2.1168, accuracy: 1689/7424 (23%)\n*Task <1> SubProcess*: Train - batches : 59, average loss: 2.1139, accuracy: 1721/7552 (23%)\n*Task <1> SubProcess*: Train - batches : 60, average loss: 2.1112, accuracy: 1753/7680 (23%)\n*Task <1> SubProcess*: Train - batches : 61, average loss: 2.1088, accuracy: 1795/7808 (23%)\n*Task <1> SubProcess*: Train - batches : 62, average loss: 2.1078, accuracy: 1824/7936 (23%)\n*Task <1> SubProcess*: Train - batches : 63, average loss: 2.1044, accuracy: 1867/8064 (23%)\n*Task <1> SubProcess*: Train - batches : 64, average loss: 2.1019, accuracy: 1901/8192 (23%)\n*Task <1> SubProcess*: Train - batches : 65, average loss: 2.0988, accuracy: 1946/8320 (23%)\n*Task <1> SubProcess*: Train - batches : 66, average loss: 2.0965, accuracy: 1982/8448 (23%)\n*Task <1> SubProcess*: Train - batches : 67, average loss: 2.0955, accuracy: 2012/8576 (23%)\n*Task <1> SubProcess*: Train - batches : 68, average loss: 2.0926, accuracy: 2048/8704 (24%)\n*Task <1> SubProcess*: Train - batches : 69, average loss: 2.0893, accuracy: 2088/8832 (24%)\n*Task <1> SubProcess*: Train - batches : 70, average loss: 2.0865, accuracy: 2126/8960 (24%)\n*Task <1> SubProcess*: Train - batches : 71, average loss: 2.0845, accuracy: 2165/9088 (24%)\n*Task <1> SubProcess*: Train - batches : 72, average loss: 2.0815, accuracy: 2211/9216 (24%)\n*Task <1> SubProcess*: Train - batches : 73, average loss: 2.0795, accuracy: 2241/9344 (24%)\n*Task <1> SubProcess*: Train - batches : 74, average loss: 2.0782, accuracy: 2276/9472 (24%)\n*Task <1> SubProcess*: Train - batches : 75, average loss: 2.0754, accuracy: 2324/9600 (24%)\n*Task <1> SubProcess*: Train - batches : 76, average loss: 2.0729, accuracy: 2363/9728 (24%)\n*Task <1> SubProcess*: Train - batches : 77, average loss: 2.0702, accuracy: 2405/9856 (24%)\n*Task <1> SubProcess*: Train - batches : 78, average loss: 2.0695, accuracy: 2433/9984 (24%)\n*Task <1> SubProcess*: Train - batches : 79, average loss: 2.0669, accuracy: 2469/10112 (24%)\n*Task <1> SubProcess*: Train - batches : 80, average loss: 2.0646, accuracy: 2502/10240 (24%)\n*Task <1> SubProcess*: Train - batches : 81, average loss: 2.0615, accuracy: 2541/10368 (25%)\n*Task <1> SubProcess*: Train - batches : 82, average loss: 2.0591, accuracy: 2579/10496 (25%)\n*Task <1> SubProcess*: Train - batches : 83, average loss: 2.0559, accuracy: 2617/10624 (25%)\n*Task <1> SubProcess*: Train - batches : 84, average loss: 2.0533, accuracy: 2662/10752 (25%)\n*Task <1> SubProcess*: Train - batches : 85, average loss: 2.0514, accuracy: 2705/10880 (25%)\n*Task <1> SubProcess*: Train - batches : 86, average loss: 2.0488, accuracy: 2748/11008 (25%)\n*Task <1> SubProcess*: Train - batches : 87, average loss: 2.0468, accuracy: 2783/11136 (25%)\n*Task <1> SubProcess*: Train - batches : 88, average loss: 2.0454, accuracy: 2819/11264 (25%)\n*Task <1> SubProcess*: Train - batches : 89, average loss: 2.0438, accuracy: 2849/11392 (25%)\n*Task <1> SubProcess*: Train - batches : 90, average loss: 2.0416, accuracy: 2889/11520 (25%)\n*Task <1> SubProcess*: Train - batches : 91, average loss: 2.0393, accuracy: 2929/11648 (25%)\n*Task <1> SubProcess*: Train - batches : 92, average loss: 2.0362, accuracy: 2971/11776 (25%)\n*Task <1> SubProcess*: Train - batches : 93, average loss: 2.0343, accuracy: 3005/11904 (25%)\n*Task <1> SubProcess*: Train - batches : 94, average loss: 2.0316, accuracy: 3048/12032 (25%)\n*Task <1> SubProcess*: Train - batches : 95, average loss: 2.0307, accuracy: 3080/12160 (25%)\n*Task <1> SubProcess*: Train - batches : 96, average loss: 2.0280, accuracy: 3120/12288 (25%)\n*Task <1> SubProcess*: Train - batches : 97, average loss: 2.0263, accuracy: 3157/12416 (25%)\n*Task <1> SubProcess*: Train - batches : 98, average loss: 2.0247, accuracy: 3200/12544 (26%)\n*Task <1> SubProcess*: Train - batches : 99, average loss: 2.0224, accuracy: 3248/12672 (26%)\n*Task <1> SubProcess*: Train - batches : 100, average loss: 2.0208, accuracy: 3297/12800 (26%)\n*Task <1> SubProcess*: Train - batches : 101, average loss: 2.0191, accuracy: 3340/12928 (26%)\n*Task <1> SubProcess*: Train - batches : 102, average loss: 2.0172, accuracy: 3377/13056 (26%)\n*Task <1> SubProcess*: Train - batches : 103, average loss: 2.0154, accuracy: 3416/13184 (26%)\n*Task <1> SubProcess*: Train - batches : 104, average loss: 2.0131, accuracy: 3458/13312 (26%)\n*Task <1> SubProcess*: Train - batches : 105, average loss: 2.0122, accuracy: 3491/13440 (26%)\n*Task <1> SubProcess*: Train - batches : 106, average loss: 2.0097, accuracy: 3535/13568 (26%)\n*Task <1> SubProcess*: Train - batches : 107, average loss: 2.0082, accuracy: 3572/13696 (26%)\n*Task <1> SubProcess*: Train - batches : 108, average loss: 2.0070, accuracy: 3613/13824 (26%)\n*Task <1> SubProcess*: Train - batches : 109, average loss: 2.0047, accuracy: 3660/13952 (26%)\n*Task <1> SubProcess*: Train - batches : 110, average loss: 2.0035, accuracy: 3703/14080 (26%)\n*Task <1> SubProcess*: Train - batches : 111, average loss: 2.0018, accuracy: 3756/14208 (26%)\n*Task <1> SubProcess*: Train - batches : 112, average loss: 2.0000, accuracy: 3798/14336 (26%)\n*Task <1> SubProcess*: Train - batches : 113, average loss: 1.9988, accuracy: 3838/14464 (27%)\n*Task <1> SubProcess*: Train - batches : 114, average loss: 1.9970, accuracy: 3882/14592 (27%)\n*Task <1> SubProcess*: Train - batches : 115, average loss: 1.9953, accuracy: 3927/14720 (27%)\n*Task <1> SubProcess*: Train - batches : 116, average loss: 1.9941, accuracy: 3966/14848 (27%)\n*Task <1> SubProcess*: Train - batches : 117, average loss: 1.9925, accuracy: 4014/14976 (27%)\n*Task <1> SubProcess*: Train - batches : 118, average loss: 1.9910, accuracy: 4055/15104 (27%)\n*Task <1> SubProcess*: Train - batches : 119, average loss: 1.9893, accuracy: 4107/15232 (27%)\n*Task <1> SubProcess*: Train - batches : 120, average loss: 1.9884, accuracy: 4148/15360 (27%)\n*Task <1> SubProcess*: Train - batches : 121, average loss: 1.9874, accuracy: 4195/15488 (27%)\n*Task <1> SubProcess*: Train - batches : 122, average loss: 1.9857, accuracy: 4239/15616 (27%)\n*Task <1> SubProcess*: Train - batches : 123, average loss: 1.9850, accuracy: 4285/15744 (27%)\n*Task <1> SubProcess*: Train - batches : 124, average loss: 1.9846, accuracy: 4324/15872 (27%)\n*Task <1> SubProcess*: Train - batches : 125, average loss: 1.9833, accuracy: 4361/16000 (27%)\n*Task <1> SubProcess*: Train - batches : 126, average loss: 1.9820, accuracy: 4395/16128 (27%)\n*Task <1> SubProcess*: Train - batches : 127, average loss: 1.9800, accuracy: 4437/16256 (27%)\n*Task <1> SubProcess*: Train - batches : 128, average loss: 1.9779, accuracy: 4492/16384 (27%)\n*Task <1> SubProcess*: Train - batches : 129, average loss: 1.9763, accuracy: 4542/16512 (28%)\n*Task <1> SubProcess*: Train - batches : 130, average loss: 1.9743, accuracy: 4597/16640 (28%)\n*Task <1> SubProcess*: Train - batches : 131, average loss: 1.9730, accuracy: 4650/16768 (28%)\n*Task <1> SubProcess*: Train - batches : 132, average loss: 1.9708, accuracy: 4698/16896 (28%)\n*Task <1> SubProcess*: Train - batches : 133, average loss: 1.9696, accuracy: 4733/17024 (28%)\n*Task <1> SubProcess*: Train - batches : 134, average loss: 1.9675, accuracy: 4778/17152 (28%)\n*Task <1> SubProcess*: Train - batches : 135, average loss: 1.9666, accuracy: 4821/17280 (28%)\n*Task <1> SubProcess*: Train - batches : 136, average loss: 1.9653, accuracy: 4868/17408 (28%)\n*Task <1> SubProcess*: Train - batches : 137, average loss: 1.9633, accuracy: 4916/17536 (28%)\n*Task <1> SubProcess*: Train - batches : 138, average loss: 1.9627, accuracy: 4958/17664 (28%)\n*Task <1> SubProcess*: Train - batches : 139, average loss: 1.9607, accuracy: 5007/17792 (28%)\n*Task <1> SubProcess*: Train - batches : 140, average loss: 1.9591, accuracy: 5053/17920 (28%)\n*Task <1> SubProcess*: Train - batches : 141, average loss: 1.9572, accuracy: 5103/18048 (28%)\n*Task <1> SubProcess*: Train - batches : 142, average loss: 1.9557, accuracy: 5155/18176 (28%)\n*Task <1> SubProcess*: Train - batches : 143, average loss: 1.9547, accuracy: 5205/18304 (28%)\n*Task <1> SubProcess*: Train - batches : 144, average loss: 1.9535, accuracy: 5239/18432 (28%)\n*Task <1> SubProcess*: Train - batches : 145, average loss: 1.9524, accuracy: 5277/18560 (28%)\n*Task <1> SubProcess*: Train - batches : 146, average loss: 1.9517, accuracy: 5315/18688 (28%)\n*Task <1> SubProcess*: Train - batches : 147, average loss: 1.9507, accuracy: 5359/18816 (28%)\n*Task <1> SubProcess*: Train - batches : 148, average loss: 1.9491, accuracy: 5405/18944 (29%)\n*Task <1> SubProcess*: Train - batches : 149, average loss: 1.9479, accuracy: 5447/19072 (29%)\n*Task <1> SubProcess*: Train - batches : 150, average loss: 1.9459, accuracy: 5485/19200 (29%)\n*Task <1> SubProcess*: Train - batches : 151, average loss: 1.9446, accuracy: 5527/19328 (29%)\n*Task <1> SubProcess*: Train - batches : 152, average loss: 1.9437, accuracy: 5571/19456 (29%)\n*Task <1> SubProcess*: Train - batches : 153, average loss: 1.9430, accuracy: 5610/19584 (29%)\n*Task <1> SubProcess*: Train - batches : 154, average loss: 1.9428, accuracy: 5648/19712 (29%)\n*Task <1> SubProcess*: Train - batches : 155, average loss: 1.9419, accuracy: 5693/19840 (29%)\n*Task <1> SubProcess*: Train - batches : 156, average loss: 1.9417, accuracy: 5730/19968 (29%)\n*Task <1> SubProcess*: Train - batches : 157, average loss: 1.9410, accuracy: 5780/20096 (29%)\n*Task <1> SubProcess*: Train - batches : 158, average loss: 1.9401, accuracy: 5818/20224 (29%)\n*Task <1> SubProcess*: Train - batches : 159, average loss: 1.9390, accuracy: 5862/20352 (29%)\n*Task <1> SubProcess*: Train - batches : 160, average loss: 1.9379, accuracy: 5909/20480 (29%)\n*Task <1> SubProcess*: Train - batches : 161, average loss: 1.9370, accuracy: 5956/20608 (29%)\n*Task <1> SubProcess*: Train - batches : 162, average loss: 1.9359, accuracy: 6003/20736 (29%)\n*Task <1> SubProcess*: Train - batches : 163, average loss: 1.9349, accuracy: 6051/20864 (29%)\n*Task <1> SubProcess*: Train - batches : 164, average loss: 1.9339, accuracy: 6095/20992 (29%)\n*Task <1> SubProcess*: Train - batches : 165, average loss: 1.9325, accuracy: 6144/21120 (29%)\n*Task <1> SubProcess*: Train - batches : 166, average loss: 1.9315, accuracy: 6189/21248 (29%)\n*Task <1> SubProcess*: Train - batches : 167, average loss: 1.9302, accuracy: 6237/21376 (29%)\n*Task <1> SubProcess*: Train - batches : 168, average loss: 1.9298, accuracy: 6279/21504 (29%)\n*Task <1> SubProcess*: Train - batches : 169, average loss: 1.9290, accuracy: 6324/21632 (29%)\n*Task <1> SubProcess*: Train - batches : 170, average loss: 1.9274, accuracy: 6370/21760 (29%)\n*Task <1> SubProcess*: Train - batches : 171, average loss: 1.9262, accuracy: 6418/21888 (29%)\n*Task <1> SubProcess*: Train - batches : 172, average loss: 1.9252, accuracy: 6459/22016 (29%)\n*Task <1> SubProcess*: Train - batches : 173, average loss: 1.9248, accuracy: 6497/22144 (29%)\n*Task <1> SubProcess*: Train - batches : 174, average loss: 1.9239, accuracy: 6545/22272 (29%)\n*Task <1> SubProcess*: Train - batches : 175, average loss: 1.9226, accuracy: 6591/22400 (29%)\n*Task <1> SubProcess*: Train - batches : 176, average loss: 1.9216, accuracy: 6639/22528 (29%)\n*Task <1> SubProcess*: Train - batches : 177, average loss: 1.9209, accuracy: 6677/22656 (29%)\n*Task <1> SubProcess*: Train - batches : 178, average loss: 1.9203, accuracy: 6729/22784 (30%)\n*Task <1> SubProcess*: Train - batches : 179, average loss: 1.9194, accuracy: 6771/22912 (30%)\n*Task <1> SubProcess*: Train - batches : 180, average loss: 1.9180, accuracy: 6825/23040 (30%)\n*Task <1> SubProcess*: Train - batches : 181, average loss: 1.9169, accuracy: 6871/23168 (30%)\n*Task <1> SubProcess*: Train - batches : 182, average loss: 1.9157, accuracy: 6921/23296 (30%)\n*Task <1> SubProcess*: Train - batches : 183, average loss: 1.9151, accuracy: 6960/23424 (30%)\n*Task <1> SubProcess*: Train - batches : 184, average loss: 1.9145, accuracy: 6999/23552 (30%)\n*Task <1> SubProcess*: Train - batches : 185, average loss: 1.9134, accuracy: 7044/23680 (30%)\n*Task <1> SubProcess*: Train - batches : 186, average loss: 1.9121, accuracy: 7091/23808 (30%)\n*Task <1> SubProcess*: Train - batches : 187, average loss: 1.9112, accuracy: 7134/23936 (30%)\n*Task <1> SubProcess*: Train - batches : 188, average loss: 1.9101, accuracy: 7191/24064 (30%)\n*Task <1> SubProcess*: Train - batches : 189, average loss: 1.9090, accuracy: 7240/24192 (30%)\n*Task <1> SubProcess*: Train - batches : 190, average loss: 1.9084, accuracy: 7290/24320 (30%)\n*Task <1> SubProcess*: Train - batches : 191, average loss: 1.9066, accuracy: 7347/24448 (30%)\n*Task <1> SubProcess*: Train - batches : 192, average loss: 1.9058, accuracy: 7390/24576 (30%)\n*Task <1> SubProcess*: Train - batches : 193, average loss: 1.9044, accuracy: 7439/24704 (30%)\n*Task <1> SubProcess*: Train - batches : 194, average loss: 1.9034, accuracy: 7488/24832 (30%)\n*Task <1> SubProcess*: Train - batches : 195, average loss: 1.9024, accuracy: 7541/24960 (30%)\n*Task <1> SubProcess*: Train - batches : 196, average loss: 1.9014, accuracy: 7592/25088 (30%)\n*Task <1> SubProcess*: Train - batches : 197, average loss: 1.9002, accuracy: 7652/25216 (30%)\n*Task <1> SubProcess*: Train - batches : 198, average loss: 1.8996, accuracy: 7704/25344 (30%)\n*Task <1> SubProcess*: Train - batches : 199, average loss: 1.8989, accuracy: 7760/25472 (30%)\n*Task <1> SubProcess*: Train - batches : 200, average loss: 1.8978, accuracy: 7809/25600 (31%)\n*Task <1> SubProcess*: Train - batches : 201, average loss: 1.8965, accuracy: 7861/25728 (31%)\n*Task <1> SubProcess*: Train - batches : 202, average loss: 1.8953, accuracy: 7905/25856 (31%)\n*Task <1> SubProcess*: Train - batches : 203, average loss: 1.8944, accuracy: 7957/25984 (31%)\n*Task <1> SubProcess*: Train - batches : 204, average loss: 1.8935, accuracy: 8009/26112 (31%)\n*Task <1> SubProcess*: Train - batches : 205, average loss: 1.8925, accuracy: 8050/26240 (31%)\n*Task <1> SubProcess*: Train - batches : 206, average loss: 1.8917, accuracy: 8100/26368 (31%)\n*Task <1> SubProcess*: Train - batches : 207, average loss: 1.8908, accuracy: 8148/26496 (31%)\n*Task <1> SubProcess*: Train - batches : 208, average loss: 1.8911, accuracy: 8186/26624 (31%)\n*Task <1> SubProcess*: Train - batches : 209, average loss: 1.8900, accuracy: 8240/26752 (31%)\n*Task <1> SubProcess*: Train - batches : 210, average loss: 1.8894, accuracy: 8283/26880 (31%)\n*Task <1> SubProcess*: Train - batches : 211, average loss: 1.8877, accuracy: 8339/27008 (31%)\n*Task <1> SubProcess*: Train - batches : 212, average loss: 1.8866, accuracy: 8389/27136 (31%)\n*Task <1> SubProcess*: Train - batches : 213, average loss: 1.8860, accuracy: 8437/27264 (31%)\n*Task <1> SubProcess*: Train - batches : 214, average loss: 1.8844, accuracy: 8497/27392 (31%)\n*Task <1> SubProcess*: Train - batches : 215, average loss: 1.8832, accuracy: 8560/27520 (31%)\n*Task <1> SubProcess*: Train - batches : 216, average loss: 1.8820, accuracy: 8616/27648 (31%)\n*Task <1> SubProcess*: Train - batches : 217, average loss: 1.8809, accuracy: 8663/27776 (31%)\n*Task <1> SubProcess*: Train - batches : 218, average loss: 1.8797, accuracy: 8713/27904 (31%)\n*Task <1> SubProcess*: Train - batches : 219, average loss: 1.8794, accuracy: 8756/28032 (31%)\n*Task <1> SubProcess*: Train - batches : 220, average loss: 1.8783, accuracy: 8806/28160 (31%)\n*Task <1> SubProcess*: Train - batches : 221, average loss: 1.8774, accuracy: 8860/28288 (31%)\n*Task <1> SubProcess*: Train - batches : 222, average loss: 1.8762, accuracy: 8908/28416 (31%)\n*Task <1> SubProcess*: Train - batches : 223, average loss: 1.8755, accuracy: 8951/28544 (31%)\n*Task <1> SubProcess*: Train - batches : 224, average loss: 1.8753, accuracy: 8995/28672 (31%)\n*Task <1> SubProcess*: Train - batches : 225, average loss: 1.8748, accuracy: 9040/28800 (31%)\n*Task <1> SubProcess*: Train - batches : 226, average loss: 1.8742, accuracy: 9089/28928 (31%)\n*Task <1> SubProcess*: Train - batches : 227, average loss: 1.8739, accuracy: 9127/29056 (31%)\n*Task <1> SubProcess*: Train - batches : 228, average loss: 1.8728, accuracy: 9185/29184 (31%)\n*Task <1> SubProcess*: Train - batches : 229, average loss: 1.8720, accuracy: 9229/29312 (31%)\n*Task <1> SubProcess*: Train - batches : 230, average loss: 1.8712, accuracy: 9279/29440 (32%)\n*Task <1> SubProcess*: Train - batches : 231, average loss: 1.8705, accuracy: 9321/29568 (32%)\n*Task <1> SubProcess*: Train - batches : 232, average loss: 1.8695, accuracy: 9382/29696 (32%)\n*Task <1> SubProcess*: Train - batches : 233, average loss: 1.8690, accuracy: 9427/29824 (32%)\n*Task <1> SubProcess*: Train - batches : 234, average loss: 1.8683, accuracy: 9476/29952 (32%)\n*Task <1> SubProcess*: Train - batches : 235, average loss: 1.8674, accuracy: 9520/30080 (32%)\n*Task <1> SubProcess*: Train - batches : 236, average loss: 1.8664, accuracy: 9565/30208 (32%)\n*Task <1> SubProcess*: Train - batches : 237, average loss: 1.8651, accuracy: 9620/30336 (32%)\n*Task <1> SubProcess*: Train - batches : 238, average loss: 1.8641, accuracy: 9676/30464 (32%)\n*Task <1> SubProcess*: Train - batches : 239, average loss: 1.8633, accuracy: 9724/30592 (32%)\n*Task <1> SubProcess*: Train - batches : 240, average loss: 1.8628, accuracy: 9765/30720 (32%)\n*Task <1> SubProcess*: Train - batches : 241, average loss: 1.8624, accuracy: 9815/30848 (32%)\n*Task <1> SubProcess*: Train - batches : 242, average loss: 1.8619, accuracy: 9862/30976 (32%)\n*Task <1> SubProcess*: Train - batches : 243, average loss: 1.8609, accuracy: 9908/31104 (32%)\n*Task <1> SubProcess*: Train - batches : 244, average loss: 1.8598, accuracy: 9956/31232 (32%)\n*Task <1> SubProcess*: Train - batches : 245, average loss: 1.8589, accuracy: 10005/31360 (32%)\n*Task <1> SubProcess*: Train - batches : 246, average loss: 1.8582, accuracy: 10052/31488 (32%)\n*Task <1> SubProcess*: Train - batches : 247, average loss: 1.8577, accuracy: 10102/31616 (32%)\n*Task <1> SubProcess*: Train - batches : 248, average loss: 1.8572, accuracy: 10150/31744 (32%)\n*Task <1> SubProcess*: Train - batches : 249, average loss: 1.8563, accuracy: 10198/31872 (32%)\n*Task <1> SubProcess*: Train - batches : 250, average loss: 1.8557, accuracy: 10237/32000 (32%)\n*Task <1> SubProcess*: Train - batches : 251, average loss: 1.8548, accuracy: 10286/32128 (32%)\n*Task <1> SubProcess*: Train - batches : 252, average loss: 1.8540, accuracy: 10334/32256 (32%)\n*Task <1> SubProcess*: Train - batches : 253, average loss: 1.8528, accuracy: 10387/32384 (32%)\n*Task <1> SubProcess*: Train - batches : 254, average loss: 1.8518, accuracy: 10434/32512 (32%)\n*Task <1> SubProcess*: Train - batches : 255, average loss: 1.8509, accuracy: 10486/32640 (32%)\n*Task <1> SubProcess*: Train - batches : 256, average loss: 1.8503, accuracy: 10537/32768 (32%)\n*Task <1> SubProcess*: Train - batches : 257, average loss: 1.8490, accuracy: 10594/32896 (32%)\n*Task <1> SubProcess*: Train - batches : 258, average loss: 1.8487, accuracy: 10640/33024 (32%)\n*Task <1> SubProcess*: Train - batches : 259, average loss: 1.8480, accuracy: 10694/33152 (32%)\n*Task <1> SubProcess*: Train - batches : 260, average loss: 1.8473, accuracy: 10741/33280 (32%)\n*Task <1> SubProcess*: Train - batches : 261, average loss: 1.8463, accuracy: 10807/33408 (32%)\n*Task <1> SubProcess*: Train - batches : 262, average loss: 1.8455, accuracy: 10858/33536 (32%)\n*Task <1> SubProcess*: Train - batches : 263, average loss: 1.8449, accuracy: 10910/33664 (32%)\n*Task <1> SubProcess*: Train - batches : 264, average loss: 1.8442, accuracy: 10956/33792 (32%)\n*Task <1> SubProcess*: Train - batches : 265, average loss: 1.8434, accuracy: 11000/33920 (32%)\n*Task <1> SubProcess*: Train - batches : 266, average loss: 1.8427, accuracy: 11050/34048 (32%)\n*Task <1> SubProcess*: Train - batches : 267, average loss: 1.8421, accuracy: 11099/34176 (32%)\n*Task <1> SubProcess*: Train - batches : 268, average loss: 1.8412, accuracy: 11155/34304 (33%)\n*Task <1> SubProcess*: Train - batches : 269, average loss: 1.8402, accuracy: 11207/34432 (33%)\n*Task <1> SubProcess*: Train - batches : 270, average loss: 1.8393, accuracy: 11260/34560 (33%)\n*Task <1> SubProcess*: Train - batches : 271, average loss: 1.8392, accuracy: 11306/34688 (33%)\n*Task <1> SubProcess*: Train - batches : 272, average loss: 1.8382, accuracy: 11369/34816 (33%)\n*Task <1> SubProcess*: Train - batches : 273, average loss: 1.8375, accuracy: 11410/34944 (33%)\n*Task <1> SubProcess*: Train - batches : 274, average loss: 1.8365, accuracy: 11463/35072 (33%)\n*Task <1> SubProcess*: Train - batches : 275, average loss: 1.8360, accuracy: 11510/35200 (33%)\n*Task <1> SubProcess*: Train - batches : 276, average loss: 1.8356, accuracy: 11555/35328 (33%)\n*Task <1> SubProcess*: Train - batches : 277, average loss: 1.8348, accuracy: 11608/35456 (33%)\n*Task <1> SubProcess*: Train - batches : 278, average loss: 1.8342, accuracy: 11662/35584 (33%)\n*Task <1> SubProcess*: Train - batches : 279, average loss: 1.8333, accuracy: 11714/35712 (33%)\n*Task <1> SubProcess*: Train - batches : 280, average loss: 1.8328, accuracy: 11755/35840 (33%)\n*Task <1> SubProcess*: Train - batches : 281, average loss: 1.8320, accuracy: 11802/35968 (33%)\n*Task <1> SubProcess*: Train - batches : 282, average loss: 1.8312, accuracy: 11857/36096 (33%)\n*Task <1> SubProcess*: Train - batches : 283, average loss: 1.8306, accuracy: 11914/36224 (33%)\n*Task <1> SubProcess*: Train - batches : 284, average loss: 1.8300, accuracy: 11964/36352 (33%)\n*Task <1> SubProcess*: Train - batches : 285, average loss: 1.8290, accuracy: 12019/36480 (33%)\n*Task <1> SubProcess*: Train - batches : 286, average loss: 1.8284, accuracy: 12069/36608 (33%)\n*Task <1> SubProcess*: Train - batches : 287, average loss: 1.8275, accuracy: 12123/36736 (33%)\n*Task <1> SubProcess*: Train - batches : 288, average loss: 1.8268, accuracy: 12172/36864 (33%)\n*Task <1> SubProcess*: Train - batches : 289, average loss: 1.8265, accuracy: 12212/36992 (33%)\n*Task <1> SubProcess*: Train - batches : 290, average loss: 1.8255, accuracy: 12270/37120 (33%)\n*Task <1> SubProcess*: Train - batches : 291, average loss: 1.8249, accuracy: 12325/37248 (33%)\n*Task <1> SubProcess*: Train - batches : 292, average loss: 1.8236, accuracy: 12387/37376 (33%)\n*Task <1> SubProcess*: Train - batches : 293, average loss: 1.8227, accuracy: 12446/37504 (33%)\n*Task <1> SubProcess*: Train - batches : 294, average loss: 1.8223, accuracy: 12492/37632 (33%)\n*Task <1> SubProcess*: Train - batches : 295, average loss: 1.8214, accuracy: 12547/37760 (33%)\n*Task <1> SubProcess*: Train - batches : 296, average loss: 1.8205, accuracy: 12605/37888 (33%)\n*Task <1> SubProcess*: Train - batches : 297, average loss: 1.8204, accuracy: 12639/38016 (33%)\n*Task <1> SubProcess*: Train - batches : 298, average loss: 1.8201, accuracy: 12683/38144 (33%)\n*Task <1> SubProcess*: Train - batches : 299, average loss: 1.8195, accuracy: 12736/38272 (33%)\n*Task <1> SubProcess*: Train - batches : 300, average loss: 1.8187, accuracy: 12787/38400 (33%)\n*Task <1> SubProcess*: Train - batches : 301, average loss: 1.8183, accuracy: 12835/38528 (33%)\n*Task <1> SubProcess*: Train - batches : 302, average loss: 1.8176, accuracy: 12884/38656 (33%)\n*Task <1> SubProcess*: Train - batches : 303, average loss: 1.8168, accuracy: 12945/38784 (33%)\n*Task <1> SubProcess*: Train - batches : 304, average loss: 1.8163, accuracy: 12990/38912 (33%)\n*Task <1> SubProcess*: Train - batches : 305, average loss: 1.8158, accuracy: 13033/39040 (33%)\n*Task <1> SubProcess*: Train - batches : 306, average loss: 1.8153, accuracy: 13075/39168 (33%)\n*Task <1> SubProcess*: Train - batches : 307, average loss: 1.8146, accuracy: 13128/39296 (33%)\n*Task <1> SubProcess*: Train - batches : 308, average loss: 1.8136, accuracy: 13189/39424 (33%)\n*Task <1> SubProcess*: Train - batches : 309, average loss: 1.8127, accuracy: 13245/39552 (33%)\n*Task <1> SubProcess*: Train - batches : 310, average loss: 1.8120, accuracy: 13299/39680 (34%)\n*Task <1> SubProcess*: Train - batches : 311, average loss: 1.8114, accuracy: 13352/39808 (34%)\n*Task <1> SubProcess*: Train - batches : 312, average loss: 1.8106, accuracy: 13407/39936 (34%)\n*Task <1> SubProcess*: Train - batches : 313, average loss: 1.8097, accuracy: 13452/40064 (34%)\n*Task <1> SubProcess*: Train - batches : 314, average loss: 1.8087, accuracy: 13513/40192 (34%)\n*Task <1> SubProcess*: Train - batches : 315, average loss: 1.8077, accuracy: 13568/40320 (34%)\n*Task <1> SubProcess*: Train - batches : 316, average loss: 1.8069, accuracy: 13624/40448 (34%)\n*Task <1> SubProcess*: Train - batches : 317, average loss: 1.8065, accuracy: 13668/40576 (34%)\n*Task <1> SubProcess*: Train - batches : 318, average loss: 1.8062, accuracy: 13723/40704 (34%)\n*Task <1> SubProcess*: Train - batches : 319, average loss: 1.8056, accuracy: 13778/40832 (34%)\n*Task <1> SubProcess*: Train - batches : 320, average loss: 1.8050, accuracy: 13826/40960 (34%)\n*Task <1> SubProcess*: Train - batches : 321, average loss: 1.8047, accuracy: 13869/41088 (34%)\n*Task <1> SubProcess*: Train - batches : 322, average loss: 1.8042, accuracy: 13914/41216 (34%)\n*Task <1> SubProcess*: Train - batches : 323, average loss: 1.8038, accuracy: 13957/41344 (34%)\n*Task <1> SubProcess*: Train - batches : 324, average loss: 1.8031, accuracy: 14010/41472 (34%)\n*Task <1> SubProcess*: Train - batches : 325, average loss: 1.8027, accuracy: 14065/41600 (34%)\n*Task <1> SubProcess*: Train - batches : 326, average loss: 1.8020, accuracy: 14120/41728 (34%)\n*Task <1> SubProcess*: Train - batches : 327, average loss: 1.8016, accuracy: 14164/41856 (34%)\n*Task <1> SubProcess*: Train - batches : 328, average loss: 1.8013, accuracy: 14215/41984 (34%)\n*Task <1> SubProcess*: Train - batches : 329, average loss: 1.8006, accuracy: 14276/42112 (34%)\n*Task <1> SubProcess*: Train - batches : 330, average loss: 1.7997, accuracy: 14340/42240 (34%)\n*Task <1> SubProcess*: Train - batches : 331, average loss: 1.7991, accuracy: 14389/42368 (34%)\n*Task <1> SubProcess*: Train - batches : 332, average loss: 1.7981, accuracy: 14443/42496 (34%)\n*Task <1> SubProcess*: Train - batches : 333, average loss: 1.7976, accuracy: 14492/42624 (34%)\n*Task <1> SubProcess*: Train - batches : 334, average loss: 1.7969, accuracy: 14545/42752 (34%)\n*Task <1> SubProcess*: Train - batches : 335, average loss: 1.7957, accuracy: 14602/42880 (34%)\n*Task <1> SubProcess*: Train - batches : 336, average loss: 1.7948, accuracy: 14661/43008 (34%)\n*Task <1> SubProcess*: Train - batches : 337, average loss: 1.7940, accuracy: 14722/43136 (34%)\n*Task <1> SubProcess*: Train - batches : 338, average loss: 1.7939, accuracy: 14769/43264 (34%)\n*Task <1> SubProcess*: Train - batches : 339, average loss: 1.7933, accuracy: 14825/43392 (34%)\n*Task <1> SubProcess*: Train - batches : 340, average loss: 1.7928, accuracy: 14876/43520 (34%)\n*Task <1> SubProcess*: Train - batches : 341, average loss: 1.7922, accuracy: 14928/43648 (34%)\n*Task <1> SubProcess*: Train - batches : 342, average loss: 1.7913, accuracy: 14988/43776 (34%)\n*Task <1> SubProcess*: Train - batches : 343, average loss: 1.7912, accuracy: 15033/43904 (34%)\n*Task <1> SubProcess*: Train - batches : 344, average loss: 1.7909, accuracy: 15075/44032 (34%)\n*Task <1> SubProcess*: Train - batches : 345, average loss: 1.7904, accuracy: 15123/44160 (34%)\n*Task <1> SubProcess*: Train - batches : 346, average loss: 1.7899, accuracy: 15175/44288 (34%)\n*Task <1> SubProcess*: Train - batches : 347, average loss: 1.7895, accuracy: 15225/44416 (34%)\n*Task <1> SubProcess*: Train - batches : 348, average loss: 1.7890, accuracy: 15280/44544 (34%)\n*Task <1> SubProcess*: Train - batches : 349, average loss: 1.7887, accuracy: 15326/44672 (34%)\n*Task <1> SubProcess*: Train - batches : 350, average loss: 1.7882, accuracy: 15383/44800 (34%)\n*Task <1> SubProcess*: Train - batches : 351, average loss: 1.7876, accuracy: 15444/44928 (34%)\n*Task <1> SubProcess*: Train - batches : 352, average loss: 1.7867, accuracy: 15505/45056 (34%)\n*Task <1> SubProcess*: Train - batches : 353, average loss: 1.7861, accuracy: 15550/45184 (34%)\n*Task <1> SubProcess*: Train - batches : 354, average loss: 1.7854, accuracy: 15600/45312 (34%)\n*Task <1> SubProcess*: Train - batches : 355, average loss: 1.7850, accuracy: 15645/45440 (34%)\n*Task <1> SubProcess*: Train - batches : 356, average loss: 1.7840, accuracy: 15701/45568 (34%)\n*Task <1> SubProcess*: Train - batches : 357, average loss: 1.7832, accuracy: 15753/45696 (34%)\n*Task <1> SubProcess*: Train - batches : 358, average loss: 1.7824, accuracy: 15819/45824 (35%)\n*Task <1> SubProcess*: Train - batches : 359, average loss: 1.7817, accuracy: 15873/45952 (35%)\n*Task <1> SubProcess*: Train - batches : 360, average loss: 1.7810, accuracy: 15925/46080 (35%)\n*Task <1> SubProcess*: Train - batches : 361, average loss: 1.7804, accuracy: 15981/46208 (35%)\n*Task <1> SubProcess*: Train - batches : 362, average loss: 1.7800, accuracy: 16037/46336 (35%)\n*Task <1> SubProcess*: Train - batches : 363, average loss: 1.7791, accuracy: 16095/46464 (35%)\n*Task <1> SubProcess*: Train - batches : 364, average loss: 1.7784, accuracy: 16148/46592 (35%)\n*Task <1> SubProcess*: Train - batches : 365, average loss: 1.7780, accuracy: 16200/46720 (35%)\n*Task <1> SubProcess*: Train - batches : 366, average loss: 1.7776, accuracy: 16245/46848 (35%)\n*Task <1> SubProcess*: Train - batches : 367, average loss: 1.7769, accuracy: 16302/46976 (35%)\n*Task <1> SubProcess*: Train - batches : 368, average loss: 1.7763, accuracy: 16359/47104 (35%)\n*Task <1> SubProcess*: Train - batches : 369, average loss: 1.7756, accuracy: 16423/47232 (35%)\n*Task <1> SubProcess*: Train - batches : 370, average loss: 1.7749, accuracy: 16477/47360 (35%)\n*Task <1> SubProcess*: Train - batches : 371, average loss: 1.7741, accuracy: 16538/47488 (35%)\n*Task <1> SubProcess*: Train - batches : 372, average loss: 1.7733, accuracy: 16595/47616 (35%)\n*Task <1> SubProcess*: Train - batches : 373, average loss: 1.7730, accuracy: 16643/47744 (35%)\n*Task <1> SubProcess*: Train - batches : 374, average loss: 1.7725, accuracy: 16698/47872 (35%)\n*Task <1> SubProcess*: Train - batches : 375, average loss: 1.7720, accuracy: 16752/48000 (35%)\n*Task <1> SubProcess*: Train - batches : 376, average loss: 1.7714, accuracy: 16806/48128 (35%)\n*Task <1> SubProcess*: Train - batches : 377, average loss: 1.7711, accuracy: 16861/48256 (35%)\n*Task <1> SubProcess*: Train - batches : 378, average loss: 1.7708, accuracy: 16906/48384 (35%)\n*Task <1> SubProcess*: Train - batches : 379, average loss: 1.7702, accuracy: 16957/48512 (35%)\n*Task <1> SubProcess*: Train - batches : 380, average loss: 1.7699, accuracy: 17007/48640 (35%)\n*Task <1> SubProcess*: Train - batches : 381, average loss: 1.7693, accuracy: 17062/48768 (35%)\n*Task <1> SubProcess*: Train - batches : 382, average loss: 1.7691, accuracy: 17113/48896 (35%)\n*Task <1> SubProcess*: Train - batches : 383, average loss: 1.7685, accuracy: 17170/49024 (35%)\n*Task <1> SubProcess*: Train - batches : 384, average loss: 1.7680, accuracy: 17223/49152 (35%)\n*Task <1> SubProcess*: Train - batches : 385, average loss: 1.7677, accuracy: 17275/49280 (35%)\n*Task <1> SubProcess*: Train - batches : 386, average loss: 1.7672, accuracy: 17329/49408 (35%)\n*Task <1> SubProcess*: Train - batches : 387, average loss: 1.7666, accuracy: 17388/49536 (35%)\n*Task <1> SubProcess*: Train - batches : 388, average loss: 1.7660, accuracy: 17440/49664 (35%)\n*Task <1> SubProcess*: Train - batches : 389, average loss: 1.7655, accuracy: 17496/49792 (35%)\n*Task <1> SubProcess*: Train - batches : 390, average loss: 1.7651, accuracy: 17544/49920 (35%)\n*Task <1> SubProcess*: Train - batches : 391, average loss: 1.7644, accuracy: 17583/50000 (35%)\n*Task <1> SubProcess*: Test - batches: 391, average loss: 0.0138, accuracy: 3796/10000 (38%)\n*Task <1> SubProcess*: 2021-02-08 21:19:39.685194 439 INFO Finish running user model with exit code 0\n*Task <1> SubProcess*: 2021-02-08 21:19:39.689747 439 INFO Finalize\n*Task <1> SubProcess*: Enforce permission\n*Task <1> SubProcess*: finish with rc=0\n*Task <1> SubProcess*: 2021-02-08 21:19:40.268725 308 finishing command. Results are:  0\n*Task <1> SubProcess*: 2021-02-08 21:19:40.268965 308 callback on_task_invoke_end is started\n*Task <1> SubProcess*: 2021-02-08 21:19:40.378255 308 callback on_task_invoke_end is finished\n*Task <1> SubProcess*: 2021-02-08 21:19:40.378479 308 on_task_invoke finished. return_code=0, time_cost=2.04 minutes\n*Task <1> SubProcess*: 2021-02-08 21:19:40.393417 37 INFO List GPUs\n*Task <1> SubProcess*: Mon Feb  8 21:19:40 2021       \n*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n*Task <1> SubProcess*: | NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\n*Task <1> SubProcess*: |-------------------------------+----------------------+----------------------+\n*Task <1> SubProcess*: | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n*Task <1> SubProcess*: | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n*Task <1> SubProcess*: |===============================+======================+======================|\n*Task <1> SubProcess*: |   0  Tesla V100-PCIE...  On   | 00000000:06:00.0 Off |                  Off |\n*Task <1> SubProcess*: | N/A   57C    P0    51W / 250W |      0MiB / 16160MiB |      0%      Default |\n*Task <1> SubProcess*: +-------------------------------+----------------------+----------------------+\n*Task <1> SubProcess*:                                                                                \n*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n*Task <1> SubProcess*: | Processes:                                                       GPU Memory |\n*Task <1> SubProcess*: |  GPU       PID   Type   Process name                             Usage      |\n*Task <1> SubProcess*: |=============================================================================|\n*Task <1> SubProcess*: |  No running processes found                                                 |\n*Task <1> SubProcess*: +-----------------------------------------------------------------------------+\n*Task <1> SubProcess*: 2021-02-08 21:19:40.447892 37 INFO NVIDIA_VISIBLE_DEVICES=GPU-2c33df03-e129-bd55-5ff2-1288461e4b44\n*Task <1> SubProcess*: 2021-02-08 21:19:40.452676 37 INFO NVIDIA_DRIVER_CAPABILITIES=compute,utility\n*Task <1> SubProcess*: 2021-02-08 21:19:40.467333 37 INFO Command exit with 0\n\n", "name": "stdout"}]}, {"metadata": {"id": "2e6b41ad92f648798297104d2c3ad558"}, "cell_type": "markdown", "source": "## Download trained model from Watson Machine Learning Accelerator "}, {"metadata": {"id": "d6f15e89a33948ea872a5e8499312ddd"}, "cell_type": "code", "source": "download_trained_model(r.json())", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "https://wmla-console-xwmla.apps.wml1x180.ma.platformlab.ibm.com/platform/rest/deeplearning/v1/execs/xwmla-1882/result\nSave model:  /project_data/data_asset/xwmla-1882.zip\n", "name": "stdout"}]}, {"metadata": {"id": "d54fd7cf1f2549b38b72c11a1e5293ae"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}