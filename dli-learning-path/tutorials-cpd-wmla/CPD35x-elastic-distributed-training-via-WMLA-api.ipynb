{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Accelerated Elastic Deep Learning Service in Cloud Pak for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook created by Kelvin Lui, Xue Yin Zhuang, Xue Zhou Yuan (January 2021)\n",
    "\n",
    "Watson Machine Learning Accelerator in Cloud Pak for Data offers GPU Accelerated Elastic Deep Learning service.   This service enables multiple data scientists to accelerate deep learning model training across multiple GPUs and server, share GPUs in a dynamic fashion,  and drives data scientist productivity and overall GPU utilization.\n",
    "\n",
    "In this notebook, you will learn how to scale PyTorch model with multiple GPUs with GPU Accelerated Elastic Deep Learning service, monitor the running job, and debug any issues seen.\n",
    "\n",
    "This notebook uses Watson Machine learning Accelerator 2.2 with Cloud Pak for Data 3.5. \n",
    "\n",
    "### Contents\n",
    "\n",
    "- [The big picture](#The-big-picture)\n",
    "- [Changes to your code](#Changes-to-your-code)\n",
    "- [Set up API end point and log on](#Set-up-API-end-point-and-log-on)\n",
    "- [Submit job via API](#Submit-job-via-API)\n",
    "- [Monitor running job](#Monitor-running-job)\n",
    "- [Training metrics and logs](#Training-metrics-and-logs)\n",
    "- [Download trained model](#Download-trained-model)\n",
    "- [Further information and useful links](#Further-information-and-useful-links)\n",
    "- [Appendix](#Appendix)\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The big picture\n",
    "[Back to top](#Contents)\n",
    "\n",
    "This notebook details the process of taking your PyTorch model and making the changes required to train the model using [IBM Watson Machine Learning GPU Accelerated Elastic Deep Learning service](https://developer.ibm.com/series/learning-path-get-started-with-watson-machine-learning-accelerator/) (WML Accelerator) \n",
    "\n",
    "\n",
    "The image below shows the various elements required to use Elastic Deep Learning Service. In this notebook we will step through each of these elements in more detail. Through this process you will offload your code to a WML Accelerator cluster, monitor the running job, retrieve the output and debug any issues seen. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/5_running_job.png) is also available.\n",
    "\n",
    "![overall](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/5_running_job.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to your code\n",
    "[Back to top](#Contents)\n",
    "\n",
    "In this section we will use the PyTorch Resnet 50 model and make the required changes needed to use this model with the elastic distributed training engine (EDT). An overview of these changes can be seen in the diagram below. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/2_code_adaptations.png) is also available.\n",
    "\n",
    "![code](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/2_code_adaptations.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key changes to your code in order to use elastic distributed training are the following:\n",
    "- Importing libraries and setting up environment variables\n",
    "- Data loading function for elastic distributed training\n",
    "- Extract parameters for training\n",
    "- Replace training and testing loops with the loop equivalents for elastic distributed training\n",
    "\n",
    "For the purpose of this tutorial we train RestNet50 model with Elastic Distributed Training (EDT).\n",
    "\n",
    "See the blog associated with this notebook with more detailed explanation of the above changes.\n",
    "https://developer.ibm.com/articles/elastic-distributed-training-edt-in-watson-machine-learning-accelerator/\n",
    "\n",
    "See more information about the Elastic Distributed Training API in \n",
        " [IBM Documentation](https://www.ibm.com/docs/en/wmla/2.2.0?topic=SSFHA8_2.2.0/wmla_workloads_elastic_distributed_training.html).\n",
    "\n",
    "\n",
    "\n",
    "Your modified code should be made available in a directory which also contains the EDT helper scripts: `edtcallback.py`, `emetrics.py` and `elog.py`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper methods",
        "\n",
            "Define the required helper methods. \n",
        "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from IPython.display import display, FileLink, clear_output\n",
    "\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import tarfile\n",
    "\n",
    "\n",
    "def query_job_status(job_id,refresh_rate=3) :\n",
    "\n",
    "    execURL = dl_rest_url  +'/execs/'+ job_id['id']\n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "    keep_running=True\n",
    "    res=None\n",
    "    while(keep_running):\n",
    "        res = req.get(execURL, headers=commonHeaders, verify=False)\n",
    "        monitoring = pd.DataFrame(res.json(), index=[0])\n",
    "        pd.set_option('max_colwidth', 120)\n",
    "        clear_output()\n",
    "        print(\"Refreshing every {} seconds\".format(refresh_rate))\n",
    "        display(monitoring)\n",
    "        pp.pprint(res.json())\n",
    "        if(res.json()['state'] not in ['PENDING_CRD_SCHEDULER', 'SUBMITTED','RUNNING']) :\n",
    "            keep_running=False\n",
    "        time.sleep(refresh_rate)\n",
    "    return res\n",
    "\n",
    "def query_executor_stdout_log(job_id) :\n",
    "\n",
    "    execURL = dl_rest_url  +'/scheduler/applications/'+ job_id['id'] + '/executor/1/logs/stdout?lastlines=1000'\n",
    "    #'https://{}/platform/rest/deeplearning/v1/scheduler/applications/wmla-267/driver/logs/stderr?lastlines=10'.format(hostname)\n",
    "    commonHeaders2={'accept': 'text/plain', 'X-Auth-Token': access_token}\n",
    "    print (execURL)\n",
    "    res = req.get(execURL, headers=commonHeaders2, verify=False)\n",
    "    print(res.text)\n",
    "    \n",
    "    \n",
    "def query_train_metric(job_id) :\n",
    "\n",
    "    #execURL = dl_rest_url  +'/execs/'+ job_id['id'] + '/log'\n",
    "    execURL = dl_rest_url  +'/execs/'+ job_id['id'] + '/log'\n",
    "    #'https://{}/platform/rest/deeplearning/v1/scheduler/applications/wmla-267/driver/logs/stderr?lastlines=10'.format(hostname)\n",
    "    commonHeaders2={'accept': 'text/plain', 'X-Auth-Token': access_token}\n",
    "    print (execURL)\n",
    "    res = req.get(execURL, headers=commonHeaders2, verify=False)\n",
    "    print(res.text)\n",
    "\n",
    "    # save result file    \n",
    "def download_trained_model(job_id) :\n",
    "\n",
    "    from IPython.display import display, FileLink\n",
    "\n",
    "    # save result file\n",
    "    commonHeaders3={'accept': 'application/octet-stream', 'X-Auth-Token': access_token}\n",
    "    execURL = dl_rest_url  +'/execs/'+ r.json()['id'] + '/result'\n",
    "    res = req.get(execURL, headers=commonHeaders3, verify=False, stream=True)\n",
    "    print (execURL)\n",
    "\n",
    "    tmpfile = model_dir + '/' + r.json()['id'] +'.zip'\n",
    "    print ('Save model: ', tmpfile )\n",
    "    with open(tmpfile,'wb') as f:\n",
    "        f.write(res.content)\n",
    "        f.close()\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_dir = f'./resnet-wmla' \n",
    "model_main = f'elastic-main.py'\n",
    "model_callback = f'edtcallback.py'\n",
    "model_elog = f'elog.py'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet50 model: elastic-main.py\n",
    "This is the main file that is required by the elastic distributed training engine. It acts as the program main entrance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./resnet-wmla/elastic-main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_main}\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from callbacks import Callback\n",
    "from fabric_model import FabricModel\n",
    "from edtcallback import EDTLoggerCallback\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "## Define model and extract training parameters\n",
    "def get_max_worker():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='EDT Example')\n",
    "    parser.add_argument('--numWorker', type=int, default='16', help='input the max number ')\n",
    "    parser.add_argument('--gpuPerWorker', type=int, default='1', help='input the path of initial weight file')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    num_worker = args.numWorker * args.gpuPerWorker\n",
    "    print ('args.numWorker: ', args.numWorker , 'args.gpuPerWorker: ', args.gpuPerWorker)\n",
    "    return num_worker\n",
    "\n",
    "BATCH_SIZE_PER_DEVICE = 64\n",
    "NUM_EPOCHS = 3\n",
    "MAX_NUM_WORKERS = get_max_worker()\n",
    "START_LEARNING_RATE = 0.4\n",
    "LR_STEP_SIZE = 30\n",
    "LR_GAMMA = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "## Define dataset location \n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "if DATA_DIR is None:\n",
    "    DATA_DIR = '/tmp'\n",
    "print(\"DATA_DIR: \" + DATA_DIR)\n",
    "TRAIN_DATA = DATA_DIR + \"/cifar10\"\n",
    "TEST_DATA = DATA_DIR + \"/cifar10\"\n",
    "\n",
    "\n",
    "## <Xue Yin>  Documentation of Callback function\n",
    "class LRScheduleCallback(Callback):\n",
    "    def __init__(self, step_size, gamma):\n",
    "        super(LRScheduleCallback, self).__init__()\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        if (epoch != 0) and (epoch % self.step_size == 0):\n",
    "            for param_group in self.params['optimizer'].param_groups:\n",
    "                param_group['lr'] *= self.gamma\n",
    "\n",
    "        print(\"LRScheduleCallback epoch={}, learning_rate={}\".format(epoch,\n",
    "              self.params['optimizer'].param_groups[0]['lr']))\n",
    "\n",
    "## Data loading function for EDT\n",
    "def getDatasets():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    return (torchvision.datasets.CIFAR10(root=TRAIN_DATA, train=True, download=True, transform=transform_train),\n",
    "            torchvision.datasets.CIFAR10(root=TEST_DATA, train=False, download=True, transform=transform_test))\n",
    "\n",
    "def custom_train(model, data, eva, train_loader, fn_args):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    opt = model.get_optimizer()\n",
    "    opt.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    cri = model.get_loss_function()\n",
    "    loss = cri(outputs, labels)\n",
    "    loss.backward()\n",
    "    acc = eva(outputs, labels)\n",
    "    return acc, loss\n",
    "\n",
    "def custom_test(model, test_iter, fn_args):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    cri = model.get_loss_function()\n",
    "    valid_loss = 0.0\n",
    "    counter = 0\n",
    "    for(inputs, labels) in test_iter:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = cri(output, labels)\n",
    "        valid_loss += loss.item()\n",
    "        counter += 1\n",
    "    valid_loss /= counter\n",
    "    return valid_loss\n",
    "\n",
    "def main(model_type):\n",
    "    print('==> Building model..' + str(model_type))\n",
    "    model = models.__dict__[model_type]()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=START_LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    loss_function = F.cross_entropy\n",
    "    \n",
    "    edt_m = FabricModel(model, getDatasets, loss_function, optimizer, enable_onnx=True, fn_step_train=custom_train, fn_test=custom_test, user_callback=[LRScheduleCallback(LR_STEP_SIZE, LR_GAMMA)],  driver_logger=EDTLoggerCallback())\n",
    "    print('==> epochs:' + str(NUM_EPOCHS) + ', batchsize:' + str(BATCH_SIZE_PER_DEVICE) + ', engines_number:' + str(MAX_NUM_WORKERS))\n",
    "    edt_m.train(NUM_EPOCHS, BATCH_SIZE_PER_DEVICE, MAX_NUM_WORKERS, num_dataloader_threads=4, validation_freq=10, checkpoint_freq=0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(\"resnet50\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDT helper scripts: edtcallback.py\n",
    "The edtcallback.py scripts counts model loss and accuracy and logs them to a the driver log. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./resnet-wmla/edtcallback.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_callback}\n",
    "#! /usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from callbacks import LoggerCallback\n",
    "from emetrics import EMetrics\n",
    "from elog import ELog\n",
    "\n",
    "'''\n",
    "    EDTLoggerCallback class define LoggerCallback to trigger Elog.\n",
    "'''\n",
    "\n",
    "class EDTLoggerCallback(LoggerCallback):\n",
    "    def __init__(self):\n",
    "        self.gs =0\n",
    "\n",
    "    def log_train_metrics(self, loss, acc, completed_batch,  worker=0):\n",
    "        acc = acc/100.0\n",
    "        self.gs += 1\n",
    "        with EMetrics.open() as em:\n",
    "            em.record(EMetrics.TEST_GROUP,completed_batch,{'loss': loss, 'accuracy': acc})\n",
    "        with ELog.open() as log:\n",
    "            log.recordTrain(\"Train\", completed_batch, self.gs, loss, acc, worker)\n",
    "\n",
    "    def log_test_metrics(self, loss, acc, completed_batch, worker=0):\n",
    "        acc = acc/100.0\n",
    "        with ELog.open() as log:\n",
    "            log.recordTest(\"Test\", loss, acc, worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDT helper scripts: elog.py\n",
    "The elog.py script defines the path and content of the training and test log. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./resnet-wmla/elog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/{model_elog}\n",
    "import time\n",
    "import os\n",
    "\n",
    "'''\n",
    "    ELog class define the path and content of train and test log.\n",
    "'''\n",
    "\n",
    "class ELog(object):\n",
    "\n",
    "    def __init__(self,subId,f):\n",
    "        if \"TRAINING_ID\" in os.environ:\n",
    "            self.trainingId = os.environ[\"TRAINING_ID\"]\n",
    "        elif \"DLI_EXECID\" in os.environ:\n",
    "            self.trainingId = os.environ[\"DLI_EXECID\"]\n",
    "        else:\n",
    "            self.trainingId = \"\"\n",
    "        self.subId = subId\n",
    "        self.f = f\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, tb):\n",
    "        self.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def open(subId=None):\n",
    "        if \"LOG_DIR\" in os.environ:\n",
    "            folder = os.environ[\"LOG_DIR\"]\n",
    "        elif \"JOB_STATE_DIR\" in os.environ:\n",
    "            folder = os.path.join(os.environ[\"JOB_STATE_DIR\"],\"logs\")\n",
    "        else:\n",
    "            folder = \"/tmp\"\n",
    "\n",
    "        if subId is not None:\n",
    "            folder = os.path.join(folder, subId)\n",
    "\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        f = open(os.path.join(folder, \"stdout\"), \"a\")\n",
    "        return ELog(subId,f)\n",
    "\n",
    "    def recordText(self,text):\n",
    "        timestr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        timestr = \"[\"+ timestr + \"]\"\n",
    "        if self.f:\n",
    "            self.f.write(timestr + \" \" + text + \"\\n\")\n",
    "            self.f.flush()\n",
    "\n",
    "    def recordTrain(self,title,iteration,global_steps,loss,accuracy,worker):\n",
    "        text = title\n",
    "        text = text + \",\tTimestamp: \" + str(int(round(time.time() * 1000)))\n",
    "        text = text + \",\tGlobal steps: \" + str(global_steps)\n",
    "        text = text + \",\tIteration: \" + str(iteration)\n",
    "        text = text + \",\tLoss: \" + str(float('%.5f' % loss) )\n",
    "        text = text + \",\tAccuracy: \" + str(float('%.5f' % accuracy) )\n",
    "        self.recordText(text)\n",
    "\n",
    "    def recordTest(self,title,loss,accuracy,worker):\n",
    "        text = title\n",
    "        text = text + \",\tTimestamp: \" + str(int(round(time.time() * 1000)))\n",
    "        text = text + \",\tLoss: \" + str(float('%.5f' % loss) )\n",
    "        text = text + \",\tAccuracy: \" + str(float('%.5f' % accuracy) )\n",
    "        self.recordText(text)\n",
    "\n",
    "    def close(self):\n",
    "        if self.f:\n",
    "            self.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Package model files for training\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [24, 8.0]\n",
    "#import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "#Package the updated model files into a tar file ending with `.modelDir.tar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tempFile: /var/folders/5n/bsvbwc4x2pv391y0zqg1b22c0000gn/T/tmpgft46na3.modelDir.tar\n"
     ]
    }
   ],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "\n",
    "MODEL_DIR_SUFFIX = \".modelDir.tar\"\n",
    "tempFile = tempfile.mktemp(MODEL_DIR_SUFFIX)\n",
    "\n",
    "make_tarfile(tempFile, model_dir)\n",
    "\n",
    "print(\" tempFile: \" + tempFile)\n",
    "files = {'file': open(tempFile, 'rb')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up API end point and log on\n",
    "[Back to top](#Contents)\n",
    "\n",
    "In this section we set up the API endpoint which will be used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections use the Watson ML Accelerator API to complete the various tasks required. \n",
    "We've given examples of a number of tasks but you should refer to the documentation at to see more details \n",
    "of what is possible and sample output you might expect.\n",
    "\n",
    "- https://www.ibm.com/support/knowledgecenter/SSFHA8_2.2.0/cm/deeplearning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "# from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [24, 8.0]\n",
    "#import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "import base64\n",
    "import urllib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZHNlX3VzZXI6Y3BkNGV2ZXI=\n",
      "https://wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com/auth/v1/logon\n",
      "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImRzZV91c2VyIiwicm9sZSI6IlVzZXIiLCJwZXJtaXNzaW9ucyI6WyJhY2Nlc3NfY2F0YWxvZyIsImNhbl9wcm92aXNpb24iXSwiZ3JvdXBzIjpbMTAwMDBdLCJzdWIiOiJkc2VfdXNlciIsImlzcyI6IktOT1hTU08iLCJhdWQiOiJEU1giLCJ1aWQiOiIxMDAwMzMxMDAxIiwiYXV0aGVudGljYXRvciI6ImRlZmF1bHQiLCJpYXQiOjE2MTUxNzM4NzQsImV4cCI6MTYxNTIxNzAzOH0.DF3bdqAcuFVomGZ1lYMg4AqLPGmJRY1T0sXZwcK1urpVCV6gqIIKeqPmhrp3mUABkJ5R8M4h3oJi4-ul5EjPs10IJg4hm3dJDFFtgekK2jVBhesTuVMK7dEzh0SFm755YcRVtUrvHyA2s702pOWpNswddZVjG15BJVASGXFDz0sXZWjNHchRxjOztGvqvv2YkDSGQh6sKraLlQL2NThDI5ZdfBQ0Lub-fvDYon9lFoWEVEUW8cg0EbCXyDdt7xdgKZ8ar7hOPDdjSpe93YEltMlMwH1RiPr4bGLcggn0VrKC223I-Dys3UmJ2xoXpVIFBcnb0KgyOMqchmwMzmSuoA\n"
     ]
    }
   ],
   "source": [
    "#hostname='wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com'  # please enter Watson Machine Learning Accelerator host name\n",
    "#hostname = 'wmla-console-liqbj.apps.wml1x210.ma.platformlab.ibm.com'\n",
    "hostname = 'wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com'\n",
    "login='dse_user:cpd4ever' # please enter the login and password\n",
    "# hostname='wmla-console-xwmla.apps.wml1x180.ma.platformlab.ibm.com'\n",
    "# login='admin:password'\n",
    "\n",
    "es = base64.b64encode(login.encode('utf-8')).decode(\"utf-8\")\n",
    "print(es)\n",
    "commonHeaders={'Authorization': 'Basic '+es}\n",
    "req = requests.Session()\n",
    "auth_url = 'https://{}/auth/v1/logon'.format(hostname)\n",
    "print(auth_url)\n",
    "a=requests.get(auth_url,headers=commonHeaders, verify=False)\n",
    "access_token=a.json()['accessToken']\n",
    "print(access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log on\n",
    "\n",
    "\n",
    "Obtain login session tokens to be used for session authentication within the RESTful API. Tokens are valid for 8 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_rest_url = 'https://{}/platform/rest/deeplearning/v1'.format(hostname)\n",
    "commonHeaders={'accept': 'application/json', 'X-Auth-Token': access_token}\n",
    "req = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check deep learning framework details\n",
    "\n",
    "Check what framework plugins are available and see example execution commands.  In this demonstration we will use **edtPyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"PyTorch\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"PyTorch\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start PyTorch <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\",\n",
      "            \"\",\n",
      "            \"Prebuilt Models:\",\n",
      "            \"  $ python dlicmd.py --exec-start PyTorch <connection-options> <prebuilt-model-params>\",\n",
      "            \"\",\n",
      "            \"  where:\",\n",
      "            \"    <prebuilt-model-params>:\",\n",
      "            \"      --pbmodel-cmd <command>: <command> is 'train'\",\n",
      "            \"        Specify 'train' to train the prebuilt models below with fake data\",\n",
      "            \"      --pbmodel-name <name>:\",\n",
      "            \"        For 'train', <name> can be either 'AlexNet', 'VGG16', or 'ResNet18'\",\n",
      "            \"      --epochs. Optional for 'train'. Default 10\",\n",
      "            \"      --batch-size. Optional for 'train'. Default 20\",\n",
      "            \"      --lr. Learning rate. Optional for 'train'. Default 0.01\",\n",
      "            \"      --momentum. Optional for 'train'. Default 0.9\",\n",
      "            \"  Examples:\",\n",
      "            \"    $ python dlicmd.py --exec-start PyTorch --rest-host localhost --pbmodel-cmd train --pbmodel-name AlexNet --epochs 10 --batch-size 12\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"\",\n",
      "        \"execMode\": \"single\",\n",
      "        \"appName\": \"SingleNodePytorchTrain\",\n",
      "        \"numWorkers\": 1,\n",
      "        \"maxWorkers\": 1,\n",
      "        \"workerMemory\": \"4G\",\n",
      "        \"frameworkCmdGenerator\": \"PyTorchCmdGen.py\",\n",
      "        \"backend\": \"PyTorch\",\n",
      "        \"prebuiltModelMain\": \"PyTorchPrebuiltMain.py\",\n",
      "        \"prebuiltModelDir\": \"PyTorch\",\n",
      "        \"frameworkVersion\": \"1.3.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"distPyTorch\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Distributed PyTorch\",\n",
      "            \"You only have to specify the number of workers as shown in the example below,\",\n",
      "            \"and the following environment variables will be available when the model runs:\",\n",
      "            \"MASTER_ADDR, MASTER_PORT, WORLD_SIZE, RANK\",\n",
      "            \"Use --gpuPerWorker flag to specify number of GPUs per worker.\",\n",
      "            \"The maximum number of worker is 1024.\",\n",
      "            \"The maximum number of GPUs per worker is 2.\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start distPyTorch <connection-options> --model-main mnist.py --numWorker 2 --gpuPerWorker 1\",\n",
      "            \"\",\n",
      "            \"Prebuilt Models:\",\n",
      "            \"  $ python dlicmd.py --exec-start PyTorch <connection-options> <prebuilt-model-params>\",\n",
      "            \"\",\n",
      "            \"  where:\",\n",
      "            \"    <prebuilt-model-params>:\",\n",
      "            \"      --pbmodel-cmd <command>: <command> is 'train'\",\n",
      "            \"        Specify 'train' to train the prebuilt models below with fake data\",\n",
      "            \"      --pbmodel-name <name>:\",\n",
      "            \"        For 'train', <name> can be either 'AlexNet', 'VGG16', or 'ResNet18'\",\n",
      "            \"      --epochs. Optional for 'train'. Default 10\",\n",
      "            \"      --batch-size. Optional for 'train'. Default 20\",\n",
      "            \"      --lr. Learning rate. Optional for 'train'. Default 0.01\",\n",
      "            \"      --momentum. Optional for 'train'. Default 0.9\",\n",
      "            \"  Examples:\",\n",
      "            \"    $ python dlicmd.py --exec-start distPyTorch --rest-host localhost --numWorker 2 --pbmodel-cmd train --pbmodel-name AlexNet --epochs 10 --batch-size 12\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"false\",\n",
      "        \"execMode\": \"distributed\",\n",
      "        \"appName\": \"DistributePyTorchTrain\",\n",
      "        \"exports\": {\n",
      "            \"GLOO_SOCKET_IFNAME\": \"eth0\"\n",
      "        },\n",
      "        \"numWorkers\": 1,\n",
      "        \"frameworkCmdGenerator\": \"distPyTorchCmdGen.py\",\n",
      "        \"backend\": \"PyTorch\",\n",
      "        \"prebuiltModelMain\": \"PyTorchPrebuiltMain.py\",\n",
      "        \"prebuiltModelDir\": \"PyTorch\",\n",
      "        \"frameworkVersion\": \"1.3.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"disttensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Distributed TensorFlow\",\n",
      "            \"Instead of passing parameters such as ps_hosts, worker_hosts, specify --numPs\",\n",
      "            \"as in example below. Parameter servers (ps) and worker hosts will be allocated\",\n",
      "            \"dynamically.\",\n",
      "            \"Use --gpuPerWorker flag to specify number of GPUs per worker.\",\n",
      "            \"The maximum number of worker is 2.\",\n",
      "            \"The maximum number of GPUs per worker is 2.\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start disttensorflow <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py --numPs 1 --numWorker 1 --gpuPerWorker 1\",\n",
      "            \"$ python dlicmd.py --exec-start disttensorflow <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py --numPs 1 --numWorker 2 --gpuPerWorker 1\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"false\",\n",
      "        \"execMode\": \"distributed\",\n",
      "        \"appName\": \"DistributeTensorflow2xTrain\",\n",
      "        \"numWorkers\": 1,\n",
      "        \"workerMemory\": \"8G\",\n",
      "        \"frameworkCmdGenerator\": \"disttensorflowCmdGen.py\",\n",
      "        \"distributeStrategy\": \"MultiWorkerMirroredStrategy\",\n",
      "        \"numPs\": 1,\n",
      "        \"backend\": \"TensorFlow\",\n",
      "        \"frameworkVersion\": \"2.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"edtPyTorch\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"PyTorch - IBM Elastic Distributed Training (EDT)\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start edtPyTorch <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\",\n",
      "            \"\",\n",
      "            \"Prebuilt Models:\",\n",
      "            \"  $ python dlicmd.py --exec-start edtPyTorch <connection-options> <prebuilt-model-params>\",\n",
      "            \"\",\n",
      "            \"  where:\",\n",
      "            \"    <prebuilt-model-params>:\",\n",
      "            \"      --pbmodel-cmd <command>: <command> is 'train'\",\n",
      "            \"        Specify 'train' to train the prebuilt models below\",\n",
      "            \"      --pbmodel-name <name>:\",\n",
      "            \"        For 'train', <name> is 'MNIST'\",\n",
      "            \"      --epochs. Optional for 'train'. Default 10\",\n",
      "            \"      --batch-size. Optional for 'train'. Default 20\",\n",
      "            \"      --lr. Learning rate. Optional for 'train'. Default 0.01\",\n",
      "            \"      --momentum. Optional for 'train'. Default 0.9\",\n",
      "            \"  Examples:\",\n",
      "            \"    $ python dlicmd.py --exec-start edtPyTorch --rest-host localhost --numWorker 2 --pbmodel-cmd train --pbmodel-name MNIST --epochs 10 --batch-size 12\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"\",\n",
      "        \"execMode\": \"distributed\",\n",
      "        \"appName\": \"ElasticPyTorchTrain\",\n",
      "        \"workerMemory\": \"4G\",\n",
      "        \"frameworkCmdGenerator\": \"edtCmdGen.py\",\n",
      "        \"backend\": \"pytorch\",\n",
      "        \"prebuiltModelMain\": \"PyTorchPrebuiltMain.py\",\n",
      "        \"prebuiltModelDir\": \"edtPyTorch\",\n",
      "        \"frameworkVersion\": \"1.3.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"edtTensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Tensorflow - IBM Elastic Distributed Training (EDT)\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start edtTf1x <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"\",\n",
      "        \"execMode\": \"distributed\",\n",
      "        \"appName\": \"ElasticTensorflow1xTrain\",\n",
      "        \"workerMemory\": \"8G\",\n",
      "        \"frameworkCmdGenerator\": \"edtCmdGen.py\",\n",
      "        \"backend\": \"tensorflow\",\n",
      "        \"frameworkVersion\": \"2.1\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"tensorflow\",\n",
      "        \"description\": \"\",\n",
      "        \"desc\": [\n",
      "            \"Single-node TensorFlow\",\n",
      "            \"NOTES:\",\n",
      "            \"- Since DLI manages GPU allocation, if you explicitly assign devices using\",\n",
      "            \"  calls such as `tf.device`, you should use Tensorflow configuration flag\",\n",
      "            \"  `allow_soft_placement=True`\",\n",
      "            \"\",\n",
      "            \"Prebuilt Models:\",\n",
      "            \"  $ python dlicmd.py --exec-start tensorflow <connection-options> <prebuilt-model-params>\",\n",
      "            \"\",\n",
      "            \"  where:\",\n",
      "            \"    <prebuilt-model-params>:\",\n",
      "            \"      --pbmodel-cmd <command>: <command> is 'train'\",\n",
      "            \"        Specify 'train' to train the prebuilt models\",\n",
      "            \"      --pbmodel-name <name>:\",\n",
      "            \"        For 'train', <name> can be 'MNIST'\",\n",
      "            \"      --epochs. Optional for 'train'. Default 10\",\n",
      "            \"      --batch-size. Optional for 'train'. Default 20\",\n",
      "            \"      --lr. Learning rate. Optional for 'train'. Default 0.01\",\n",
      "            \"      --momentum. Optional for 'train'. Default 0.9\",\n",
      "            \"  Examples:\",\n",
      "            \"    $ python dlicmd.py --exec-start tensorflow --rest-host localhost --pbmodel-cmd train --pbmodel-name MNIST --epochs 10 --batch-size 12\",\n",
      "            \"Examples:\",\n",
      "            \"$ python dlicmd.py --exec-start tensorflow <connection-options> --cs-datastore-meta type=fs,data_path=mnist --model-main mnist.py\"\n",
      "        ],\n",
      "        \"deployMode\": \"cluster\",\n",
      "        \"hasTask0\": \"\",\n",
      "        \"execMode\": \"single\",\n",
      "        \"appName\": \"SingleNodeTensorflowTrain\",\n",
      "        \"numWorkers\": 1,\n",
      "        \"maxWorkers\": 1,\n",
      "        \"workerMemory\": \"8G\",\n",
      "        \"frameworkCmdGenerator\": \"tensorflowCmdGen.py\",\n",
      "        \"backend\": \"TensorFlow\",\n",
      "        \"prebuiltModelMain\": \"tensorflowPrebuiltMain.py\",\n",
      "        \"prebuiltModelDir\": \"tensorflow\",\n",
      "        \"frameworkVersion\": \"2.1\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(dl_rest_url+'/execs/frameworks', headers=commonHeaders, verify=False).json()\n",
    "# Using the raw json, easier to see the examples given\n",
    "print(json.dumps(r, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job via API\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Now we need to structure our API job submission. There are various elements to this process as seen in the diagram below. Note that **this** Jupyter notebook is the one referred to below. A [static version](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/4_api_setup.png) is also available.\n",
    "\n",
    "![code](https://github.com/IBM/wmla-assets/raw/master/WMLA-learning-journey/shared-images/4_api_setup.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "framework_name = 'edtPyTorch' # DL Framework to use, from list given above\n",
    "local_dir_containing_your_code = 'resnet-wmla'\n",
    "number_of_GPU = '2' # number of GPUs for elastic distribution\n",
    "name_of_your_code_file = 'elastic-main.py' # Main model file as opened locally above\n",
    "\n",
    "\n",
    "args = '--exec-start {} \\\n",
    "        --cs-datastore-meta type=fs\\\n",
    "        --model-dir {} \\\n",
    "        --numWorker={} \\\n",
    "        --model-main {} \\\n",
    "        '.format(framework_name, local_dir_containing_your_code, number_of_GPU, name_of_your_code_file)\n",
    "\n",
    "print (\"args: \" + args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: --exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker 2                      --model-main elastic-main.py --model-dir resnet-wmla\n"
     ]
    }
   ],
   "source": [
    "framework_name = 'edtPyTorch' # DL Framework to use, from list given above\n",
    "#dataset_location = 'pytorch-mnist' # relative path of your data set under $DLI_DATA_FS\n",
    "local_dir_containing_your_code = 'resnet-wmla'\n",
    "number_of_GPU = '2' # number of GPUs for elastic distribution\n",
    "name_of_your_code_file = 'elastic-main.py' # Main model file as opened locally above\n",
    "\n",
    "args = '--exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker 2 \\\n",
    "                     --model-main elastic-main.py --model-dir resnet-wmla'\n",
    "\n",
    "    \n",
    "print (\"args: \" + args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor running job\n",
    "[Back to top](#Contents)\n",
    "\n",
    "Once the job is submitted successfully we can monitor the running job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing every 5 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>args</th>\n",
       "      <th>submissionId</th>\n",
       "      <th>creator</th>\n",
       "      <th>state</th>\n",
       "      <th>appId</th>\n",
       "      <th>schedulerUrl</th>\n",
       "      <th>modelFileOwnerName</th>\n",
       "      <th>workDir</th>\n",
       "      <th>appName</th>\n",
       "      <th>createTime</th>\n",
       "      <th>elastic</th>\n",
       "      <th>nameSpace</th>\n",
       "      <th>numWorker</th>\n",
       "      <th>framework</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>wmla-398</td>\n",
       "      <td>--exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker 2                      --model-main elastic-main.py...</td>\n",
       "      <td>wmla-398</td>\n",
       "      <td>dse_user</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>wmla-398</td>\n",
       "      <td>https://wmla-mss:9080</td>\n",
       "      <td>wmla</td>\n",
       "      <td>/gpfs/myresultfs/dse_user/batchworkdir/wmla-398/_submitted_code/resnet-wmla</td>\n",
       "      <td>ElasticPyTorchTrain</td>\n",
       "      <td>2021-03-08T03:24:40Z</td>\n",
       "      <td>True</td>\n",
       "      <td>wmla</td>\n",
       "      <td>2</td>\n",
       "      <td>edtPyTorch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "0  wmla-398   \n",
       "\n",
       "                                                                                                                      args  \\\n",
       "0  --exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker 2                      --model-main elastic-main.py...   \n",
       "\n",
       "  submissionId   creator     state     appId           schedulerUrl  \\\n",
       "0     wmla-398  dse_user  FINISHED  wmla-398  https://wmla-mss:9080   \n",
       "\n",
       "  modelFileOwnerName  \\\n",
       "0               wmla   \n",
       "\n",
       "                                                                       workDir  \\\n",
       "0  /gpfs/myresultfs/dse_user/batchworkdir/wmla-398/_submitted_code/resnet-wmla   \n",
       "\n",
       "               appName            createTime  elastic nameSpace  numWorker  \\\n",
       "0  ElasticPyTorchTrain  2021-03-08T03:24:40Z     True      wmla          2   \n",
       "\n",
       "    framework  \n",
       "0  edtPyTorch  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'appId': 'wmla-398',\n",
      "  'appName': 'ElasticPyTorchTrain',\n",
      "  'args': '--exec-start edtPyTorch --cs-datastore-meta type=fs  --numWorker '\n",
      "          '2                      --model-main elastic-main.py --model-dir '\n",
      "          'resnet-wmla ',\n",
      "  'createTime': '2021-03-08T03:24:40Z',\n",
      "  'creator': 'dse_user',\n",
      "  'elastic': True,\n",
      "  'framework': 'edtPyTorch',\n",
      "  'id': 'wmla-398',\n",
      "  'modelFileOwnerName': 'wmla',\n",
      "  'nameSpace': 'wmla',\n",
      "  'numWorker': 2,\n",
      "  'schedulerUrl': 'https://wmla-mss:9080',\n",
      "  'state': 'FINISHED',\n",
      "  'submissionId': 'wmla-398',\n",
      "  'workDir': '/gpfs/myresultfs/dse_user/batchworkdir/wmla-398/_submitted_code/resnet-wmla'}\n"
     ]
    }
   ],
   "source": [
    "r = requests.post(dl_rest_url+'/execs?args='+args, files=files, \n",
    "                  headers=commonHeaders, verify=False)\n",
    "\n",
    "\n",
    "if not r.ok:\n",
    "    print('submit job failed: code=%s, %s'%(r.status_code, r.content))\n",
    "\n",
    "\n",
    "job_status = query_job_status(r.json(),refresh_rate=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training metrics and logs\n",
    "\n",
    "#### Retrieve and display the model training metrics:\n",
    "[Back to top](#Contents)\n",
    "\n",
    "After the job completes then we can retrieve the output, logs and saved models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com/platform/rest/deeplearning/v1/scheduler/applications/wmla-398/executor/1/logs/stdout?lastlines=1000\n",
      "Iteration 1939: tag train_loss, simple_value 2.31841\n",
      "Timestamp 1615174255910, Iteration 975\n",
      "batches :975 2.2616612911224365\n",
      "Iteration 1941: tag train_accuracy, simple_value 0.09954\n",
      "Iteration 1941: tag train_loss, simple_value 2.26166\n",
      "Timestamp 1615174256121, Iteration 976\n",
      "batches :976 2.2919421195983887\n",
      "Iteration 1943: tag train_accuracy, simple_value 0.09955\n",
      "Iteration 1943: tag train_loss, simple_value 2.29194\n",
      "Timestamp 1615174256336, Iteration 977\n",
      "batches :977 2.340242385864258\n",
      "Iteration 1945: tag train_accuracy, simple_value 0.09932\n",
      "Iteration 1945: tag train_loss, simple_value 2.34024\n",
      "Timestamp 1615174256603, Iteration 978\n",
      "batches :978 2.310180425643921\n",
      "Iteration 1947: tag train_accuracy, simple_value 0.09913\n",
      "Iteration 1947: tag train_loss, simple_value 2.31018\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174257099, Iteration 979\n",
      "batches :979 2.2831058502197266\n",
      "Iteration 1949: tag train_accuracy, simple_value 0.09926\n",
      "Iteration 1949: tag train_loss, simple_value 2.28311\n",
      "Timestamp 1615174257317, Iteration 980\n",
      "batches :980 2.31589674949646\n",
      "Iteration 1951: tag train_accuracy, simple_value 0.09943\n",
      "Iteration 1951: tag train_loss, simple_value 2.3159\n",
      "Timestamp 1615174257599, Iteration 981\n",
      "batches :981 2.308974027633667\n",
      "Iteration 1953: tag train_accuracy, simple_value 0.0994\n",
      "Iteration 1953: tag train_loss, simple_value 2.30897\n",
      "Timestamp 1615174257814, Iteration 982\n",
      "batches :982 2.331015110015869\n",
      "Iteration 1955: tag train_accuracy, simple_value 0.09946\n",
      "Iteration 1955: tag train_loss, simple_value 2.33102\n",
      "Timestamp 1615174258096, Iteration 983\n",
      "batches :983 2.3325037956237793\n",
      "Iteration 1957: tag train_accuracy, simple_value 0.09947\n",
      "Iteration 1957: tag train_loss, simple_value 2.3325\n",
      "Timestamp 1615174258309, Iteration 984\n",
      "batches :984 2.298051595687866\n",
      "Iteration 1959: tag train_accuracy, simple_value 0.09956\n",
      "Iteration 1959: tag train_loss, simple_value 2.29805\n",
      "Timestamp 1615174258523, Iteration 985\n",
      "batches :985 2.2885780334472656\n",
      "Iteration 1961: tag train_accuracy, simple_value 0.09972\n",
      "Iteration 1961: tag train_loss, simple_value 2.28858\n",
      "Timestamp 1615174258738, Iteration 986\n",
      "batches :986 2.315380096435547\n",
      "Iteration 1963: tag train_accuracy, simple_value 0.09965\n",
      "Iteration 1963: tag train_loss, simple_value 2.31538\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174259127, Iteration 987\n",
      "batches :987 2.2908849716186523\n",
      "Iteration 1965: tag train_accuracy, simple_value 0.09966\n",
      "Iteration 1965: tag train_loss, simple_value 2.29088\n",
      "Timestamp 1615174259344, Iteration 988\n",
      "batches :988 2.286316156387329\n",
      "Iteration 1967: tag train_accuracy, simple_value 0.09983\n",
      "Iteration 1967: tag train_loss, simple_value 2.28632\n",
      "Timestamp 1615174259565, Iteration 989\n",
      "batches :989 2.2677001953125\n",
      "Iteration 1969: tag train_accuracy, simple_value 0.09995\n",
      "Iteration 1969: tag train_loss, simple_value 2.2677\n",
      "Timestamp 1615174259780, Iteration 990\n",
      "batches :990 2.308093309402466\n",
      "Iteration 1971: tag train_accuracy, simple_value 0.09977\n",
      "Iteration 1971: tag train_loss, simple_value 2.30809\n",
      "Timestamp 1615174259993, Iteration 991\n",
      "batches :991 2.308436393737793\n",
      "Iteration 1973: tag train_accuracy, simple_value 0.09974\n",
      "Iteration 1973: tag train_loss, simple_value 2.30844\n",
      "Timestamp 1615174260209, Iteration 992\n",
      "batches :992 2.2838523387908936\n",
      "Iteration 1975: tag train_accuracy, simple_value 0.0999\n",
      "Iteration 1975: tag train_loss, simple_value 2.28385\n",
      "Timestamp 1615174260424, Iteration 993\n",
      "batches :993 2.2900941371917725\n",
      "Iteration 1977: tag train_accuracy, simple_value 0.10006\n",
      "Iteration 1977: tag train_loss, simple_value 2.29009\n",
      "Timestamp 1615174260638, Iteration 994\n",
      "batches :994 2.2983176708221436\n",
      "Iteration 1979: tag train_accuracy, simple_value 0.10007\n",
      "Iteration 1979: tag train_loss, simple_value 2.29832\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174261030, Iteration 995\n",
      "batches :995 2.298434257507324\n",
      "Iteration 1982: tag train_accuracy, simple_value 0.10021\n",
      "Iteration 1982: tag train_loss, simple_value 2.29843\n",
      "Timestamp 1615174261303, Iteration 996\n",
      "batches :996 2.2963969707489014\n",
      "Iteration 1984: tag train_accuracy, simple_value 0.10018\n",
      "Iteration 1984: tag train_loss, simple_value 2.2964\n",
      "Timestamp 1615174261599, Iteration 997\n",
      "batches :997 2.3352320194244385\n",
      "Iteration 1986: tag train_accuracy, simple_value 0.1\n",
      "Iteration 1986: tag train_loss, simple_value 2.33523\n",
      "Timestamp 1615174261812, Iteration 998\n",
      "batches :998 2.3024404048919678\n",
      "Iteration 1988: tag train_accuracy, simple_value 0.10001\n",
      "Iteration 1988: tag train_loss, simple_value 2.30244\n",
      "Timestamp 1615174262021, Iteration 999\n",
      "batches :999 2.2917990684509277\n",
      "Iteration 1990: tag train_accuracy, simple_value 0.10012\n",
      "Iteration 1990: tag train_loss, simple_value 2.2918\n",
      "Timestamp 1615174262237, Iteration 1000\n",
      "batches :1000 2.32257080078125\n",
      "Iteration 1992: tag train_accuracy, simple_value 0.10013\n",
      "Iteration 1992: tag train_loss, simple_value 2.32257\n",
      "Timestamp 1615174262451, Iteration 1001\n",
      "batches :1001 2.331122875213623\n",
      "Iteration 1994: tag train_accuracy, simple_value 0.10017\n",
      "Iteration 1994: tag train_loss, simple_value 2.33112\n",
      "Timestamp 1615174262665, Iteration 1002\n",
      "batches :1002 2.291053295135498\n",
      "Iteration 1996: tag train_accuracy, simple_value 0.10043\n",
      "Iteration 1996: tag train_loss, simple_value 2.29105\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174262993, Iteration 1003\n",
      "batches :1003 2.325631856918335\n",
      "Iteration 1997: tag train_accuracy, simple_value 0.10033\n",
      "Iteration 1997: tag train_loss, simple_value 2.32563\n",
      "Timestamp 1615174263208, Iteration 1004\n",
      "batches :1004 2.340898275375366\n",
      "Iteration 2000: tag train_accuracy, simple_value 0.10048\n",
      "Iteration 2000: tag train_loss, simple_value 2.3409\n",
      "Timestamp 1615174263492, Iteration 1005\n",
      "batches :1005 2.343243360519409\n",
      "Iteration 2002: tag train_accuracy, simple_value 0.10063\n",
      "Iteration 2002: tag train_loss, simple_value 2.34324\n",
      "Timestamp 1615174263790, Iteration 1006\n",
      "batches :1006 2.3116908073425293\n",
      "Iteration 2004: tag train_accuracy, simple_value 0.10053\n",
      "Iteration 2004: tag train_loss, simple_value 2.31169\n",
      "Timestamp 1615174264010, Iteration 1007\n",
      "batches :1007 2.263528347015381\n",
      "Iteration 2006: tag train_accuracy, simple_value 0.10085\n",
      "Iteration 2006: tag train_loss, simple_value 2.26353\n",
      "Timestamp 1615174264226, Iteration 1008\n",
      "batches :1008 2.2799601554870605\n",
      "Iteration 2008: tag train_accuracy, simple_value 0.10099\n",
      "Iteration 2008: tag train_loss, simple_value 2.27996\n",
      "Timestamp 1615174264445, Iteration 1009\n",
      "batches :1009 2.3220574855804443\n",
      "Iteration 2010: tag train_accuracy, simple_value 0.10075\n",
      "Iteration 2010: tag train_loss, simple_value 2.32206\n",
      "Timestamp 1615174264660, Iteration 1010\n",
      "batches :1010 2.3106937408447266\n",
      "Iteration 2012: tag train_accuracy, simple_value 0.10075\n",
      "Iteration 2012: tag train_loss, simple_value 2.31069\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174264970, Iteration 1011\n",
      "batches :1011 2.272489547729492\n",
      "Iteration 2013: tag train_accuracy, simple_value 0.10088\n",
      "Iteration 2013: tag train_loss, simple_value 2.27249\n",
      "Timestamp 1615174265183, Iteration 1012\n",
      "batches :1012 2.2906999588012695\n",
      "Iteration 2015: tag train_accuracy, simple_value 0.10112\n",
      "Iteration 2015: tag train_loss, simple_value 2.2907\n",
      "Timestamp 1615174265397, Iteration 1013\n",
      "batches :1013 2.31671142578125\n",
      "Iteration 2017: tag train_accuracy, simple_value 0.10126\n",
      "Iteration 2017: tag train_loss, simple_value 2.31671\n",
      "Timestamp 1615174265700, Iteration 1014\n",
      "batches :1014 2.2943315505981445\n",
      "Iteration 2019: tag train_accuracy, simple_value 0.10123\n",
      "Iteration 2019: tag train_loss, simple_value 2.29433\n",
      "Timestamp 1615174265914, Iteration 1015\n",
      "batches :1015 2.2957916259765625\n",
      "Iteration 2021: tag train_accuracy, simple_value 0.10123\n",
      "Iteration 2021: tag train_loss, simple_value 2.29579\n",
      "Timestamp 1615174266133, Iteration 1016\n",
      "batches :1016 2.3206794261932373\n",
      "Iteration 2023: tag train_accuracy, simple_value 0.10137\n",
      "Iteration 2023: tag train_loss, simple_value 2.32068\n",
      "Timestamp 1615174266394, Iteration 1017\n",
      "batches :1017 2.3193562030792236\n",
      "Iteration 2025: tag train_accuracy, simple_value 0.10117\n",
      "Iteration 2025: tag train_loss, simple_value 2.31936\n",
      "Timestamp 1615174266613, Iteration 1018\n",
      "batches :1018 2.330735206604004\n",
      "Iteration 2027: tag train_accuracy, simple_value 0.10107\n",
      "Iteration 2027: tag train_loss, simple_value 2.33074\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174267018, Iteration 1019\n",
      "batches :1019 2.324957847595215\n",
      "Iteration 2029: tag train_accuracy, simple_value 0.101\n",
      "Iteration 2029: tag train_loss, simple_value 2.32496\n",
      "Timestamp 1615174267485, Iteration 1020\n",
      "batches :1020 2.3040976524353027\n",
      "Iteration 2032: tag train_accuracy, simple_value 0.10112\n",
      "Iteration 2032: tag train_loss, simple_value 2.3041\n",
      "Timestamp 1615174267700, Iteration 1021\n",
      "batches :1021 2.3178491592407227\n",
      "Iteration 2033: tag train_accuracy, simple_value 0.10111\n",
      "Iteration 2033: tag train_loss, simple_value 2.31785\n",
      "Timestamp 1615174267911, Iteration 1022\n",
      "batches :1022 2.3224411010742188\n",
      "Iteration 2035: tag train_accuracy, simple_value 0.10111\n",
      "Iteration 2035: tag train_loss, simple_value 2.32244\n",
      "Timestamp 1615174268126, Iteration 1023\n",
      "batches :1023 2.324117422103882\n",
      "Iteration 2037: tag train_accuracy, simple_value 0.10108\n",
      "Iteration 2037: tag train_loss, simple_value 2.32412\n",
      "Timestamp 1615174268402, Iteration 1024\n",
      "batches :1024 2.297914981842041\n",
      "Iteration 2039: tag train_accuracy, simple_value 0.10118\n",
      "Iteration 2039: tag train_loss, simple_value 2.29791\n",
      "Timestamp 1615174268616, Iteration 1025\n",
      "batches :1025 2.3379080295562744\n",
      "Iteration 2041: tag train_accuracy, simple_value 0.10108\n",
      "Iteration 2041: tag train_loss, simple_value 2.33791\n",
      "Timestamp 1615174268829, Iteration 1026\n",
      "batches :1026 2.2772247791290283\n",
      "Iteration 2043: tag train_accuracy, simple_value 0.10131\n",
      "Iteration 2043: tag train_loss, simple_value 2.27722\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174269222, Iteration 1027\n",
      "batches :1027 2.2944583892822266\n",
      "Iteration 2045: tag train_accuracy, simple_value 0.10131\n",
      "Iteration 2045: tag train_loss, simple_value 2.29446\n",
      "Timestamp 1615174269693, Iteration 1028\n",
      "batches :1028 2.309093713760376\n",
      "Iteration 2047: tag train_accuracy, simple_value 0.10133\n",
      "Iteration 2047: tag train_loss, simple_value 2.30909\n",
      "Timestamp 1615174269911, Iteration 1029\n",
      "batches :1029 2.312718152999878\n",
      "Iteration 2050: tag train_accuracy, simple_value 0.1012\n",
      "Iteration 2050: tag train_loss, simple_value 2.31272\n",
      "Timestamp 1615174270190, Iteration 1030\n",
      "batches :1030 2.3156867027282715\n",
      "Iteration 2052: tag train_accuracy, simple_value 0.10124\n",
      "Iteration 2052: tag train_loss, simple_value 2.31569\n",
      "Timestamp 1615174270407, Iteration 1031\n",
      "batches :1031 2.318769931793213\n",
      "Iteration 2054: tag train_accuracy, simple_value 0.10108\n",
      "Iteration 2054: tag train_loss, simple_value 2.31877\n",
      "Timestamp 1615174270622, Iteration 1032\n",
      "batches :1032 2.331198215484619\n",
      "Iteration 2056: tag train_accuracy, simple_value 0.10102\n",
      "Iteration 2056: tag train_loss, simple_value 2.3312\n",
      "Timestamp 1615174270895, Iteration 1033\n",
      "batches :1033 2.2954835891723633\n",
      "Iteration 2058: tag train_accuracy, simple_value 0.10096\n",
      "Iteration 2058: tag train_loss, simple_value 2.29548\n",
      "Timestamp 1615174271116, Iteration 1034\n",
      "batches :1034 2.2924609184265137\n",
      "Iteration 2060: tag train_accuracy, simple_value 0.1009\n",
      "Iteration 2060: tag train_loss, simple_value 2.29246\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174271433, Iteration 1035\n",
      "batches :1035 2.3118011951446533\n",
      "Iteration 2062: tag train_accuracy, simple_value 0.1009\n",
      "Iteration 2062: tag train_loss, simple_value 2.3118\n",
      "Timestamp 1615174271700, Iteration 1036\n",
      "batches :1036 2.290428876876831\n",
      "Iteration 2064: tag train_accuracy, simple_value 0.10071\n",
      "Iteration 2064: tag train_loss, simple_value 2.29043\n",
      "Timestamp 1615174271998, Iteration 1037\n",
      "batches :1037 2.29290509223938\n",
      "Iteration 2066: tag train_accuracy, simple_value 0.10059\n",
      "Iteration 2066: tag train_loss, simple_value 2.29291\n",
      "Timestamp 1615174272214, Iteration 1038\n",
      "batches :1038 2.2990975379943848\n",
      "Iteration 2068: tag train_accuracy, simple_value 0.10053\n",
      "Iteration 2068: tag train_loss, simple_value 2.2991\n",
      "Timestamp 1615174272425, Iteration 1039\n",
      "batches :1039 2.303699254989624\n",
      "Iteration 2070: tag train_accuracy, simple_value 0.10066\n",
      "Iteration 2070: tag train_loss, simple_value 2.3037\n",
      "Timestamp 1615174272634, Iteration 1040\n",
      "batches :1040 2.3095145225524902\n",
      "Iteration 2072: tag train_accuracy, simple_value 0.1007\n",
      "Iteration 2072: tag train_loss, simple_value 2.30951\n",
      "Timestamp 1615174272845, Iteration 1041\n",
      "batches :1041 2.292907953262329\n",
      "Iteration 2074: tag train_accuracy, simple_value 0.10067\n",
      "Iteration 2074: tag train_loss, simple_value 2.29291\n",
      "Timestamp 1615174273058, Iteration 1042\n",
      "batches :1042 2.2740302085876465\n",
      "Iteration 2076: tag train_accuracy, simple_value 0.10083\n",
      "Iteration 2076: tag train_loss, simple_value 2.27403\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174273399, Iteration 1043\n",
      "batches :1043 2.289433002471924\n",
      "Iteration 2077: tag train_accuracy, simple_value 0.10075\n",
      "Iteration 2077: tag train_loss, simple_value 2.28943\n",
      "Timestamp 1615174273784, Iteration 1044\n",
      "batches :1044 2.321960926055908\n",
      "Iteration 2079: tag train_accuracy, simple_value 0.10081\n",
      "Iteration 2079: tag train_loss, simple_value 2.32196\n",
      "Timestamp 1615174274003, Iteration 1045\n",
      "batches :1045 2.311070442199707\n",
      "Iteration 2081: tag train_accuracy, simple_value 0.10079\n",
      "Iteration 2081: tag train_loss, simple_value 2.31107\n",
      "Timestamp 1615174274217, Iteration 1046\n",
      "batches :1046 2.303662061691284\n",
      "Iteration 2083: tag train_accuracy, simple_value 0.10079\n",
      "Iteration 2083: tag train_loss, simple_value 2.30366\n",
      "Timestamp 1615174274431, Iteration 1047\n",
      "batches :1047 2.310433864593506\n",
      "Iteration 2085: tag train_accuracy, simple_value 0.10073\n",
      "Iteration 2085: tag train_loss, simple_value 2.31043\n",
      "Timestamp 1615174274650, Iteration 1048\n",
      "batches :1048 2.296266794204712\n",
      "Iteration 2087: tag train_accuracy, simple_value 0.10083\n",
      "Iteration 2087: tag train_loss, simple_value 2.29627\n",
      "Timestamp 1615174274863, Iteration 1049\n",
      "batches :1049 2.300908088684082\n",
      "Iteration 2089: tag train_accuracy, simple_value 0.1008\n",
      "Iteration 2089: tag train_loss, simple_value 2.30091\n",
      "Timestamp 1615174275076, Iteration 1050\n",
      "batches :1050 2.3379712104797363\n",
      "Iteration 2091: tag train_accuracy, simple_value 0.10074\n",
      "Iteration 2091: tag train_loss, simple_value 2.33797\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174275475, Iteration 1051\n",
      "batches :1051 2.302645206451416\n",
      "Iteration 2094: tag train_accuracy, simple_value 0.101\n",
      "Iteration 2094: tag train_loss, simple_value 2.30265\n",
      "Timestamp 1615174275701, Iteration 1052\n",
      "batches :1052 2.3469648361206055\n",
      "Iteration 2096: tag train_accuracy, simple_value 0.101\n",
      "Iteration 2096: tag train_loss, simple_value 2.34696\n",
      "Timestamp 1615174275995, Iteration 1053\n",
      "batches :1053 2.29050874710083\n",
      "Iteration 2098: tag train_accuracy, simple_value 0.10077\n",
      "Iteration 2098: tag train_loss, simple_value 2.29051\n",
      "Timestamp 1615174276205, Iteration 1054\n",
      "batches :1054 2.3465981483459473\n",
      "Iteration 2100: tag train_accuracy, simple_value 0.10077\n",
      "Iteration 2100: tag train_loss, simple_value 2.3466\n",
      "Timestamp 1615174276417, Iteration 1055\n",
      "batches :1055 2.3147499561309814\n",
      "Iteration 2102: tag train_accuracy, simple_value 0.10072\n",
      "Iteration 2102: tag train_loss, simple_value 2.31475\n",
      "Timestamp 1615174276687, Iteration 1056\n",
      "batches :1056 2.2949092388153076\n",
      "Iteration 2104: tag train_accuracy, simple_value 0.10083\n",
      "Iteration 2104: tag train_loss, simple_value 2.29491\n",
      "Timestamp 1615174276906, Iteration 1057\n",
      "batches :1057 2.312340021133423\n",
      "Iteration 2106: tag train_accuracy, simple_value 0.10087\n",
      "Iteration 2106: tag train_loss, simple_value 2.31234\n",
      "Timestamp 1615174277119, Iteration 1058\n",
      "batches :1058 2.317302942276001\n",
      "Iteration 2108: tag train_accuracy, simple_value 0.10087\n",
      "Iteration 2108: tag train_loss, simple_value 2.3173\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174277516, Iteration 1059\n",
      "batches :1059 2.3526997566223145\n",
      "Iteration 2109: tag train_accuracy, simple_value 0.10074\n",
      "Iteration 2109: tag train_loss, simple_value 2.3527\n",
      "Timestamp 1615174277889, Iteration 1060\n",
      "batches :1060 2.3231875896453857\n",
      "Iteration 2111: tag train_accuracy, simple_value 0.10069\n",
      "Iteration 2111: tag train_loss, simple_value 2.32319\n",
      "Timestamp 1615174278108, Iteration 1061\n",
      "batches :1061 2.3009915351867676\n",
      "Iteration 2113: tag train_accuracy, simple_value 0.1008\n",
      "Iteration 2113: tag train_loss, simple_value 2.30099\n",
      "Timestamp 1615174278321, Iteration 1062\n",
      "batches :1062 2.3324060440063477\n",
      "Iteration 2115: tag train_accuracy, simple_value 0.10081\n",
      "Iteration 2115: tag train_loss, simple_value 2.33241\n",
      "Timestamp 1615174278534, Iteration 1063\n",
      "batches :1063 2.295074224472046\n",
      "Iteration 2117: tag train_accuracy, simple_value 0.10078\n",
      "Iteration 2117: tag train_loss, simple_value 2.29507\n",
      "Timestamp 1615174278789, Iteration 1064\n",
      "batches :1064 2.2881925106048584\n",
      "Iteration 2119: tag train_accuracy, simple_value 0.10092\n",
      "Iteration 2119: tag train_loss, simple_value 2.28819\n",
      "Timestamp 1615174279004, Iteration 1065\n",
      "batches :1065 2.3607358932495117\n",
      "Iteration 2121: tag train_accuracy, simple_value 0.1009\n",
      "Iteration 2121: tag train_loss, simple_value 2.36074\n",
      "Timestamp 1615174279219, Iteration 1066\n",
      "batches :1066 2.3012964725494385\n",
      "Iteration 2123: tag train_accuracy, simple_value 0.10101\n",
      "Iteration 2123: tag train_loss, simple_value 2.3013\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174279998, Iteration 1067\n",
      "batches :1067 2.3382785320281982\n",
      "Iteration 2126: tag train_accuracy, simple_value 0.10125\n",
      "Iteration 2126: tag train_loss, simple_value 2.33828\n",
      "Timestamp 1615174280218, Iteration 1068\n",
      "batches :1068 2.360908269882202\n",
      "Iteration 2128: tag train_accuracy, simple_value 0.10112\n",
      "Iteration 2128: tag train_loss, simple_value 2.36091\n",
      "Timestamp 1615174280503, Iteration 1069\n",
      "batches :1069 2.290163040161133\n",
      "Iteration 2130: tag train_accuracy, simple_value 0.10137\n",
      "Iteration 2130: tag train_loss, simple_value 2.29016\n",
      "Timestamp 1615174280897, Iteration 1070\n",
      "batches :1070 2.30461049079895\n",
      "Iteration 2132: tag train_accuracy, simple_value 0.1015\n",
      "Iteration 2132: tag train_loss, simple_value 2.30461\n",
      "Timestamp 1615174281113, Iteration 1071\n",
      "batches :1071 2.2879276275634766\n",
      "Iteration 2134: tag train_accuracy, simple_value 0.10172\n",
      "Iteration 2134: tag train_loss, simple_value 2.28793\n",
      "Timestamp 1615174281324, Iteration 1072\n",
      "batches :1072 2.330714702606201\n",
      "Iteration 2136: tag train_accuracy, simple_value 0.10167\n",
      "Iteration 2136: tag train_loss, simple_value 2.33071\n",
      "Timestamp 1615174281535, Iteration 1073\n",
      "batches :1073 2.308969736099243\n",
      "Iteration 2138: tag train_accuracy, simple_value 0.10156\n",
      "Iteration 2138: tag train_loss, simple_value 2.30897\n",
      "Timestamp 1615174281792, Iteration 1074\n",
      "batches :1074 2.3050148487091064\n",
      "Iteration 2140: tag train_accuracy, simple_value 0.10153\n",
      "Iteration 2140: tag train_loss, simple_value 2.30501\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174282190, Iteration 1075\n",
      "batches :1075 2.271397590637207\n",
      "Iteration 2142: tag train_accuracy, simple_value 0.10169\n",
      "Iteration 2142: tag train_loss, simple_value 2.2714\n",
      "Timestamp 1615174282409, Iteration 1076\n",
      "batches :1076 2.2705016136169434\n",
      "Iteration 2144: tag train_accuracy, simple_value 0.10172\n",
      "Iteration 2144: tag train_loss, simple_value 2.2705\n",
      "Timestamp 1615174282628, Iteration 1077\n",
      "batches :1077 2.268592357635498\n",
      "Iteration 2146: tag train_accuracy, simple_value 0.10207\n",
      "Iteration 2146: tag train_loss, simple_value 2.26859\n",
      "Timestamp 1615174282899, Iteration 1078\n",
      "batches :1078 2.28851318359375\n",
      "Iteration 2148: tag train_accuracy, simple_value 0.10226\n",
      "Iteration 2148: tag train_loss, simple_value 2.28851\n",
      "Timestamp 1615174283110, Iteration 1079\n",
      "batches :1079 2.294081687927246\n",
      "Iteration 2150: tag train_accuracy, simple_value 0.10223\n",
      "Iteration 2150: tag train_loss, simple_value 2.29408\n",
      "Timestamp 1615174283324, Iteration 1080\n",
      "batches :1080 2.2888195514678955\n",
      "Iteration 2152: tag train_accuracy, simple_value 0.10236\n",
      "Iteration 2152: tag train_loss, simple_value 2.28882\n",
      "Timestamp 1615174283534, Iteration 1081\n",
      "batches :1081 2.2901670932769775\n",
      "Iteration 2154: tag train_accuracy, simple_value 0.10243\n",
      "Iteration 2154: tag train_loss, simple_value 2.29017\n",
      "Timestamp 1615174283742, Iteration 1082\n",
      "batches :1082 2.3024439811706543\n",
      "Iteration 2156: tag train_accuracy, simple_value 0.10251\n",
      "Iteration 2156: tag train_loss, simple_value 2.30244\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174284191, Iteration 1083\n",
      "batches :1083 2.3108832836151123\n",
      "Iteration 2157: tag train_accuracy, simple_value 0.10247\n",
      "Iteration 2157: tag train_loss, simple_value 2.31088\n",
      "Timestamp 1615174284492, Iteration 1084\n",
      "batches :1084 2.306952714920044\n",
      "Iteration 2159: tag train_accuracy, simple_value 0.10258\n",
      "Iteration 2159: tag train_loss, simple_value 2.30695\n",
      "Timestamp 1615174284796, Iteration 1085\n",
      "batches :1085 2.26625657081604\n",
      "Iteration 2162: tag train_accuracy, simple_value 0.10263\n",
      "Iteration 2162: tag train_loss, simple_value 2.26626\n",
      "Timestamp 1615174285100, Iteration 1086\n",
      "batches :1086 2.2909843921661377\n",
      "Iteration 2164: tag train_accuracy, simple_value 0.10271\n",
      "Iteration 2164: tag train_loss, simple_value 2.29098\n",
      "Timestamp 1615174285321, Iteration 1087\n",
      "batches :1087 2.2889821529388428\n",
      "Iteration 2166: tag train_accuracy, simple_value 0.10299\n",
      "Iteration 2166: tag train_loss, simple_value 2.28898\n",
      "Timestamp 1615174285535, Iteration 1088\n",
      "batches :1088 2.2998392581939697\n",
      "Iteration 2168: tag train_accuracy, simple_value 0.10304\n",
      "Iteration 2168: tag train_loss, simple_value 2.29984\n",
      "Timestamp 1615174285790, Iteration 1089\n",
      "batches :1089 2.299886465072632\n",
      "Iteration 2170: tag train_accuracy, simple_value 0.10311\n",
      "Iteration 2170: tag train_loss, simple_value 2.29989\n",
      "Timestamp 1615174286011, Iteration 1090\n",
      "batches :1090 2.3013193607330322\n",
      "Iteration 2172: tag train_accuracy, simple_value 0.10315\n",
      "Iteration 2172: tag train_loss, simple_value 2.30132\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174286390, Iteration 1091\n",
      "batches :1091 2.2933578491210938\n",
      "Iteration 2173: tag train_accuracy, simple_value 0.10309\n",
      "Iteration 2173: tag train_loss, simple_value 2.29336\n",
      "Timestamp 1615174286607, Iteration 1092\n",
      "batches :1092 2.3706178665161133\n",
      "Iteration 2175: tag train_accuracy, simple_value 0.10303\n",
      "Iteration 2175: tag train_loss, simple_value 2.37062\n",
      "Timestamp 1615174286819, Iteration 1093\n",
      "batches :1093 2.354356050491333\n",
      "Iteration 2177: tag train_accuracy, simple_value 0.10305\n",
      "Iteration 2177: tag train_loss, simple_value 2.35436\n",
      "Timestamp 1615174287099, Iteration 1094\n",
      "batches :1094 2.3388538360595703\n",
      "Iteration 2179: tag train_accuracy, simple_value 0.103\n",
      "Iteration 2179: tag train_loss, simple_value 2.33885\n",
      "Timestamp 1615174287311, Iteration 1095\n",
      "batches :1095 2.301809310913086\n",
      "Iteration 2181: tag train_accuracy, simple_value 0.10304\n",
      "Iteration 2181: tag train_loss, simple_value 2.30181\n",
      "Timestamp 1615174287599, Iteration 1096\n",
      "batches :1096 2.335439920425415\n",
      "Iteration 2183: tag train_accuracy, simple_value 0.10332\n",
      "Iteration 2183: tag train_loss, simple_value 2.33544\n",
      "Timestamp 1615174287893, Iteration 1097\n",
      "batches :1097 2.305180072784424\n",
      "Iteration 2185: tag train_accuracy, simple_value 0.10334\n",
      "Iteration 2185: tag train_loss, simple_value 2.30518\n",
      "Timestamp 1615174288105, Iteration 1098\n",
      "batches :1098 2.299089193344116\n",
      "Iteration 2187: tag train_accuracy, simple_value 0.10325\n",
      "Iteration 2187: tag train_loss, simple_value 2.29909\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174288500, Iteration 1099\n",
      "batches :1099 2.3390042781829834\n",
      "Iteration 2190: tag train_accuracy, simple_value 0.10318\n",
      "Iteration 2190: tag train_loss, simple_value 2.339\n",
      "Timestamp 1615174288714, Iteration 1100\n",
      "batches :1100 2.317068576812744\n",
      "Iteration 2192: tag train_accuracy, simple_value 0.10315\n",
      "Iteration 2192: tag train_loss, simple_value 2.31707\n",
      "Timestamp 1615174288987, Iteration 1101\n",
      "batches :1101 2.3075506687164307\n",
      "Iteration 2194: tag train_accuracy, simple_value 0.10315\n",
      "Iteration 2194: tag train_loss, simple_value 2.30755\n",
      "Timestamp 1615174289202, Iteration 1102\n",
      "batches :1102 2.317826747894287\n",
      "Iteration 2196: tag train_accuracy, simple_value 0.10302\n",
      "Iteration 2196: tag train_loss, simple_value 2.31783\n",
      "Timestamp 1615174289418, Iteration 1103\n",
      "batches :1103 2.3113412857055664\n",
      "Iteration 2198: tag train_accuracy, simple_value 0.10304\n",
      "Iteration 2198: tag train_loss, simple_value 2.31134\n",
      "Timestamp 1615174289643, Iteration 1104\n",
      "batches :1104 2.296678304672241\n",
      "Iteration 2200: tag train_accuracy, simple_value 0.10313\n",
      "Iteration 2200: tag train_loss, simple_value 2.29668\n",
      "Timestamp 1615174289862, Iteration 1105\n",
      "batches :1105 2.2892892360687256\n",
      "Iteration 2202: tag train_accuracy, simple_value 0.10313\n",
      "Iteration 2202: tag train_loss, simple_value 2.28929\n",
      "Timestamp 1615174290095, Iteration 1106\n",
      "batches :1106 2.3330650329589844\n",
      "Iteration 2204: tag train_accuracy, simple_value 0.10303\n",
      "Iteration 2204: tag train_loss, simple_value 2.33307\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174290391, Iteration 1107\n",
      "batches :1107 2.341972827911377\n",
      "Iteration 2205: tag train_accuracy, simple_value 0.10299\n",
      "Iteration 2205: tag train_loss, simple_value 2.34197\n",
      "Timestamp 1615174290606, Iteration 1108\n",
      "batches :1108 2.3038580417633057\n",
      "Iteration 2207: tag train_accuracy, simple_value 0.1031\n",
      "Iteration 2207: tag train_loss, simple_value 2.30386\n",
      "Timestamp 1615174290991, Iteration 1109\n",
      "batches :1109 2.2712514400482178\n",
      "Iteration 2209: tag train_accuracy, simple_value 0.10322\n",
      "Iteration 2209: tag train_loss, simple_value 2.27125\n",
      "Timestamp 1615174291207, Iteration 1110\n",
      "batches :1110 2.311673879623413\n",
      "Iteration 2211: tag train_accuracy, simple_value 0.10322\n",
      "Iteration 2211: tag train_loss, simple_value 2.31167\n",
      "Timestamp 1615174291420, Iteration 1111\n",
      "batches :1111 2.319152355194092\n",
      "Iteration 2213: tag train_accuracy, simple_value 0.10311\n",
      "Iteration 2213: tag train_loss, simple_value 2.31915\n",
      "Timestamp 1615174291635, Iteration 1112\n",
      "batches :1112 2.2668654918670654\n",
      "Iteration 2215: tag train_accuracy, simple_value 0.10311\n",
      "Iteration 2215: tag train_loss, simple_value 2.26687\n",
      "Timestamp 1615174291849, Iteration 1113\n",
      "batches :1113 2.312253475189209\n",
      "Iteration 2217: tag train_accuracy, simple_value 0.10306\n",
      "Iteration 2217: tag train_loss, simple_value 2.31225\n",
      "Timestamp 1615174292060, Iteration 1114\n",
      "batches :1114 2.299912929534912\n",
      "Iteration 2219: tag train_accuracy, simple_value 0.1032\n",
      "Iteration 2219: tag train_loss, simple_value 2.29991\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174292460, Iteration 1115\n",
      "batches :1115 2.3285491466522217\n",
      "Iteration 2222: tag train_accuracy, simple_value 0.10315\n",
      "Iteration 2222: tag train_loss, simple_value 2.32855\n",
      "Timestamp 1615174292689, Iteration 1116\n",
      "batches :1116 2.301372766494751\n",
      "Iteration 2224: tag train_accuracy, simple_value 0.10312\n",
      "Iteration 2224: tag train_loss, simple_value 2.30137\n",
      "Timestamp 1615174293095, Iteration 1117\n",
      "batches :1117 2.303873300552368\n",
      "Iteration 2226: tag train_accuracy, simple_value 0.10313\n",
      "Iteration 2226: tag train_loss, simple_value 2.30387\n",
      "Timestamp 1615174293311, Iteration 1118\n",
      "batches :1118 2.326305389404297\n",
      "Iteration 2227: tag train_accuracy, simple_value 0.1031\n",
      "Iteration 2227: tag train_loss, simple_value 2.32631\n",
      "Timestamp 1615174293527, Iteration 1119\n",
      "batches :1119 2.2916152477264404\n",
      "Iteration 2229: tag train_accuracy, simple_value 0.10312\n",
      "Iteration 2229: tag train_loss, simple_value 2.29162\n",
      "Timestamp 1615174293743, Iteration 1120\n",
      "batches :1120 2.293607234954834\n",
      "Iteration 2231: tag train_accuracy, simple_value 0.10321\n",
      "Iteration 2231: tag train_loss, simple_value 2.29361\n",
      "Timestamp 1615174293957, Iteration 1121\n",
      "batches :1121 2.2886955738067627\n",
      "Iteration 2233: tag train_accuracy, simple_value 0.10321\n",
      "Iteration 2233: tag train_loss, simple_value 2.2887\n",
      "Timestamp 1615174294172, Iteration 1122\n",
      "batches :1122 2.3064281940460205\n",
      "Iteration 2235: tag train_accuracy, simple_value 0.1032\n",
      "Iteration 2235: tag train_loss, simple_value 2.30643\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174294590, Iteration 1123\n",
      "batches :1123 2.310621976852417\n",
      "Iteration 2238: tag train_accuracy, simple_value 0.10325\n",
      "Iteration 2238: tag train_loss, simple_value 2.31062\n",
      "Timestamp 1615174294806, Iteration 1124\n",
      "batches :1124 2.290149688720703\n",
      "Iteration 2240: tag train_accuracy, simple_value 0.10343\n",
      "Iteration 2240: tag train_loss, simple_value 2.29015\n",
      "Timestamp 1615174295185, Iteration 1125\n",
      "batches :1125 2.2758545875549316\n",
      "Iteration 2242: tag train_accuracy, simple_value 0.1035\n",
      "Iteration 2242: tag train_loss, simple_value 2.27585\n",
      "Timestamp 1615174295402, Iteration 1126\n",
      "batches :1126 2.2928647994995117\n",
      "Iteration 2244: tag train_accuracy, simple_value 0.10353\n",
      "Iteration 2244: tag train_loss, simple_value 2.29286\n",
      "Timestamp 1615174295702, Iteration 1127\n",
      "batches :1127 2.3359029293060303\n",
      "Iteration 2246: tag train_accuracy, simple_value 0.10349\n",
      "Iteration 2246: tag train_loss, simple_value 2.3359\n",
      "Timestamp 1615174295917, Iteration 1128\n",
      "batches :1128 2.2962937355041504\n",
      "Iteration 2248: tag train_accuracy, simple_value 0.10349\n",
      "Iteration 2248: tag train_loss, simple_value 2.29629\n",
      "Timestamp 1615174296198, Iteration 1129\n",
      "batches :1129 2.2695600986480713\n",
      "Iteration 2249: tag train_accuracy, simple_value 0.10358\n",
      "Iteration 2249: tag train_loss, simple_value 2.26956\n",
      "Timestamp 1615174296420, Iteration 1130\n",
      "batches :1130 2.283155679702759\n",
      "Iteration 2252: tag train_accuracy, simple_value 0.10356\n",
      "Iteration 2252: tag train_loss, simple_value 2.28316\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174296809, Iteration 1131\n",
      "batches :1131 2.3054912090301514\n",
      "Iteration 2253: tag train_accuracy, simple_value 0.10352\n",
      "Iteration 2253: tag train_loss, simple_value 2.30549\n",
      "Timestamp 1615174297095, Iteration 1132\n",
      "batches :1132 2.315408706665039\n",
      "Iteration 2256: tag train_accuracy, simple_value 0.10341\n",
      "Iteration 2256: tag train_loss, simple_value 2.31541\n",
      "Timestamp 1615174297385, Iteration 1133\n",
      "batches :1133 2.317720413208008\n",
      "Iteration 2258: tag train_accuracy, simple_value 0.1035\n",
      "Iteration 2258: tag train_loss, simple_value 2.31772\n",
      "Timestamp 1615174297690, Iteration 1134\n",
      "batches :1134 2.288548469543457\n",
      "Iteration 2260: tag train_accuracy, simple_value 0.10336\n",
      "Iteration 2260: tag train_loss, simple_value 2.28855\n",
      "Timestamp 1615174297909, Iteration 1135\n",
      "batches :1135 2.3016107082366943\n",
      "Iteration 2262: tag train_accuracy, simple_value 0.10335\n",
      "Iteration 2262: tag train_loss, simple_value 2.30161\n",
      "Timestamp 1615174298119, Iteration 1136\n",
      "batches :1136 2.314913749694824\n",
      "Iteration 2264: tag train_accuracy, simple_value 0.10341\n",
      "Iteration 2264: tag train_loss, simple_value 2.31491\n",
      "Timestamp 1615174298332, Iteration 1137\n",
      "batches :1137 2.2863121032714844\n",
      "Iteration 2266: tag train_accuracy, simple_value 0.10345\n",
      "Iteration 2266: tag train_loss, simple_value 2.28631\n",
      "Timestamp 1615174298548, Iteration 1138\n",
      "batches :1138 2.3024027347564697\n",
      "Iteration 2268: tag train_accuracy, simple_value 0.10347\n",
      "Iteration 2268: tag train_loss, simple_value 2.3024\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174299000, Iteration 1139\n",
      "batches :1139 2.3455677032470703\n",
      "Iteration 2270: tag train_accuracy, simple_value 0.10344\n",
      "Iteration 2270: tag train_loss, simple_value 2.34557\n",
      "Timestamp 1615174299293, Iteration 1140\n",
      "batches :1140 2.269775152206421\n",
      "Iteration 2272: tag train_accuracy, simple_value 0.10346\n",
      "Iteration 2272: tag train_loss, simple_value 2.26978\n",
      "Timestamp 1615174299507, Iteration 1141\n",
      "batches :1141 2.3005309104919434\n",
      "Iteration 2273: tag train_accuracy, simple_value 0.10347\n",
      "Iteration 2273: tag train_loss, simple_value 2.30053\n",
      "Timestamp 1615174299784, Iteration 1142\n",
      "batches :1142 2.335998773574829\n",
      "Iteration 2275: tag train_accuracy, simple_value 0.10346\n",
      "Iteration 2275: tag train_loss, simple_value 2.336\n",
      "Timestamp 1615174300005, Iteration 1143\n",
      "batches :1143 2.280461311340332\n",
      "Iteration 2277: tag train_accuracy, simple_value 0.10344\n",
      "Iteration 2277: tag train_loss, simple_value 2.28046\n",
      "Timestamp 1615174300220, Iteration 1144\n",
      "batches :1144 2.3094325065612793\n",
      "Iteration 2279: tag train_accuracy, simple_value 0.10345\n",
      "Iteration 2279: tag train_loss, simple_value 2.30943\n",
      "Timestamp 1615174300437, Iteration 1145\n",
      "batches :1145 2.261514902114868\n",
      "Iteration 2281: tag train_accuracy, simple_value 0.10338\n",
      "Iteration 2281: tag train_loss, simple_value 2.26151\n",
      "Timestamp 1615174300648, Iteration 1146\n",
      "batches :1146 2.3252851963043213\n",
      "Iteration 2283: tag train_accuracy, simple_value 0.10344\n",
      "Iteration 2283: tag train_loss, simple_value 2.32529\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174301189, Iteration 1147\n",
      "batches :1147 2.2750463485717773\n",
      "Iteration 2285: tag train_accuracy, simple_value 0.1034\n",
      "Iteration 2285: tag train_loss, simple_value 2.27505\n",
      "Timestamp 1615174301406, Iteration 1148\n",
      "batches :1148 2.305166482925415\n",
      "Iteration 2288: tag train_accuracy, simple_value 0.10344\n",
      "Iteration 2288: tag train_loss, simple_value 2.30517\n",
      "Timestamp 1615174301622, Iteration 1149\n",
      "batches :1149 2.3027267456054688\n",
      "Iteration 2290: tag train_accuracy, simple_value 0.10337\n",
      "Iteration 2290: tag train_loss, simple_value 2.30273\n",
      "Timestamp 1615174301899, Iteration 1150\n",
      "batches :1150 2.314424991607666\n",
      "Iteration 2292: tag train_accuracy, simple_value 0.10336\n",
      "Iteration 2292: tag train_loss, simple_value 2.31442\n",
      "Timestamp 1615174302112, Iteration 1151\n",
      "batches :1151 2.3420443534851074\n",
      "Iteration 2294: tag train_accuracy, simple_value 0.10336\n",
      "Iteration 2294: tag train_loss, simple_value 2.34204\n",
      "Timestamp 1615174302323, Iteration 1152\n",
      "batches :1152 2.325528383255005\n",
      "Iteration 2296: tag train_accuracy, simple_value 0.10338\n",
      "Iteration 2296: tag train_loss, simple_value 2.32553\n",
      "Timestamp 1615174302590, Iteration 1153\n",
      "batches :1153 2.377204418182373\n",
      "Iteration 2298: tag train_accuracy, simple_value 0.10339\n",
      "Iteration 2298: tag train_loss, simple_value 2.3772\n",
      "Timestamp 1615174302809, Iteration 1154\n",
      "batches :1154 2.3116300106048584\n",
      "Iteration 2300: tag train_accuracy, simple_value 0.10345\n",
      "Iteration 2300: tag train_loss, simple_value 2.31163\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174303174, Iteration 1155\n",
      "batches :1155 2.2814090251922607\n",
      "Iteration 2301: tag train_accuracy, simple_value 0.10344\n",
      "Iteration 2301: tag train_loss, simple_value 2.28141\n",
      "Timestamp 1615174303392, Iteration 1156\n",
      "batches :1156 2.2987191677093506\n",
      "Iteration 2303: tag train_accuracy, simple_value 0.10337\n",
      "Iteration 2303: tag train_loss, simple_value 2.29872\n",
      "Timestamp 1615174303601, Iteration 1157\n",
      "batches :1157 2.317816734313965\n",
      "Iteration 2305: tag train_accuracy, simple_value 0.10328\n",
      "Iteration 2305: tag train_loss, simple_value 2.31782\n",
      "Timestamp 1615174303818, Iteration 1158\n",
      "batches :1158 2.3107917308807373\n",
      "Iteration 2307: tag train_accuracy, simple_value 0.10332\n",
      "Iteration 2307: tag train_loss, simple_value 2.31079\n",
      "Timestamp 1615174304032, Iteration 1159\n",
      "batches :1159 2.3272171020507812\n",
      "Iteration 2309: tag train_accuracy, simple_value 0.10325\n",
      "Iteration 2309: tag train_loss, simple_value 2.32722\n",
      "Timestamp 1615174304251, Iteration 1160\n",
      "batches :1160 2.2968363761901855\n",
      "Iteration 2311: tag train_accuracy, simple_value 0.10308\n",
      "Iteration 2311: tag train_loss, simple_value 2.29684\n",
      "Timestamp 1615174304464, Iteration 1161\n",
      "batches :1161 2.297107696533203\n",
      "Iteration 2313: tag train_accuracy, simple_value 0.10299\n",
      "Iteration 2313: tag train_loss, simple_value 2.29711\n",
      "Timestamp 1615174304676, Iteration 1162\n",
      "batches :1162 2.3103325366973877\n",
      "Iteration 2315: tag train_accuracy, simple_value 0.10294\n",
      "Iteration 2315: tag train_loss, simple_value 2.31033\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174305068, Iteration 1163\n",
      "batches :1163 2.3143110275268555\n",
      "Iteration 2318: tag train_accuracy, simple_value 0.10289\n",
      "Iteration 2318: tag train_loss, simple_value 2.31431\n",
      "Timestamp 1615174305297, Iteration 1164\n",
      "batches :1164 2.287180185317993\n",
      "Iteration 2320: tag train_accuracy, simple_value 0.10296\n",
      "Iteration 2320: tag train_loss, simple_value 2.28718\n",
      "Timestamp 1615174305517, Iteration 1165\n",
      "batches :1165 2.2948625087738037\n",
      "Iteration 2321: tag train_accuracy, simple_value 0.10287\n",
      "Iteration 2321: tag train_loss, simple_value 2.29486\n",
      "Timestamp 1615174305732, Iteration 1166\n",
      "batches :1166 2.321331262588501\n",
      "Iteration 2323: tag train_accuracy, simple_value 0.10287\n",
      "Iteration 2323: tag train_loss, simple_value 2.32133\n",
      "Timestamp 1615174305996, Iteration 1167\n",
      "batches :1167 2.3057775497436523\n",
      "Iteration 2325: tag train_accuracy, simple_value 0.10287\n",
      "Iteration 2325: tag train_loss, simple_value 2.30578\n",
      "Timestamp 1615174306211, Iteration 1168\n",
      "batches :1168 2.2983181476593018\n",
      "Iteration 2327: tag train_accuracy, simple_value 0.10276\n",
      "Iteration 2327: tag train_loss, simple_value 2.29832\n",
      "Timestamp 1615174306425, Iteration 1169\n",
      "batches :1169 2.294619560241699\n",
      "Iteration 2329: tag train_accuracy, simple_value 0.10269\n",
      "Iteration 2329: tag train_loss, simple_value 2.29462\n",
      "Timestamp 1615174306645, Iteration 1170\n",
      "batches :1170 2.2687835693359375\n",
      "Iteration 2331: tag train_accuracy, simple_value 0.10279\n",
      "Iteration 2331: tag train_loss, simple_value 2.26878\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 16 reduce Alpha = 0.0625\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174307049, Iteration 1171\n",
      "batches :1171 2.3112010955810547\n",
      "Iteration 2333: tag train_accuracy, simple_value 0.10281\n",
      "Iteration 2333: tag train_loss, simple_value 2.3112\n",
      "Timestamp 1615174307265, Iteration 1172\n",
      "batches :1172 2.302628517150879\n",
      "Iteration 2335: tag train_accuracy, simple_value 0.10275\n",
      "Iteration 2335: tag train_loss, simple_value 2.30263\n",
      "Timestamp 1615174307479, Iteration 1173\n",
      "batches :1173 2.2531514167785645\n",
      "Iteration 2337: tag train_accuracy, simple_value 0.1028\n",
      "Iteration 2337: tag train_loss, simple_value 2.25315\n",
      "Timestamp 1615174307786, Iteration 1174\n",
      "batches :1174 2.3025641441345215\n",
      "Iteration 2339: tag train_accuracy, simple_value 0.10286\n",
      "Iteration 2339: tag train_loss, simple_value 2.30256\n",
      "Timestamp 1615174308002, Iteration 1175\n",
      "batches :1175 2.2964444160461426\n",
      "Iteration 2341: tag train_accuracy, simple_value 0.10294\n",
      "Iteration 2341: tag train_loss, simple_value 2.29644\n",
      "Timestamp 1615174308214, Iteration 1176\n",
      "batches :1176 2.3064069747924805\n",
      "Iteration 2343: tag train_accuracy, simple_value 0.10292\n",
      "Iteration 2343: tag train_loss, simple_value 2.30641\n",
      "Timestamp 1615174308428, Iteration 1177\n",
      "batches :1177 2.319366455078125\n",
      "Iteration 2345: tag train_accuracy, simple_value 0.10297\n",
      "Iteration 2345: tag train_loss, simple_value 2.31937\n",
      "intput worker ptr is: 0x55b07e32e070\n",
      "eseAllReduce is called.\n",
      "allreduce size of data: 102228128\n",
      "allreduce: syncInterval = 14 reduce Alpha = 0.0714286\n",
      "Before ddl allreduce\n",
      "after ddl allreduce\n",
      "UnformatedEseWorker allreduce done.\n",
      "Timestamp 1615174309797, Iteration 1178\n",
      "batches :1178 2.3206937313079834\n",
      "Iteration 2345: tag train_accuracy, simple_value 0.125\n",
      "Iteration 2345: tag train_loss, simple_value 2.32069\n",
      "--------END TRAINING: completed 1178 iterations --------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_executor_stdout_log(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download trained model from Watson Machine Learning Accelerator \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wmla-console-wmla.apps.cpd35-beta.cpolab.ibm.com/platform/rest/deeplearning/v1/execs/wmla-398/result\n",
      "Save model:  ./resnet-wmla/wmla-398.zip\n"
     ]
    }
   ],
   "source": [
    "download_trained_model(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further information and useful links\n",
    "[Back to top](#Contents)\n",
    "\n",
    "**WML Accelerator Introduction videos:**\n",
    "- WML Accelerator overview video (1 minute): http://ibm.biz/wmla-video\n",
    "- Overview of adapting your code for Elastic Distributed Training via API: [video](https://youtu.be/RnZtYNX6meM) | [PDF](docs/wmla_api_pieces.pdf) (screenshot below)\n",
    "\n",
    "**Further WML Accelerator information & documentation**\n",
    "- [Learning path: Get started with Watson Machine Learning Accelerator](http://ibm.biz/wmla-learning-path)\n",
    "- [IBM Documentation on Watson Machine Learning Accelerator](https://www.ibm.com/docs/en/wmla/2.2.0)\n",
    "- [Blog: Expert Q&A: Accelerate deep learning on IBM Cloud Pak for Data](https://www.ibm.com/blogs/journey-to-ai/2020/10/expert-qa-accelerate-deep-learning-on-ibm-cloud-pak-for-data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "[Back to top](#Contents)\n",
    "\n",
    "\n",
    "#### This is version 1.0 and its content is copyright of IBM.   All rights reserved.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
