{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Livy test - run a remote Spark MLlib job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sparkmagic notebook extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Connect to the Spark cluster running on Hadoop.*\n",
    "\n",
    "*Replace YOUR_HADOOP_HOSTNAME with the hadoop host that is running Livy.*\n",
    "\n",
    "*You may need to update the default port.  Common defaults are 8998 and 8999.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark add -s test -l python -u http://YOUR_HADOOP_HOSTNAME:8999 -a u -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the Spark KMeans Clustering sample shown in https://spark.apache.org/docs/latest/ml-clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the sample_kmeans_data.txt into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Declare a utility function to run commands on the Hadoop cluster\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "def run_command(command, sleepAfter=None):\n",
    "    p = Popen(command, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n",
    "    output = p.stdout.read()\n",
    "    if (output):\n",
    "        print(output)\n",
    "    if (sleepAfter != None):\n",
    "        time.sleep(sleepAfter)\n",
    "\n",
    "# download the sample_kmeans_data.txt raw data file \n",
    "run_command(\"curl -X GET https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_kmeans_data.txt --output sample_kmeans_data.txt\")\n",
    "\n",
    "# put the data in hdfs \n",
    "run_command(\"hdfs dfs -put -f sample_kmeans_data.txt /tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the clustering model.  \n",
    "\n",
    "Note: Spark MLlib requires the python numpy package on the Hadoop worker nodes. You may need to load the package using commands:\n",
    "\n",
    "`yum install python-pip`\n",
    "\n",
    "`pip install numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/tmp/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
